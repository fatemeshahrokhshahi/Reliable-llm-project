source,identifier,title,abstract,subjects
Springer,10.1007/s11930-024-00397-y,The Impact of Artificial Intelligence on Human Sexuality: A Five-Year Literature Review 2020–2024,"Purpose of Review Millions of people now use generative artificial intelligence (GenAI) tools in their daily lives for a variety of purposes, including sexual ones. This narrative literature review provides the first scoping overview of current research on generative AI use in the context of sexual health and behaviors. Recent Findings The review includes 88 peer-reviewed English language publications from 2020 to 2024 that report on 106 studies and address four main areas of AI use in sexual health and behaviors among the general population: (1) People use AI tools such as ChatGPT to obtain sexual information and education. We identified k  = 14 publications that evaluated the quality of AI-generated sexual health information. They found high accuracy and completeness. (2) People use AI tools such as ChatGPT and dedicated counseling/therapy chatbots to solve their sexual and relationship problems. We identified k  = 16 publications providing empirical results on therapists’ and clients’ perspectives and AI tools’ therapeutic capabilities with mixed but overall promising results. (3) People use AI tools such as companion and adult chatbots (e.g., Replika) to experience sexual and romantic intimacy. We identified k  = 22 publications in this area that confirm sexual and romantic gratifications of AI conversational agents, but also point to risks such as emotional dependence. (4) People use image- and video-generating AI tools to produce pornography with different sexual and non-sexual motivations. We found k  = 36 studies on AI pornography that primarily address the production, uses, and consequences of – as well as the countermeasures against – non-consensual deepfake pornography. This sort of content predominantly victimizes women and girls whose faces are swapped into pornographic material and circulated without their consent. Research on ethical AI pornography is largely missing. Summary Generative AI tools present new risks and opportunities for human sexuality and sexual health. More research is needed to better understand the intersection of GenAI and sexuality in order to a) help people navigate their sexual GenAI experiences, b) guide sex educators, counselors, and therapists on how to address and incorporate AI tools into their professional work, c) advise AI developers on how to design tools that avoid harm, d) enlighten policymakers on how to regulate AI for the sake of sexual health, and e) inform journalists and knowledge workers on how to report about AI and sexuality in an evidence-based manner.","['Medicine & Public Health', 'Urology', 'Endocrinology', 'Urology', 'Endocrinology']"
Springer,10.1007/s13280-024-02086-5,Reaping what we sow: Centering values in food systems transformations research,"In many transdisciplinary research settings, a lack of attention to the values underpinning project aims can inhibit stakeholder engagement and ultimately slow or undermine project outcomes. As a research collective (The Careoperative), we have developed a set of four shared values through a facilitated visioning process, as central to the way we work together: care, reflexivity, inclusivity, and collectivity. In this paper, we explore the implications of a values-centered approach to collaboration in food system transformation research. The paper presents two cases that illustrate how researchers might approach centering values in practice. Where much research on food system transformation focuses on values of food system stakeholders, we contribute insights into the values of researchers in such transdisciplinary endeavors. Specifically, we argue that researchers working on sustainability transformations need to be better prepared to engage in such reflections and aspire to embody values aligned with the transformations they seek to research.","['Environment', 'Environment, general', 'Ecology', 'Atmospheric Sciences', 'Physical Geography', 'Environmental Management', 'Environmental Engineering/Biotechnology', 'Environmental Sciences', 'Ecology', 'Atmospheric Science', 'Physical Geography', 'Environmental Management', 'Environmental Engineering/Biotechnology']"
Springer,10.1007/s12083-024-01826-4,A multi-cycle recursive clustering algorithm for the analysis of social media data streams,"Events are usually embedded in latent topics and the extraction of these latent topics are enabled by event detection algorithms. Unsupervised algorithms like Clustering algorithms are very useful for detecting events but with requirements which may not be relevant or easy to determine when using unstructured textual social media data. For instance, some algorithms are required to be used on specific data shapes, but determining the shape of an unstructured data may not be practical aside from the high level of noise in the data. Many of the existing algorithms work well with structured data, however, some of these algorithms can be adapted to unstructured data with the caveat that cluster formations may not contain consistent contextual information. We propose a novel Multi-Cycle Recursive Clustering Algorithm (MCRCA), able to sequentially eliminate noise, resulting in high homogeneous cluster formations. MCRCA does not require the initial specification of clusters numbers as the estimated number of clusters can be deduced at convergence. Our algorithm out-performs the classical LDA and K-Means algorithms in forming highly homogeneous clusters, context-wise.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing', 'Communications Engineering, Networks', 'Computer Engineering and Networks', 'Computer Communication Networks', 'Signal, Speech and Image Processing']"
Springer,10.1007/s13280-024-02083-8,Geodesign to advance boundary work in urban planning: A study in Stockholm focused on nature-based solutions,"Geodesign supports collaborative urban planning by managing ‘boundaries’ between diverse knowledge holders. However, there is a paucity of empirical evidence of its contribution to ‘boundary work’. This paper aims to evaluate how a geodesign process facilitates knowledge co-production through boundary work and to assess the scientific credibility, political saliency, and procedural legitimacy of its outputs in urban planning. We propose a replicable geodesign framework to assess boundary work, and test it in a case study on urban transformations with nature-based solutions in the Skarpnäck district of Stockholm, Sweden. Findings indicate that all geodesign steps facilitated communication by promoting collective reasoning. Participants acknowledged contributions to knowledge co-production and decision-making by mediating between different perspectives. However, data quality and modeling simplicity were identified as critical factors affecting the outputs’ perceived credibility. Future applications should include co-designing the geodesign process, improving capacity and skills, and facilitating more integrated planning.","['Environment', 'Environment, general', 'Ecology', 'Atmospheric Sciences', 'Physical Geography', 'Environmental Management', 'Environmental Engineering/Biotechnology', 'Environmental Sciences', 'Ecology', 'Atmospheric Science', 'Physical Geography', 'Environmental Management', 'Environmental Engineering/Biotechnology']"
Springer,10.1007/s10695-024-01418-2,Evaluation of carp sperm respiration: fluorometry with optochemical oxygen sensor versus polarography,"The primary function of spermatozoa is to fertilize the oocyte, which depends on their motility and is directly associated with their metabolic state. The oxygen consumption rate (OCR) of spermatozoa reflects the respiratory capacity of sperm mitochondria under various physiological conditions and is an essential marker of sperm quality. We determined the OCR of common carp ( Cyprinus carpio ) sperm using two respirometry methods: the conventionally used polarographic method with a Clark-type electrode and fluorometric assay with an Oxo Dish optochemical oxygen sensor. The latter was used for the first time to evaluate spermatozoa oxygen consumption in various metabolic states (under different treatments) at different dilution rates. These two methods were compared using Bland–Altman analysis, and the applicability of the optochemical oxygen sensor for evaluating carp sperm oxygen consumption was discussed. Sperm motility and progressive velocity parameters were also assessed to evaluate the effect of sperm respiration under different metabolic states and dilution rates and preincubation period on the physiological status of spermatozoa. The comparison of these respirometry methods clearly shows that while the polarographic method allows immediate measurement of oxygen levels after adding a sperm sample, the optochemical oxygen sensor has a priority in the amount of data obtained due to simultaneous measurements of several samples (e.g., different males, different fish species, repetitions of the same sample or various experimental conditions), even at a later time after adding sperm to the measuring chamber. However, the compared methods are complementary, and the proposed methodology can be applied to other fish species.","['Life Sciences', 'Freshwater & Marine Ecology', 'Animal Physiology', 'Animal Anatomy / Morphology / Histology', 'Animal Biochemistry', 'Zoology', 'Freshwater and Marine Ecology', 'Animal Physiology', 'Animal Anatomy', 'Chemical Biology', 'Zoology']"
Springer,10.1007/s13280-024-02091-8,A conceptual framework of indicators for the suitability of forests for outdoor recreation,"Forests’ ability to provide opportunities for recreation is an important ecosystem service. This has prompted attempts to create indicators to assess forests' suitability for recreation, although hitherto with limited success. This study introduces a novel framework for indicators of potential and realised recreational values of forests, with a primary focus on Sweden and Fennoscandia. We divided forest attributes into intrinsic qualities (i.e. the structure and composition of the forest), extrinsic qualities (i.e. the location of the forest in relation to other components of the landscape), and facilitation qualities (i.e. the presence of recreational infrastructure). Using Fennoscandia as a case study, we performed a literature review to find specific indicators of recreational values, as well as evaluate the current availability of spatial data suitable to map the forest qualities on a national scale. The most important intrinsic quality we identified was tree size/age, whereas for extrinsic quality it was proximity to water. Systematic monitoring of recreational use is essential to estimate realised recreational values. The conceptual framework proved to be a valuable tool for identifying potential indicators, and applying it in other regions is likely to yield useful outcomes.","['Environment', 'Environment, general', 'Ecology', 'Atmospheric Sciences', 'Physical Geography', 'Environmental Management', 'Environmental Engineering/Biotechnology', 'Environmental Sciences', 'Ecology', 'Atmospheric Science', 'Physical Geography', 'Environmental Management', 'Environmental Engineering/Biotechnology']"
Springer,10.1007/s13346-024-01679-7,3D printed microneedles: revamping transdermal drug delivery systems,"One of the advancements of the transdermal drug delivery system (TDDS) is the development of microneedles (MNs). These micron-sized needles are used for delivering various types of drugs to address the disadvantage of other transdermal techniques as well as oral drug delivery systems. MNs have high patient acceptance due to self-administration with minimally invasive and pain compared to the parenteral drug delivery. Over the years, various methods have been adopted to evolve the MNs and make them more cost-effective, accurate, and suitable for multiple applications. One such method is the 3D printing of MNs. The development of MN platforms using 3D printing has been made possible by improved features like precision, printing resolution, and the feasibility of using low-cost raw materials. In this review, we have tried to explain various types of MNs, fabrication methods, materials used in the formulation of MNs, and the recent applications that utilize 3D-printed MNs.","['Biomedicine', 'Pharmaceutical Sciences/Technology', 'Pharmaceutics']"
Springer,10.1007/s11071-024-10333-3,Exploring iterative and non-iterative Fourier series-based methods of control optimization in application to a discontinuous capsule drive model,"The paper explains iterative and non-iterative approaches to control optimization with use of the Fourier series-based method. Both variants of the presented algorithm are used to numerically approximate optimal control of a discontinuous pendulum capsule drive. Firstly, the general algorithm and its two realizations (iterative and non-iterative) are presented. It is shown that the iterative variant assures non-decreasing quality of solutions in subsequent repetitions of the procedure and the background of such guarantees is explained. A numerical example follows: control of a self-propelled capsule drive is optimized using both approaches. Results are compared and discussed. It is expected that the presented methods can be useful in optimal control estimation for complex systems, particularly discontinuous ones.","['Physics', 'Applications of Nonlinear Dynamics and Chaos Theory', 'Statistical Physics and Dynamical Systems', 'Classical Mechanics', 'Vibration, Dynamical Systems, Control', 'Multibody Systems and Mechanical Vibrations', 'Classical Mechanics', 'Mechanical Engineering', 'Applied and Technical Physics']"
Springer,10.1038/s41586-024-08168-4,Offline ensemble co-reactivation links memories across days,"Memories are encoded in neural ensembles during learning^ 1 – 6 and are stabilized by post-learning reactivation^ 7 – 17 . Integrating recent experiences into existing memories ensures that memories contain the most recently available information, but how the brain accomplishes this critical process remains unclear. Here we show that in mice, a strong aversive experience drives offline ensemble reactivation of not only the recent aversive memory but also a neutral memory formed 2 days before, linking fear of the recent aversive memory to the previous neutral memory. Fear specifically links retrospectively, but not prospectively, to neutral memories across days. Consistent with previous studies, we find that the recent aversive memory ensemble is reactivated during the offline period after learning. However, a strong aversive experience also increases co-reactivation of the aversive and neutral memory ensembles during the offline period. Ensemble co-reactivation occurs more during wake than during sleep. Finally, the expression of fear in the neutral context is associated with reactivation of the shared ensemble between the aversive and neutral memories. Collectively, these results demonstrate that offline ensemble co-reactivation is a neural mechanism by which memories are integrated across days. In mice, a strong aversive experience drives offline ensemble reactivation of not only the recent aversive memory but also a neutral memory formed 2 days before, linking fear of the recent aversive memory to the previous neutral memory.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1038/s41586-024-08279-y,Endogenous self-peptides guard immune privilege of the central nervous system,"Despite the presence of strategically positioned anatomical barriers designed to protect the central nervous system (CNS), it is not entirely isolated from the immune system^ 1 , 2 . In fact, it remains physically connected to, and can be influenced by, the peripheral immune system^ 1 . How the CNS retains such responsiveness while maintaining an immunologically unique status remains an outstanding question. Here, in searching for molecular cues that derive from the CNS and enable its direct communication with the immune system, we identified an endogenous repertoire of CNS-derived regulatory self-peptides presented on major histocompatibility complex class II (MHC-II) molecules in the CNS and at its borders. During homeostasis, these regulatory self-peptides were found to be bound to MHC-II molecules throughout the path of lymphatic drainage from the brain to its surrounding meninges and its draining cervical lymph nodes. However, in neuroinflammatory disease, the presentation of regulatory self-peptides diminished. After boosting the presentation of these regulatory self-peptides, a population of suppressor CD4^+ T cells was expanded, controlling CNS autoimmunity in a CTLA-4- and TGFβ-dependent manner. CNS-derived regulatory self-peptides may be the molecular key to ensuring a continuous dialogue between the CNS and the immune system while balancing overt autoreactivity. This sheds light on how we conceptually think about and therapeutically target neuroinflammatory and neurodegenerative diseases. Central nervous system (CNS)-derived regulatory self-peptides are essential for maintaining a continuous dialogue between the CNS and the immune system while balancing overt autoreactivity.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1038/s41586-024-08222-1,Gut microbiome strain-sharing within isolated village social networks,"When humans assemble into face-to-face social networks, they create an extended social environment that permits exposure to the microbiome of others, thereby shaping the composition and diversity of the microbiome at individual and population levels^ 1 – 6 . Here we use comprehensive social network mapping and detailed microbiome sequencing data in 1,787 adults within 18 isolated villages in Honduras^ 7 to investigate the relationship between network structure and gut microbiome composition. Using both species-level and strain-level data, we show that microbial sharing occurs between many relationship types, notably including non-familial and non-household connections. Furthermore, strain-sharing extends to second-degree social connections, suggesting the relevance of a person’s broader network. We also observe that socially central people are more microbially similar to the overall village than socially peripheral people. Among 301 people whose microbiome was re-measured 2 years later, we observe greater convergence in strain-sharing in connected versus otherwise similar unconnected co-villagers. Clusters of species and strains occur within clusters of people in village social networks, meaning that social networks provide the social niches within which microbiome biology and phenotypic impact are manifested. An investigation into the relationship between network structure and gut microbiome composition among people living in 18 isolated Honduras villages reveals that strain-sharing can be mediated by complex, village-wide social interactions.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1007/978-981-97-6095-4_3,Cases,"The case illustrates that the patentee’s possibilities to avoid exhaustion by tailoring the patent claims are limited. It is generally up to the patentee to define the product for which he claims protection. As long as the patent covers products which are available on the market, the patentee’s decision may be relevant for the question of exhaustion.","['Law', 'Private International Law, International & Foreign Law, Comparative Law', 'IT Law, Media Law, Intellectual Property', 'History of China', 'Private International Law, International and Foreign Law, Comparative Law', 'IT Law, Media Law, Intellectual Property', 'History of China']"
Springer,10.1007/978-3-031-61194-0_1,Introduction,"The IEA International Computer and Information Literacy Study (ICILS) investigates the capacities of young people to use information and communications technology (ICT) productively for a range of different purposes, in ways that go beyond a basic use of ICT. ICILS 2023 includes authentic computer? Based assessments that are administered to students in their eighth year of schooling. These generate data reflecting two dimensions of ICT? Related capacities: computer and information literacy (CIL); and computational thinking (CT). This chapter describes the place of ICILS as part of the history of IEA studies, the relationship between ICT and educational processes, as well as factors related to the pedagogical use of ICT, since the late? 1980s. The relevance of ICILS to supranational policy development and monitoring (such as by the United Nations and the European Commission) monitoring of digital skills) is discussed, as are national policy and program initiatives in CIL and CT education in ICILS countries. The chapter further includes a summary recent research publications using ICILS data. The chapter concludes with summary information describing the ICILS study design, including the study research questions, new areas of focus in the study and the target population definitions and sampling procedures.","['Education', 'Education, general', 'International and Comparative Education', 'Digital Education and Educational Technology', 'Assessment and Testing', 'International and Comparative Education']"
Springer,10.1007/978-3-031-71177-0_24,A Pyramid Of (Formal) Software Verification,"Over the past few years there has been significant progress in the various fields of software verification resulting in many useful tools and successful deployments, both academic and commercial. However much of the work describing these tools and ideas is written by and for the research community. The scale, diversity and focus of the literature can act as a barrier, separating industrial users and the wider academic community from the tools that could make their work more efficient, more certain and more productive. This tutorial gives a simple classification of verification techniques in terms of a pyramid and uses it to describe the six main schools of verification technologies. We have found this approach valuable for building collaborations with industry as it allows us to explain the intrinsic strengths and weaknesses of techniques and pick the right tool for any given industrial application. The model also highlights some of the cultural differences and unspoken assumptions of different areas of verification and illuminates future directions.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-61194-0_3,Computational Thinking Framework,"The International Computer and Information Literacy Study (ICILS) 2023 computational thinking (CT) framework delves into the evolving concept of CT within the context of educational curricula. The chapter traces the history of CT, from its early conceptualizations focusing on logical reasoning and programming, to the advent of block-based coding platforms like Scratch and Blockly, which have made coding more accessible. It also discusses the integration of CT in the International Computer and Information Literacy Study (ICILS) and the development of an internationally comparable CT scale. The chapter further explores the differing perspectives on CT, ranging from viewing it as a subset of computer science to recognizing its broader problem-solving applications. We present and discuss various definitions of CT and the constituent components identified in the literature, underscoring the diversity of interpretations and representations of CT.The framework for ICILS 2023 CT is presented in detail, including its structure, strands, aspects, and the underlying rationale. We emphasize the importance of understanding and framing real-world problems for computational formulation, alongside the development of algorithmic solutions operationalizable by computers. The chapter concludes with insights into the future directions of CT assessment and its potential implications for educational practices and policies.","['Education', 'Education, general', 'International and Comparative Education', 'Digital Education and Educational Technology', 'Assessment and Testing', 'International and Comparative Education']"
Springer,10.1007/978-981-97-6915-5_6,GigaVision: When Computer Vision Meets Gigapixel Videography,"In previous chapters, we have explored advanced plenoptic imaging and reconstruction techniques, enabling images and videos to reach gigapixel-level resolution. This breakthrough unlocks new possibilities for a wide range of applications and industries. However, traditional computer vision methods, tailored for megapixel-level data, are ill-equipped to handle the complexities of gigapixel-level data, which often feature large-scale scenes with hundreds of objects and intricate interactions. As a result, these methods face significant limitations in both precision and efficiency.","['Computer Science', 'Image Processing and Computer Vision', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Materials Science, general', 'Computer Vision', 'Image Processing', 'Imaging Techniques']"
Springer,10.1007/978-981-97-6915-5_5,Toward Large-Scale Plenoptic Reconstruction,Reconstructing real-world scenes with unparalleled levels of realism and detail has been a long-standing goal in the fields of computer vision and graphics. Achieving this goal necessitates coordinated efforts in both sensing techniques and plenoptic reconstruction algorithms.,"['Computer Science', 'Image Processing and Computer Vision', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Materials Science, general', 'Computer Vision', 'Image Processing', 'Imaging Techniques']"
Springer,10.1007/978-3-031-71177-0_32,The Java Verification Tool KeY:A Tutorial,"The KeY tool is a state-of-the-art deductive program verifier for the Java language. Its verification engine is based on a sequent calculus for dynamic logic, realizing forward symbolic execution of the target program, whereby all symbolic paths through a program are explored. Method contracts make verification scalable. KeY combines auto-active and fine-grained proof interaction, which is possible both at the level of the verification target and its specification, as well as at the level of proof rules and program logic. This makes KeY well-suited for teaching program verification, but also permits proof debugging at the source code level. The latter made it possible to verify some of the most complex Java code to date. The article provides a self-contained introduction to the working principles and the practical usage of KeY for anyone with basic knowledge in logic and formal methods.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_11,Efficient Formally Verified Maximal End Component Decomposition for MDPs,"Identifying a Markov decision process’s maximal end components is a prerequisite for applying sound probabilistic model checking algorithms. In this paper, we present the first mechanized correctness proof of a maximal end component decomposition algorithm, which is an important algorithm in model checking, using the Isabelle/HOL theorem prover. We iteratively refine the high-level algorithm and proof into an imperative LLVM bytecode implementation that we integrate into the Modest Toolset ’s existing mcsta model checker. We bring the benefits of interactive theorem proving into practice by reducing the trusted code base of a popular probabilistic model checker and we experimentally show that our new verified maximal end component decomposition in mcsta performs on par with the tool’s previous unverified implementation.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_33,Parameterized Verification of Round-Based Distributed Algorithms via Extended Threshold Automata,"Threshold automata are a computational model that has proven to be versatile in modeling threshold-based distributed algorithms and enabling their completely automatic parameterized verification. We present novel techniques for the verification of threshold automata, based on well-structured transition systems, that allow us to extend the expressiveness of both the computational model and the specifications that can be verified. In particular, we extend the model to allow decrements and resets of shared variables, possibly on cycles, and the specifications to general coverability. While these extensions of the model in general lead to undecidability, our algorithms provide a semi-decision procedure. We demonstrate the benefit of our extensions by showing that we can model complex round-based algorithms such as the phase king consensus algorithm and the Red Belly Blockchain protocol (published in 2019), and verify them fully automatically for the first time.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-60931-2_1,From Science to Society: The Open Science and Innovation and Network Approach,"Public investment in fundamental scientific research generates societal benefits (Mazzucato in Public Aff, 2018 [ 1 ]; Barrett et al. in Why basic science matters for economic growth. Public investment in basic research will pay for itself. International Monetary Fund Blog, 2011 [ 2 ]; Zuniga and Wunsch-Vincent in Harnessing the benefits of publicly-funded research. WIPO Magazine, 2012 [ 3 ]; Adams in Calif Manage Rev 48(1):29–51, 2005 [ 4 ]; European Physical Society in Physics and the economy. Report. Centre for Economics and Business Research, 2019 [ 5 ]). At first sight it seems counterintuitive that public funding of a curiosity driven activity that does not address immediate societal challenges or urgent needs can produce wealth and be even long-term sustainable. We are rather tempted to argue that on the contrary, only applied research and targeted investments such as for instance addressing climate change, advancing microelectronics, increasing the effectiveness of battery-based energy storage or the developments of space technologies can satisfy this criterion. It is important to engage both, public and private funds to address such challenges, but science is a key ingredient to come up with the truly disruptive solutions. The funds required to address grand challenges call for globally concerted approaches over several decades with effects that will become only visible after several generations. Funding alone will, however, not be sufficient to effectively respond to societal challenges. Looking at the private sector, it turns out that a significant share of high-tech companies are ultimately results of initial public funding for curiosity driven scientific research.","['Physics', 'Particle and Nuclear Physics', 'Economic Growth', 'Investment Appraisal', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Nuclear and Particle Physics', 'Economic Development, Innovation and Growth', 'Investment Appraisal', 'Data Science', 'Space Physics', 'Measurement Science and Instrumentation']"
Springer,10.1007/978-3-031-72848-8_6,MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction,"Multi-agent trajectory prediction is crucial to autonomous driving and understanding the surrounding environment. Learning-based approaches for multi-agent trajectory prediction, such as primarily relying on graph neural networks, graph transformers, and hypergraph neural networks, have demonstrated outstanding performance on real-world datasets in recent years. However, the hypergraph transformer-based method for trajectory prediction is yet to be explored. Therefore, we present a M ultisc A le R elational T ransformer ( MART ) network for multi-agent trajectory prediction. MART is a hypergraph transformer architecture to consider individual and group behaviors in transformer machinery. The core module of MART is the encoder, which comprises a Pair-wise Relational Transformer (PRT) and a Hyper Relational Transformer (HRT). The encoder extends the capabilities of a relational transformer by introducing HRT, which integrates hyperedge features into the transformer mechanism, promoting attention weights to focus on group-wise relations. In addition, we propose an Adaptive Group Estimator (AGE) designed to infer complex group relations in real-world environments. Extensive experiments on three real-world datasets (NBA, SDD, and ETH-UCY) demonstrate that our method achieves state-of-the-art performance, enhancing ADE/FDE by 3.9%/11.8% on the NBA dataset. Code is available at https://github.com/gist-ailab/MART .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Image Processing', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems']"
Springer,10.1007/978-3-031-73257-7_16,Large and Parallel Human Sorting Networks,"This paper presents two innovative extensions of the classic Human Sorting Network (HSN) activity from the CS Unplugged program. First, we describe the implementation of a large-scale HSN with 50 input nodes, realized with high school students in Vienna, Austria. We detail the logistical challenges and solutions for creating an HSN of this magnitude, including location selection, network layout, and participant coordination. Second, we report on using parallel 6-input HSNs, which introduce a competitive element and enhance engagement. This parallel setup allows for races between teams and can be adapted for various age groups and knowledge levels. Both extensions aim to increase the educational impact and enjoyment of the HSN activity. We provide comprehensive insights into our experiences, enabling other educators and researchers to replicate or further develop these HSN variants.","['Computer Science', 'Computer Science, general', 'Mathematics of Computing', 'Computer Applications', 'Computers and Education', 'Computer Science', 'Mathematics of Computing', 'Computer and Information Systems Applications', 'Computers and Education']"
Springer,10.1007/978-3-031-71177-0_14,Reusable Specification Patterns for Verification of Resilience in Autonomous Hybrid Systems,"Autonomous hybrid systems are systems that combine discrete and continuous behavior with autonomous decision-making, e.g., using reinforcement learning. Such systems are increasingly used in safety-critical applications such as self-driving cars, autonomous robots or water supply systems. Thus, it is crucial to ensure their safety and resilience, i.e., that they function correctly even in the presence of dynamic changes and disruptions. In this paper, we present an approach to obtain formal resilience guarantees for autonomous hybrid systems using the interactive theorem prover KeYmaera X. Our key ideas are threefold: First, we derive a formalization of resilience that is tailored to autonomous hybrid systems. Second, we present reusable patterns for modeling stressors, detecting disruptions, and specifying resilience as a service level response in the differential dynamic logic (d $$\mathcal {L}$$ L ). Third, we combine these concepts with an existing approach for the safe integration of learning components using hybrid contracts, and extend it towards dynamic adaptations to stressors. By combining reusable patterns for stressors, observers, and adaptation contracts for learning components, we provide a systematic approach for the deductive verification of resilience of autonomous hybrid systems with reduced specification effort. We demonstrate the applicability of our approach with two case studies, an autonomous robot and an intelligent water distribution system.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_31,Satisfiability Modulo Theories: A Beginner’s Tutorial,"Great minds have long dreamed of creating machines that can function as general-purpose problem solvers. Satisfiability modulo theories (SMT) has emerged as one pragmatic realization of this dream, providing significant expressive power and automation. This tutorial is a beginner’s guide to SMT. It includes an overview of SMT and its formal foundations, a catalog of the main theories used in SMT solvers, and illustrations of how to obtain models and proofs. Throughout the tutorial, examples and exercises are provided as hands-on activities for the reader. They can be run using either Python or the SMT-LIB language, using either the cvc5 or the Z3 SMT solver.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-6915-5_4,Plenoptic Reconstruction,"Empowered by advanced plenoptic sensing systems, light-field imaging becomes one of the most extensively used methods for capturing 3D views of a scene. In contrast to the traditional input to a 3D graphics system, namely, scenes consisting of pre-defined geometric primitives with different materials and sets of lights, the input to a light field is only a set of 2D images which are informative and cost effective. Unfortunately, due to the limited sensor resolution, existing systems must balance the spatial and angular resolution, i.e., one can obtain dense sampling images in the spatial dimension but only sparse sampling images in the angular (viewing angle) dimension or vice versa.","['Computer Science', 'Image Processing and Computer Vision', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Materials Science, general', 'Computer Vision', 'Image Processing', 'Imaging Techniques']"
Springer,10.1007/978-3-031-71162-6_28,Proving Functional Program Equivalence via Directed Lemma Synthesis,"Proving equivalence between functional programs is a fundamental problem in program verification, which often amounts to reasoning about algebraic data types (ADTs) and compositions of structural recursions . Modern theorem provers provide structural induction for such reasoning, but a structural induction on the original theorem is often insufficient for many equivalence theorems. In such cases, one has to invent a set of lemmas, prove these lemmas by additional induction, and use these lemmas to prove the original theorem. There is, however, a lack of systematic understanding of what lemmas are needed for inductive proofs and how these lemmas can be synthesized automatically. This paper presents directed lemma synthesis , an effective approach to automating equivalence proofs by discovering critical lemmas using program synthesis techniques. We first identify two induction-friendly forms of propositions that give formal guarantees to the progress of the proof. We then propose two tactics that synthesize and apply lemmas, thereby transforming the proof goal into induction-friendly forms. Both tactics reduce lemma synthesis to a set of independent and typically small program synthesis problems that can be efficiently solved. Experimental results demonstrate the effectiveness of our approach: Compared to state-of-the-art equivalence checkers employing heuristic-based lemma enumeration, directed lemma synthesis saves 95.47% runtime on average and solves 38 more tasks over an extended version of the standard benchmark set.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_7,Automated Static Analysis of Quality of Service Properties of Communicating Systems,"We present , a bounded to statically analyse Quality of Service ( ) properties of message-passing systems. We consider QoS properties on measurable application-level attributes as well as resource consumption metrics, for example, those relating monetary cost to memory usage. The applicability of is evaluated through case studies and experiments. A first case study is based on the AWS cloud while a second one analyses a communicating system automatically extracted from code. Additionally, we consider synthetically generated experiments to assess the scalability of . These experiments showed that our model can faithfully capture and effectively analyse QoS properties in industrial-strength scenarios.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_5,Nonlinear Craig Interpolant Generation Over Unbounded Domains by Separating Semialgebraic Sets,"Interpolation-based techniques become popular in recent years, as they can improve the scalability of existing verification techniques due to their inherent modularity and local reasoning capabilities. Synthesizing Craig interpolants is the cornerstone of these techniques. In this paper, we investigate nonlinear Craig interpolant synthesis for two polynomial formulas of the general form, essentially corresponding to the underlying mathematical problem to separate two disjoint semialgebraic sets. By combining the homogenization approach with existing techniques, we prove the existence of a novel class of non-polynomial interpolants called semialgebraic interpolants. These semialgebraic interpolants subsume polynomial interpolants as a special case. To the best of our knowledge, this is the first existence result of this kind. Furthermore, we provide complete sum-of-squares characterizations for both polynomial and semialgebraic interpolants, which can be efficiently solved as semidefinite programs. Examples are provided to demonstrate the effectiveness and efficiency of our approach.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_30,Misconceptions in Finite-Trace and Infinite-Trace Linear Temporal Logic,"With the growing use of temporal logics in areas ranging from robot planning to runtime verification, it is critical that users have a clear understanding of what a specification means. Toward this end, we have been developing a catalog of semantic errors and a suite of test instruments targeting various user-groups. The catalog is of interest to educators, to logic designers, to formula authors, and to tool builders, e.g., to identify mistakes. The test instruments are suitable for classroom teaching or self-study. This paper reports on five sets of survey data collected over a three-year span. We study misconceptions about finite-trace $$\textsc {ltl}_{f}$$ L T L f in three ltl -aware audiences, and misconceptions about standard ltl in novices. We find several mistakes, even among experts. In addition, the data supports several categories of errors in both $$\textsc {ltl}_{f}$$ L T L f and ltl that have not been identified in prior work. These findings, based on data from actual users, offer insights into what specific ways temporal logics are tricky and provide a groundwork for future interventions.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-75387-9_1,QuAK: Quantitative Automata Kit,"System behaviors are traditionally evaluated through binary classifications of correctness, which do not suffice for properties involving quantitative aspects of systems and executions. Quantitative automata offer a more nuanced approach, mapping each execution to a real number by incorporating weighted transitions and value functions generalizing acceptance conditions. In this paper, we introduce QuAK, the first tool designed to automate the analysis of quantitative automata. QuAK currently supports a variety of quantitative automaton types, including $${\textsf{Inf}}$$ Inf , $${\textsf{Sup}}$$ Sup , $${\textsf{LimInf}}$$ LimInf , $${\textsf{LimSup}}$$ LimSup , $${\textsf{LimInfAvg}}$$ LimInfAvg , and $${\textsf{LimSupAvg}}$$ LimSupAvg automata, and implements decision procedures for problems such as emptiness, universality, inclusion, equivalence, as well as for checking whether an automaton is safe, live, or constant. Additionally, QuAK is able to compute extremal values when possible, construct safety-liveness decompositions, and monitor system behaviors. We demonstrate the effectiveness of QuAK through experiments focusing on the inclusion, constant-function check, and monitoring problems.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Mathematical Logic and Formal Languages', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Control Structures and Microprogramming', 'Software Engineering', 'Formal Languages and Automata Theory', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Control Structures and Microprogramming']"
Springer,10.1007/978-3-031-71162-6_23,Accurate Static Data Race Detection for C,"Data races are a particular kind of subtle, unintended program behaviour arising from thread interference in shared-memory concurrency. In this paper, we propose an automated technique for static detection of data races in multi-threaded C programs with POSIX threads. The key element of our technique is a reduction to reachability. Our prototype implementation combines such reduction with context-bounded analysis. The approach proves competitive against state-of-the-art tools, finding new issues in the implementation of well-known lock-free data structures, and shows a considerably superior accuracy of analysis in the presence of complex shared-memory access patterns.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_15,Switching Controller Synthesis for Hybrid Systems Against STL Formulas,"Switching controllers play a pivotal role in directing hybrid systems (HSs) towards the desired objective, embodying a “correct-by-construction” approach to HS design. Identifying these objectives is thus crucial for the synthesis of effective switching controllers. While most of existing works focus on safety and liveness, few of them consider timing constraints. In this paper, we delves into the synthesis of switching controllers for HSs that meet system objectives given by a fragment of STL, which essentially corresponds to a reach-avoid problem with timing constraints. Our approach involves iteratively computing the state sets that can be driven to satisfy the reach-avoid specification with timing constraints. This technique supports to create switching controllers for both constant and non-constant HSs. We validate our method’s soundness, and confirm its relative completeness for a certain subclass of HSs. Experiment results affirms the efficacy of our approach.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-73741-1_15,Model Driven Development for AI-Based Healthcare Systems: A Review,"We review our experience with integrating Artificial Intelligence (AI) into healthcare systems following the Model-Driven Development (MDD) approach. At a time when AI has the potential to instigate a paradigm shift in the health sector, better integrating healthcare experts in the development of these technologies is of paramount importance. We see MDD as a useful way to better embed non-technical stakeholders in the development process. The main goal of this review is to reflect on our experiences to date with MDD and AI in the context of developing healthcare systems. Four case studies that fall within that scope but have different profiles are introduced and summarised: the MyMM application for Multiple Myeloma diagnosis; CNN-HAR, that studies the ability to do AI on the edge for IoT-supported human activity recognition; the HIPPP web based portal for patient information in public health; and Cinco de Bio, a new model driven platform used for the first time to support a better cell-level understanding of diseases. Based on the aforementioned case studies we discuss the characteristics, the challenges faced and the postive outcomes achieved.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-71162-6_21,"
              
             A 
              
            obustness 
              
            fication Tool for 
              
            uantum Machine Learning Models","Adversarial noise attacks present a significant threat to quantum machine learning (QML) models, similar to their classical counterparts. This is especially true in the current Noisy Intermediate-Scale Quantum era, where noise is unavoidable. Therefore, it is essential to ensure the robustness of QML models before their deployment. To address this challenge, we introduce VeriQR , the first tool designed specifically for formally verifying and improving the robustness of QML models, to the best of our knowledge. This tool mimics real-world quantum hardware’s noisy impacts by incorporating random noise to formally validate a QML model’s robustness. VeriQR supports exact (sound and complete) algorithms for both local and global robustness verification. For enhanced efficiency, it implements an under-approximate (complete) algorithm and a tensor network-based algorithm to verify local and global robustness, respectively. As a formal verification tool, VeriQR can detect adversarial examples and utilize them for further analysis and to enhance the local robustness through adversarial training, as demonstrated by experiments on real-world quantum machine learning models. Moreover, it permits users to incorporate customized noise. Based on this feature, we assess VeriQR using various real-world examples, and experimental outcomes confirm that the addition of specific quantum noise can enhance the global robustness of QML models. These processes are made accessible through a user-friendly graphical interface provided by VeriQR , catering to general users without requiring a deep understanding of the counter-intuitive probabilistic nature of quantum computing.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-64451-1_4,Processing Multimodal Information: Challenges and Solutions for Multimodal Sentiment Analysis and Hate Speech Detection,"This chapter explores the challenges and solutions for processing multimodal information, specifically in the context of multimodal sentiment analysis and hate speech detection. The increasing amount of multimodal data, such as text, images and videos, presents unique challenges for machine learning algorithms. These challenges include the integration and fusion of information from multiple modalities to acquire the overall context. In this chapter, first, we present an overview of recent developments on multimodal learning techniques in the context of sentiment and hate speech detection; second, we present a multimodal model that combines different visual aspects and features for multimodal sentiment detection; and third, we present a multi-task multimodal model for misogyny detection in multimodal memes.","['Computer Science', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Statistics, general', 'Natural Language Processing (NLP)', 'Digital/New Media', 'Data Science', 'Data Mining and Knowledge Discovery', 'Data Analysis and Big Data', 'Natural Language Processing (NLP)', 'Digital and New Media']"
Springer,10.1007/978-3-031-71162-6_13,Fast Attack Graph Defense Localization via Bisimulation,"System administrators, network engineers, and IT managers can learn much about the vulnerabilities of an organization’s cyber system by constructing and analyzing analytical attack graphs (AAGs). An AAG consists of logical rule nodes, fact nodes, and derived fact nodes. It provides a graph-based representation that describes ways by which an attacker can achieve progress towards a desired goal, a.k.a. a crown jewel. Given an AAG, different types of analyses can be performed to identify attacks on a target goal, measure the vulnerability of the network, and gain insights on how to make it more secure. However, as the size of the AAGs representing real-world systems may be very large, existing analyses are slow or practically impossible. In this paper, we introduce and show how to compute an AAG’s defense core : a locally minimal subset of the AAG’s rules whose removal will prevent an attacker from reaching a crown jewel. Most importantly, in order to scale-up the performance of the detection of a defense core, we introduce a novel application of the well-known notion of bisimulation to AAGs. Our experiments show that the use of bisimulation results in significantly smaller graphs and in faster detection of defense cores, making them practical.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-64451-1_11,Claim Detection in Social Media,"In recent years, the problem of misinformation on the web has become widespread across languages, countries and various social media platforms. One problem central to stopping the spread of misinformation is identifying claims and prioritising them for fact-checking. Although there has been much work on automated claim detection from text recently, the role of images and their variety still need to be explored. As posts and content shared on social media are often multimodal, it has become crucial to view the problem of misinformation and fake news from a multimodal perspective. In this chapter, first, we present an overview of existing claim detection methods and their limitations; second, we present a unimodal approach to identify check-worthy claims; third, and lastly, we introduce a dataset that takes both the image and text into account for detecting claims and benchmark recent multimodal models on the task.","['Computer Science', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Statistics, general', 'Natural Language Processing (NLP)', 'Digital/New Media', 'Data Science', 'Data Mining and Knowledge Discovery', 'Data Analysis and Big Data', 'Natural Language Processing (NLP)', 'Digital and New Media']"
Springer,10.1007/978-3-031-71162-6_16,Learning Branching-Time Properties in CTL and ATL via Constraint Solving,"We address the problem of learning temporal properties from the branching-time behavior of systems. Existing research in this field has mostly focused on learning linear temporal properties specified using popular logics, such as Linear Temporal Logic (LTL) and Signal Temporal Logic (STL). Branching-time logics such as Computation Tree Logic (CTL) and Alternating-time Temporal Logic (ATL), despite being extensively used in specifying and verifying distributed and multi-agent systems, have not received adequate attention. Thus, in this paper, we investigate the problem of learning CTL and ATL formulas from examples of system behavior. As input to the learning problems, we rely on the typical representations of branching behavior as Kripke structures and concurrent game structures, respectively. Given a sample of structures, we learn concise formulas by encoding the learning problem into a satisfiability problem, most notably by symbolically encoding both the search for prospective formulas and their fixed-point based model checking algorithms. We also study the decision problem of checking the existence of prospective ATL formulas for a given sample. We implement our algorithms in a Python prototype and have evaluated them to extract several common CTL and ATL formulas used in practical applications.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-75434-0_2,Certainty vs. Intelligence,"Mathematical models can yield certainty, as can probabilistic models where the probabilities degenerate. The field of formal methods emphasizes developing such certainty about engineering designs. In safety-critical systems, such certainty is highly valued and, in some cases, even required by regulatory bodies. But achieving reasonable performance for sufficiently complex environments appears to require the use of AI technologies, which resist such certainty. This paper suggests that certainty and intelligence may be fundamentally incompatible. First, Bayes Theorem shows, rather trivially, that certainty implies an inability to learn when presented with new data. A more subtle issue, however, is that logic and mathematics, necessary for certainty, may be a result of intelligence rather than the foundations of intelligence. This paper makes the case that intelligence is an evolved form of prediction, that logic and mathematics were not discovered but rather were invented because of their predictive value, and that the certainty they can give us cannot be about systems that exhibit intelligence.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-73039-9_7,Elucidating the Hierarchical Nature of Behavior with Masked Autoencoders,"Natural behavior is hierarchical. Yet, there is a paucity of benchmarks addressing this aspect. Recognizing the scarcity of large-scale hierarchical behavioral benchmarks, we create a novel synthetic basketball playing benchmark (Shot7M2). Beyond synthetic data, we extend BABEL into a hierarchical action segmentation benchmark (hBABEL). Then, we develop a masked autoencoder framework (hBehaveMAE) to elucidate the hierarchical nature of motion capture data in an unsupervised fashion. We find that hBehaveMAE learns interpretable latents on Shot7M2 and hBABEL, where lower encoder levels show a superior ability to represent fine-grained movements, while higher encoder levels capture complex actions and activities. Additionally, we evaluate hBehaveMAE on MABe22, a representation learning benchmark with short and long-term behavioral states. hBehaveMAE achieves state-of-the-art performance without domain-specific feature extraction. Together, these components synergistically contribute towards unveiling the hierarchical organization of natural behavior. Models and benchmarks are available at https://github.com/amathislab/BehaveMAE .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Image Processing', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems']"
Springer,10.1007/978-981-97-3752-9_4,Human-Level Knowledge and Concept Acquisition,"To increase productivity, it is expected that a single user is able to operate multiple cybernetic avatars (CAs). However, the limited attention span of the user makes it difficult to send direct instructions to all CAs. Therefore, this chapter describes the essential technologies for CAs that solve these problems and behave autonomously according to the user's intentions. First, the realization of spatio-temporal recognition capabilities that enable CAs to move autonomously in an environments that change from moment to moment is described. Following that, methods to implement continuous learning and memory mechanisms to facilitate acquired information reuse in the future are described. In general, the observed data are time series, and future predictions are important to provide appropriate support to users. The time series analysis method is then explained, which is the most important technology. Advanced natural language processing technology is necessary to capture intentions through dialogue with the user and to process large amounts of textual data as prior knowledge and common sense. Examples of the application of these fundamental technologies in the medical field are also presented.","['Computer Science', 'User Interfaces and Human Computer Interaction', 'Robotics', 'User Interfaces and Human Computer Interaction', 'Robotics']"
Springer,10.1007/978-3-031-61194-0_2,Computer and Information Literacy Framework,"The assessment of computer and information literacy (CIL) is at the core of the International Computer and Information Literacy Study (ICILS). In this chapter we define and describe the CIL construct that underpins the assessment used in ICILS. Computer and information literacy was first defined and described for use in ICILS 2013, and is reviewed at the beginning of each new ICILS cycle, with reference to developments in CIL-related research, policies and curriculums, and with respect to its operationalization in previous ICILS cycles. In the chapter we trace the history of the CIL construct from its origins in the second half of last century through to its contemporary instantiation as a combination of technological proficiency with facets of information literacy and communication. In ICILS, the term “Computer and Information Literacy” underscores the significance of internet-based information search and evaluation within the broader competency of utilizing contemporary technology. We continue with an explanation of the process of deriving the CIL construct definition and structure from preexisting and preeminent definitions of related constructs. We then describe the structure of the CIL constructs and elaborate on the content of the four strands divided into eight aspects that comprise the CIL construct.","['Education', 'Education, general', 'International and Comparative Education', 'Digital Education and Educational Technology', 'Assessment and Testing', 'International and Comparative Education']"
Springer,10.1007/978-3-031-71177-0_4,DFAMiner: Mining Minimal Separating DFAs from Labelled Samples,"We propose DFAMiner , a passive learning tool for learning minimal separating deterministic finite automata (DFA) from a set of labelled samples. Separating automata are an interesting class of automata that occurs generally in regular model checking and has raised interest in foundational questions of parity game solving. We first propose a simple and linear-time algorithm that incrementally constructs a three-valued DFA (3DFA) from a set of labelled samples given in the usual lexicographical order. This 3DFA has accepting and rejecting states as well as don’t-care states, so that it can exactly recognise the labelled examples. We then apply our tool to mining a minimal separating DFA for the labelled samples by minimising the constructed automata via a reduction to SAT solving. Empirical evaluation shows that our tool outperforms current state-of-the-art tools significantly on standard benchmarks for learning minimal separating DFAs from samples. Progress in the efficient construction of separating DFAs can also lead to finding the lower bound of parity game solving, where we show that DFAMiner can create optimal separating automata for simple languages with up to 7 colours. Future improvements might offer inroads to better data structures.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_16,On Completeness of SDP-Based Barrier Certificate Synthesis over Unbounded Domains,"Barrier certificates, serving as differential invariants that witness system safety, play a crucial role in the verification of cyber-physical systems (CPS). Prevailing computational methods for synthesizing barrier certificates are based on semidefinite programming (SDP) by exploiting Putinar Positivstellensatz . Consequently, these approaches are limited by the Archimedean condition , which requires all variables to be bounded, i.e., systems are defined over bounded domains. For systems over unbounded domains, unfortunately, existing methods become incomplete and may fail to identify potential barrier certificates. In this paper, we address this limitation for the unbounded cases. We first give a complete characterization of polynomial barrier certificates by using homogenization , a recent technique in the optimization community to reduce an unbounded optimization problem to a bounded one. Furthermore, motivated by this formulation, we introduce the definition of homogenized systems and propose a complete characterization of a family of non-polynomial barrier certificates with more expressive power. Experimental results demonstrate that our two approaches are more effective while maintaining a comparable level of efficiency.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_9,B2SAT: A Bare-Metal Reduction of B to SAT,"We present a new SAT backend for the B-Method to enable new applications of formal methods. The new backend interleaves low-level SAT solving with high-level constraint solving. It provides a “bare metal” access to SAT solving, while pre- and post-calculations can be done in the full B language, with access to higher-order or even infinite data values. The backend is integrated into ProB, not as a general purpose backend, but as a dedicated backend for solving hard constraint satisfaction and optimisation problems on complex data. In the article we present the approach, its origin in the proof of Cook’s theorem, and illustrate and evaluate it on a few novel applications of formal methods, ranging from biology to railway applications.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_18,CauMon: An Informative Online Monitor for Signal Temporal Logic,"In this paper, we present a tool for monitoring the traces of cyber-physical systems (CPS) at runtime, with respect to Signal Temporal Logic (STL) specifications. Our tool is based on the recent advances of causation monitoring , which reports not only whether an executing trace violates the specification, but also how relevant the increment of the trace at each instant is to the specification violation. In this way, it can deliver more information about system evolution than classic online robust monitors. Moreover, by adapting two dynamic programming strategies, our implementation significantly improves the efficiency of causation monitoring, allowing its deployment in practice. The tool is implemented as a C++ executable and can be easily adapted to monitor CPS in different formalisms. We evaluate the efficiency of the proposed monitoring tool, and demonstrate its superiority over existing robust monitors in terms of the information it can deliver about system evolution.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_25,Detecting Speculative Execution Vulnerabilities on Weak Memory Models,"Speculative execution attacks affect all modern processors and much work has been done to develop techniques for detection of associated vulnerabilities. Modern processors also operate on weak memory models which allow out-of-order execution of code. Despite this, there is little work on looking at the interplay between speculative execution and weak memory models. In this paper, we provide an information flow logic for detecting speculative execution vulnerabilities on weak memory models. The logic is general enough to be used with any modern processor, and designed to be extensible to allow detection of vulnerabilities to specific attacks. The logic has been proven sound with respect to an abstract model of speculative execution in Isabelle/HOL.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-64451-1_2,Multimodal Geolocation Estimation in News Documents,"With the proliferation of news documents on the Internet, online news reading has become an important approach for information acquisition in people’s daily lives. There has, however, been increasing concern with the growing infusion of misinformation. As a complement to news text, associated photos provide readers with additional information to facilitate their ability to find the information they need. To contextualise the vast amount of news that is published worldwide, the geographic content is crucial. On the other hand, the geographic content plays an important role in news recommendation to facilitate user desires. Existing approaches for geolocation estimation are primarily based on either text or photos as separate tasks. However, news photos can lack geographical cues, and text can include multiple locations. Therefore, it is challenging to recognise the focus location of the news story based on only one modality. We introduce novel datasets for multimodal geolocation estimation of news documents. We evaluate current methods on the benchmark datasets and suggest new methods for news geolocalisation using textual and visual content. In addition, we introduce a news retrieval system called GeoWINE based on the geographic content of news photos to emphasise the importance of geolocation estimation in the news domain.","['Computer Science', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Statistics, general', 'Natural Language Processing (NLP)', 'Digital/New Media', 'Data Science', 'Data Mining and Knowledge Discovery', 'Data Analysis and Big Data', 'Natural Language Processing (NLP)', 'Digital and New Media']"
Springer,10.1007/978-3-031-73741-1_6,From Data Science to Modular Workflows Changing Perspectives from Data to Platform: DBDIrl 1864-1922 Case Study,"Many historical data collections foot on handwritten documents and registers, whose consultation is often very difficult due to the conservation state of the physical artefacts, and whose comprehension is also made difficult by the handwriting, difficult to interpret, and the language used, different from the modern terminology. Therefore significant research efforts by historians, demographers, population health scientists and others have been started in the past with the aim of making such data collections digitally available, first on the basis of images and then as readily available repositories of transcribed data in electronically queryable formats. For the purpose of extracting data from the Irish Civil registers of deaths in the DBDIrl 1864-1922 project ( https://www.dbdirl.com ), an AI-ML Data Analytics Pipeline was proposed as a working approach validated on a subset of the data. However, the pipeline requires manual steps and it is not applicable as is on similar datasets without significant modifications to its inner workings. We are currently transforming this prototyped, single purpose product to a modular, fully automated workflow, intended to be used and reconfigured for new data in a low-code/no-code fashion by domain experts like historians. We explain our adopted analysis and refactoring process, illustrate it on part of the pipeline, including how we faced obstacles and handled pitfalls. We also evaluate its potential to become a methodical approach to transforming an interactive program to a fully automated process, in a low-code/no-code workflow style, that can be easily reused, reconfigured and extended to be able to tailor it to other datasets as needed.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-981-97-8727-2_2,Characteristics of Grey System Theory,"Characteristics of grey system theory will be analyzed based on analysis of general characteristics of uncertain systems. Then, the simplicity principle of sciences will be introduced. Similarities and differences of several uncertainty methods, such as probability statistics, fuzzy mathematics, grey system theory and rough set theory will be analyzed. At last of the chapter, The author will showcase successful applications of grey system theory in the field of natural sciences, social sciences and engineering technologies.","['Business and Management', 'Operations Research/Decision Theory', 'Applied Statistics', 'Operations Research, Management Science', 'Operations Research and Decision Theory', 'Applied Statistics', 'Operations Research, Management Science']"
Springer,10.1007/978-3-031-62634-0_14,Cognitive Mechanisms of Being Imitated,"Being mimicked (BeMim) arises when one person copies the actions or choices of another person, and several studies link BeMim to liking and affiliation. BeMim effects might occur for matching of motor actions but have also been reported for the imitation of preferences and values. In this chapter we discuss various approaches to studying BeMim, from live interactions to controlled methods in the lab and from virtual reality to observation studies. We suggest that the fundamental cognitive mechanism that support BeMim effects is still unknown and it is not yet clear if various BeMim paradigms tap the same cognitive mechanisms. Three possible neurocognitive models of BeMim are considered: a specialized BeMim model, a universal model which is domain general based on cognitive predictability and a social learning model. The latter seems to be the most promising based on the current evidence. We highlight the non-monotonic character of the BeMim effects—there may be a “sweet spot” where BeMim has positive consequences but too much or too little mimicry can mean that the mimicker’s action is judged negatively rather than positively. People also dislike mimickers if they have awareness of being mimicking by them. Finally, we discuss the gaps in the BeMim literature that need to be addressed to move the BeMim field forward.","['Psychology', 'Psychology, general', 'Neuropsychology', 'Clinical Psychology', 'Cognitive Psychology', 'Personality and Social Psychology', 'Behavioral Sciences and Psychology', 'Neuropsychology', 'Clinical Psychology', 'Cognitive Psychology', 'Social Psychology']"
Springer,10.1007/978-3-031-71162-6_24,cfaults: Model-Based Diagnosis for Fault Localization in C with Multiple Test Cases,"Debugging is one of the most time-consuming and expensive tasks in software development. Several formula-based fault localization (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs with multiple faults. This paper introduces a novel fault localization approach for C programs with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified MaxSAT formula. Consequently, our method guarantees consistency across observations and simplifies the fault localization procedure. Experimental results on two benchmark sets of C programs, TCAS and C-Pack-IPAs , show that CFaults is faster than other FBFL approaches like BugAssist and SNIPER . Moreover, CFaults only generates subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_27,Unifying Weak Memory Verification Using Potentials,"Concurrency verification for weak memory models is inherently complex. Several deductive techniques based on proof calculi have recently been developed, but these are typically tailored towards a single memory model through specialised assertions and associated proof rules. In this paper, we propose an extension to the logic $${\textsf{Piccolo}}$$ Piccolo to generalise reasoning across different memory models. $${\textsf{Piccolo}}$$ Piccolo is interpreted on the semantic domain of thread potentials . By deriving potentials from weak memory model states, we can define the validity of $${\textsf{Piccolo}}$$ Piccolo formulae for multiple memory models. We moreover propose unified proof rules for verification on top of $${\textsf{Piccolo}}$$ Piccolo . Once (a set of) such rules has been shown to be sound with respect to a memory model $${\textsf{MM}} $$ MM , all correctness proofs employing this rule set are valid for $${\textsf{MM}}$$ MM . We exemplify our approach on the memory models $${\textsf{SC}}$$ SC , $${\textsf{TSO}}$$ TSO and $${\textsf{SRA}}$$ SRA using the standard litmus tests Message-Passing and IRIW.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-3752-9_9,Cybernetic Avatars and Society,"Toward a future symbiotic society with Cybernetic Avatars (CAs), it is crucial to develop socially well-accepted CAs and to discuss legal, ethical, and socioeconomic issues to update social rules and norms. This chapter provides interdisciplinary discussions for these issues from the perspectives of technological and social sciences. First, we propose avatar social implementation guidelines and present studies that contribute to the development of socially well-accepted CAs. The second part of this chapter addresses the ethical and legal issues in installing CAs in society and discusses solutions for them.","['Computer Science', 'User Interfaces and Human Computer Interaction', 'Robotics', 'User Interfaces and Human Computer Interaction', 'Robotics']"
Springer,10.1007/978-3-031-71162-6_31,Sound and Complete Witnesses for Template-Based Verification of LTL Properties on Polynomial Programs,"We study the classical problem of verifying programs with respect to formal specifications given in the linear temporal logic (LTL). We first present novel sound and complete witnesses for LTL verification over imperative programs. Our witnesses are applicable to both verification (proving) and refutation (finding bugs) settings. We then consider LTL formulas in which atomic propositions can be polynomial constraints and turn our focus to polynomial arithmetic programs, i.e. programs in which every assignment and guard consists only of polynomial expressions. For this setting, we provide an efficient algorithm to automatically synthesize such LTL witnesses. Our synthesis procedure is both sound and semi-complete. Finally, we present experimental results demonstrating the effectiveness of our approach and that it can handle programs which were beyond the reach of previous state-of-the-art tools.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_18,Certified Quantization Strategy Synthesis for Neural Networks,"Quantization plays an important role in deploying neural networks on embedded, real-time systems with limited computing and storage resources (e.g., edge devices). It significantly reduces the model storage cost and improves inference efficiency by using fewer bits to represent the parameters. However, it was recently shown that critical properties may be broken after quantization, such as robustness and backdoor-freeness. In this work, we introduce the first method for synthesizing quantization strategies that verifiably maintain desired properties after quantization, leveraging a key insight that quantization leads to a data distribution shift in each layer. We propose to compute the preimage for each layer based on which the preceding layer is quantized, ensuring that the quantized reachable region of the preceding layer remains within the preimage. To tackle the challenge of computing the exact preimage, we propose an MILP-based method to compute its under-approximation. We implement our method into a tool Quadapter and demonstrate its effectiveness and efficiency by providing certified quantization that successfully preserves model robustness and backdoor-freeness.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-73741-1_8,Digitised Historical Sources and Non-digital Humanists: An Interdisciplinary Challenge?,"The digitisation of sources has opened new perspectives for humanities scholars. Digitisation allowed a larger access to sources, removing some financial and geographical limits, and the use of digital tools provided new perspectives for humanities scholars, who are able to read the sources differently. However, working with digitised sources also created new challenges that humanities scholars are not always equipped to overcome. The ‘MedIcal Literature and Communication about Child Health’ (MILC) project uses historical medical books for a non-specialist audience to analyse discourses on children’s health in England, France and Italy between 1850 and 1914. Despite being born a non-digital humanities project, with a focus on manual qualitative analysis and a combination of history and literature methods, it took a digital turn when using digitised sources, with issues of digitisation and Optical Character Recognition (OCR) among others. The team working on the project is composed of three humanities scholars, with limited computer science skills. This required us to find digital humanities and in general IT tools adapted to our skillset, and suited to our needs. These tools did not always fit all our needs, and often presented issues in terms of accessibility and compatibility with the general standards of digital humanities. Using examples from the issues faced by this project, and from the solutions found, this paper will argue that the challenges encountered by humanities scholars are interdisciplinary, not only because they overcome the traditional disciplinary boundaries inside the humanities, but also because they mirror challenges that computer scientists are working to solve. This paper will also argue that collaboration is a necessity which would benefit both humanities scholars and computer scientists in their work on the improvement and development of new tools, with the help of AI for example. Using the work done by a team of non-digital humanities scholars, it will argue that accessibility is a central issue in digital humanities and in the creation of IT tools, which needs to be addressed.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-71162-6_9,Understanding Synthesized Reactive Systems Through Invariants,"In many applications for which reactive synthesis is attractive, computed implementations need to have understandable behavior. While some existing synthesis approaches compute finite-state machines with a structure that supports their understandability, such approaches do not scale to specifications that can only be realized with a large number of states. Furthermore, asking the engineer to understand the internal structure of the implementation is unnecessary when only the behavior of the implementation is to be understood. In this paper, we present an approach to computing understandable safety invariants that every implementation satisfying a generalized reactivity(1) specification needs to fulfill. Together with the safety part of the specification, the invariants completely define which transitions between input and output proposition valuations any correct implementation can take. We apply the approach in two case studies and demonstrate that the computed invariants highlight the strategic decisions that implementations for the given specification need to make, which not only helps the system designer with understanding what the specification entails, but also supports specification debugging.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_33,A Tutorial on Stream-Based Monitoring,"Stream-based runtime monitoring frameworks are safety assurance tools that check the runtime behavior of a system against a formal specification. This tutorial provides a hands-on introduction to RTLola, a real-time monitoring toolkit for cyber-physical systems and networks. RTLola processes, evaluates, and aggregates streams of input data, such as sensor readings, and provides a real-time analysis in the form of comprehensive statistics and logical assessments of the system’s health. RTLola has been applied successfully in monitoring autonomous systems such as unmanned aircraft. The tutorial guides the reader through the development of a stream-based specification for an autonomous drone observing other flying objects in its flight path. Each tutorial section provides an intuitive introduction, highlighting useful language features and specification patterns, and gives a more in-depth explanation of technical details for the advanced reader. Finally, we discuss how runtime monitors generated from RTLola specifications can be integrated into a variety of systems and discuss different monitoring applications.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_14,State Matching and Multiple References in Adaptive Active Automata Learning,"Active automata learning (AAL) is a method to infer state machines by interacting with black-box systems. Adaptive AAL aims to reduce the sample complexity of AAL by incorporating domain specific knowledge in the form of (similar) reference models. Such reference models appear naturally when learning multiple versions or variants of a software system. In this paper, we present state matching, which allows flexible use of the structure of these reference models by the learner. State matching is the main ingredient of adaptive $$L^{\#}$$ L # , a novel framework for adaptive learning, built on top of $$L^{\#}$$ L # . Our empirical evaluation shows that adaptive $$L^{\#}$$ L # improves the state of the art by up to two orders of magnitude.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-73741-1_11,Challenges for AI in Healthcare Systems,"This paper overviews the challenges of using artificial intelligence (AI) methods when building healthcare systems, as discussed at the AIsola Conference in 2023. It focuses on the topics (i) medical data, (ii) decision support, (iii) software engineering for AI-based health systems, (iv) regulatory affairs as well as (v) privacy-preserving machine learning and highlights the importance and challenges involved when utilizing AI in healthcare systems.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-61194-0_4,Contextual Framework,"This chapter describes the contextual information collected during the International Computer and Information Literacy Study (ICILS) 2023 in order to aid understanding of variation in the primary outcome achievement measures of the study: students’ computer and information literacy (CIL) and computational thinking (CT). Both outcome measures may be potentially influenced by a given set of contextual information. We provide a classification of contextual factors that accords with the multilevel structure inherent in the process of student CIL/CT learning. The learning of individual students happens in the overlapping contexts of school learning and out-of-school learning, both of which are embedded in the context of the wider community that comprises local, national, supranational, and international contexts. We distinguish between four contexts: wider community, schools and classrooms, home environment and the individual. In addition, we consider the relationship of these contextual factors to the learning process (antecedents or processes). We also list the different kinds of variables that are collected via the different ICILS 2023 contextual instruments and briefly outline prior findings from educational research in order to explain why these variables are included in ICILS 2023.","['Education', 'Education, general', 'International and Comparative Education', 'Digital Education and Educational Technology', 'Assessment and Testing', 'International and Comparative Education']"
Springer,10.1007/978-3-031-71162-6_10,Combining Classical and Probabilistic Independence Reasoning to Verify the Security of Oblivious Algorithms,"We consider the problem of how to verify the security of probabilistic oblivious algorithms formally and systematically. Unfortunately, prior program logics fail to support a number of complexities that feature in the semantics and invariants needed to verify the security of many practical probabilistic oblivious algorithms. We propose an approach based on reasoning over perfectly oblivious approximations, using a program logic that combines both classical Hoare logic reasoning and probabilistic independence reasoning to support all the needed features. We formalise and prove our new logic sound in Isabelle/HOL and apply our approach to formally verify the security of several challenging case studies beyond the reach of prior methods for proving obliviousness.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_2,Rigorous Floating-Point Round-Off Error Analysis in PRECiSA 4.0,"Small round-off errors in safety-critical systems can lead to catastrophic consequences. In this context, determining if the result computed by a floating-point program is accurate enough with respect to its ideal real-number counterpart is essential. This paper presents PRECiSA 4.0, a tool that rigorously estimates the accumulated round-off error of a floating-point program. PRECiSA 4.0 combines static analysis, optimization techniques, and theorem proving to provide a modular approach for computing a provably correct round-off error estimation. PRECiSA 4.0 adds several features to previous versions of the tool that enhance its applicability and performance. These features include support for data collections such as lists, records, and tuples; support for recursion schemas; an updated floating-point formalization that closely characterizes the IEEE-754 standard; an efficient and modular analysis of function calls that improves the performances for large programs; and a new user interface integrated into Visual Studio Code.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71177-0_10,PyBDR: Set-Boundary Based Reachability Analysis Toolkit in Python,"We present PyBDR, a Python reachability analysis toolkit based on set-boundary analysis, which centralizes on widely-adopted set propagation techniques for formal verification, controller synthesis, state estimation, etc. It employs boundary analysis of initial sets to mitigate the wrapping effect during computations, thus improving the performance of reachability analysis algorithms without significantly increasing computational costs. Beyond offering various set representations such as polytopes and zonotopes, our toolkit particularly excels in interval arithmetic by extending operations to the tensor level, enabling efficient parallel interval arithmetic computation and unifying vector and matrix intervals into a single framework. Furthermore, it features symbolic computation of derivatives of arbitrary order and evaluates them as real or interval-valued functions, which is essential for approximating behaviours of nonlinear systems at specific time instants. Its modular architecture design offers a series of building blocks that facilitate the prototype development of reachability analysis algorithms. Comparative studies showcase its strengths in handling verification tasks with large initial sets or long time horizons. The toolkit is available at https://github.com/ASAG-ISCAS/PyBDR .","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-6095-4_1,Main Amendments of Chinese Intellectual Property Law,"Strengthening the protection of intellectual property plays the most important role in improving the system of property rights protection, and also serves as the biggest momentum to enhance our country's economic competitiveness.","['Law', 'Private International Law, International & Foreign Law, Comparative Law', 'IT Law, Media Law, Intellectual Property', 'History of China', 'Private International Law, International and Foreign Law, Comparative Law', 'IT Law, Media Law, Intellectual Property', 'History of China']"
Springer,10.1007/978-3-031-71177-0_29,Practical Deductive Verification of OCaml Programs,"In this paper, we provide a comprehensive, hands-on tutorial on how to apply deductive verification to programs written in OCaml . In particular, we show how one can use the GOSPEL specification language and the Cameleer tool to conduct mostly-automated verification on OCaml code. In our presentation, we focus on two main classes of programs: first, purely functional programs with no mutable state; then on imperative programs, where one can mix mutable state with subtle control-flow primitives, such as locally-defined exceptions.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-75387-9_11,Intersymbolic AI,"This perspective piece calls for the study of the new field of Intersymbolic AI , by which we mean the combination of symbolic AI , whose building blocks have inherent significance/meaning, with subsymbolic AI , whose entirety creates significance/effect despite the fact that individual building blocks escape meaning. Canonical kinds of symbolic AI are logic, games and planning. Canonical kinds of subsymbolic AI are (un)supervised machine and reinforcement learning. Intersymbolic AI interlinks the worlds of symbolic AI with its compositional symbolic significance and meaning and of subsymbolic AI with its summative significance or effect to enable culminations of insights from both worlds by going between and across symbolic AI insights with subsymbolic AI techniques that are being helped by symbolic AI principles. For example, Intersymbolic AI may start with symbolic AI to understand a dynamic system, continue with subsymbolic AI to learn its control, and end with symbolic AI to safely use the outcome of the learned subsymbolic AI controller in the dynamic system. The way Intersymbolic AI combines both symbolic and subsymbolic AI to increase the effectiveness of AI compared to either kind of AI alone is likened to the way that the combination of both conscious and subconscious thought increases the effectiveness of human thought compared to either kind of thought alone. Some successful contributions to the Intersymbolic AI paradigm are surveyed here but many more are considered possible by advancing Intersymbolic AI.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Mathematical Logic and Formal Languages', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Control Structures and Microprogramming', 'Software Engineering', 'Formal Languages and Automata Theory', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Control Structures and Microprogramming']"
Springer,10.1007/978-3-031-71162-6_20,Bridging Dimensions: Confident Reachability for High-Dimensional Controllers,"Autonomous systems are increasingly implemented using end-to-end learning-based controllers. Such controllers make decisions that are executed on the real system, with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions—especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional vision-based controller can be approximated with several low-dimensional controllers. To balance the approximation accuracy and verifiability of our low-dimensional controllers, we leverage the latest verification-aware knowledge distillation. Then, we inflate low-dimensional reachability results with statistical approximation errors, yielding a high-confidence reachability guarantee for the high-dimensional controller. We investigate two inflation techniques—based on trajectories and control actions—both of which show convincing performance in three OpenAI gym benchmarks.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-73229-4_4,Quality Assured: Rethinking Annotation Strategies in Imaging AI,"This paper does not describe a novel method. Instead, it studies an essential foundation for reliable benchmarking and ultimately real-world application of AI-based image analysis: generating high-quality reference annotations. Previous research has focused on crowdsourcing as a means of outsourcing annotations. However, little attention has so far been given to annotation companies, specifically regarding their internal quality assurance (QA) processes. Therefore, our aim is to evaluate the influence of QA employed by annotation companies on annotation quality and devise methodologies for maximizing data annotation efficacy. Based on a total of 57,648 instance segmented images obtained from a total of 924 annotators and 34 QA workers from four annotation companies and Amazon Mechanical Turk (MTurk), we derived the following insights: (1) Annotation companies perform better both in terms of quantity and quality compared to the widely used platform MTurk. (2) Annotation companies’ internal QA only provides marginal improvements, if any. However, improving labeling instructions instead of investing in QA can substantially boost annotation performance. (3) The benefit of internal QA depends on specific image characteristics. Our work could enable researchers to derive substantially more value from a fixed annotation budget and change the way annotation companies conduct internal QA.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
Springer,10.1007/978-3-031-73024-5_9,Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for Studying Species Evolution,"A central problem in biology is to understand how organisms evolve and adapt to their environment by acquiring variations in the observable characteristics or traits of species across the tree of life. With the growing availability of large-scale image repositories in biology and recent advances in generative modeling, there is an opportunity to accelerate the discovery of evolutionary traits automatically from images. Toward this goal, we introduce Phylo-Diffusion, a novel framework for conditioning diffusion models with phylogenetic knowledge represented in the form of HIERarchical Embeddings (HIER-Embeds). We also propose two new experiments for perturbing the embedding space of Phylo-Diffusion: trait masking and trait swapping, inspired by counterpart experiments of gene knockout and gene editing/swapping. Our work represents a novel methodological advance in generative modeling to structure the embedding space of diffusion models using tree-based knowledge. Our work also opens a new chapter of research in evolutionary biology by using generative models to visualize evolutionary changes directly from images. We empirically demonstrate the usefulness of Phylo-Diffusion in capturing meaningful trait variations for fishes and birds, revealing novel insights about the biological mechanisms of their evolution. (Model and code can be found at imageomics.github.io/phylo-diffusion )","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems']"
Springer,10.1007/978-3-031-71162-6_1,Adversarial Robustness Certification for Bayesian Neural Networks,"We study the problem of certifying the robustness of Bayesian neural networks (BNNs) to adversarial input perturbations. Specifically, we define two notions of robustness for BNNs in an adversarial setting: probabilistic robustness and decision robustness. The former deals with the probabilistic behaviour of the network, that is, it ensures robustness across different stochastic realisations of the network, while the latter provides guarantees for the overall (output) decision of the BNN. Although these robustness properties cannot be computed analytically, we present a unified computational framework for efficiently and formally bounding them. Our approach is based on weight interval sampling, integration and bound propagation techniques, and can be applied to BNNs with a large number of parameters independently of the (approximate) inference method employed to train the BNN. We evaluate the effectiveness of our method on tasks including airborne collision avoidance, medical imaging and autonomous driving, demonstrating that it can compute non-trivial guarantees on medium size images (i.e., over 16 thousand input parameters).","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-7447-0_7,"From Backwardness to a Great Tech Power, the Chinese Modernization of Science and Technology","This article explores the development mechanism, growth process and key achievements of the Chinese modernization of science and technology. Over the past 70 years since the founding of the People’s Republic of China, the Chinese modernization of science and technology experienced a transformation from rapidly catching up with developed countries to surpassing its competitors in innovation. This highlights that “innovation is the primary driving force for development”. What is also at play is the integrated result as well as interaction between three key capabilities (the capability of introducing technologies, the capability of technology imitation, and the capability of independent innovation of technologies) and two major effects (the scale effect of a giant market of China and the scale effect of the world market). As a result, China created its own and unique path of opening up to the outside world, catching up with and surpassing advanced countries at a faster pace in spite of its status as a latecomer and pursuing independent innovation. This is what makes China a technological pioneer in the world now instead of that poor and backward country. China has the world’s largest integrated and high-quality scientific and technological team, one of the world’s largest R&D expenditure, the world’s second largest digital economy and the world’s largest amount of IPR usage fee (import and export). As the largest grain producer, it also ranks among the top countries in terms of papers published in science and technology journals, invention patent application and high-tech export. As China’s scientific and technological contribution rate continues to rise, it is now an innovative power in the world; by 2025, China’s scientific and technological modernization will be at a new level, and this will further change the future world’s science and technology development landscape. When China becomes one of the world’s three major science and technology centers, it will make greater contribution to the development of science and technology in the world.","['Economics', 'Political Economy/Economic Systems', 'Public Policy', 'Political Economy and Economic Systems', 'Public Policy']"
Springer,10.1007/978-3-031-73741-1_10,The GraphBRAIN Framework for Knowledge Graph Management and Its Applications to Cultural Heritage,"The traditional record-based approach to the description of Cultural Heritage is nowadays obsolete. It is unable to properly handle complex descriptions and it cannot support advanced functions provided by Artificial Intelligence techniques for helping practitioners, scholars, researchers and end-users in carrying out their tasks. A graph-based, semantic approach is needed, such as that provided by Semantic Web solutions. Also, a ‘holistic’ description approach is needed, that includes and inter-connects all branches and types of Cultural Heritage, and that is not limited to describing just the formal metadata of cultural objects, but can deal with their content, physicality, context and lifecycle, as well. The GraphBRAIN framework and technology for Knowledge Graph management enforces all these ideas and enjoys improved efficiency, expressiveness, and flexibility thanks to the use of the LPG model for knowledge representation. This paper describes GraphBRAIN and its application to several Cultural Heritage-related fields, including digital libraries, archives and museums, history of computing, and tourism as a way to boost fruition of these items.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-71177-0_26,"No Risk, No Fun","The aim of this tutorial is to explain to the formal methods community the area of risk management and its most prominent concepts: the definition of risk, strategies for managing risk, the risk management cycle, and the role of ISO standards. For each of these concepts, I explain how formal methods relate and contribute, making risk management more accountable: systematic, transparent, and quantitative. I will also argue that viewing Formal Methods through the lens of risk management, and making the relevance of formal methods in risk analysis explicit, helps our community to better communicate the merits of formal methods to industry.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-6915-5_3,High-Resolution Plenoptic Sensing,"The high-resolution plenoptic sensing system can capture light field with a high level of detail and clarity. Resolution refers to the amount of detail that can be discerned in the reconstructed image or light field. The cutting edge of high-resolution plenoptic sensing is gigapixel plenoptic sensing, specifically refers to a system capable of capturing images with billions of pixels, resulting in extremely high-resolution light field.","['Computer Science', 'Image Processing and Computer Vision', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Materials Science, general', 'Computer Vision', 'Image Processing', 'Imaging Techniques']"
Springer,10.1007/978-3-031-78593-1_8,"3-2-3 Multi-AI Segmentation Framework: LoD-Based, Incremental Segmentation of 3D Scan Data Using Any 2D AI","In the age of spatial computing , computer vision is central, and efficient segmentation of 3D scan data becomes a fundamental task. Existing segmentation methods are often locked to specific AI models, lack level-of-detail (LoD) capabilities, and do not support efficient incremental segmentation. These limitations hinder their application to XR systems that integrate architectural and urban scales, which demand both at scale and detailed, up-to-date segmentation information, while leveraging limited local hardware in distributed computing environments. In this work, we present a novel framework that integrates multiple 2D AI through AI-agnostic 3D geometry feature fusion, ensuring spatial consistency while taking advantage of the rapid advancements in 2D AI models. Our framework performs LoD segmentation, enabling swift segmentation of downsampled geometry and full detail on needed segments. Additionally, it progressively builds a segmentation database, processing only newly added data, thereby avoiding point cloud reprocessing, a common limitation in previous methods. In our use case, our framework analyzed a public building based on three scans: a drone LiDAR capture of the exterior, a static LiDAR capture of a room, and a user-held RGB-D camera capture of a section of the room. Our approach provided a fast understanding of building volumes, room elements, and a fully detailed geometry of a requested object, a “ panel with good lighting and a view to a nearby building ”, to implement an XR activity. Our preliminary results are promising for applications in other urban and architectural contexts and point to further developments in our Geometric Data Inference AI as a cornerstone for deeper, more accurate Multi-AI integration.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Software Engineering/Programming and Operating Systems', 'Computer Applications', 'User Interfaces and Human Computer Interaction', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Software Engineering', 'Computer and Information Systems Applications', 'User Interfaces and Human Computer Interaction', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems']"
Springer,10.1007/978-981-97-6915-5_2,Plenoptic Sensing Systems,"Plenoptic sensing systems are a type of camera that can capture the multidimensional information of the light field, e.g., capture both the light rays’ intensities and directions. This allows for reconstructing the high-dimensional light fields and supporting various post-capture features such as depth perception, refocusing, synthetic aperture, etc. As the widely used CMOS-based image sensor can only record the intensity of light rays, conventional camera cannot directly capture the multidimensional light field. To address this challenge, a variety of plenoptic sensing systems with specialized hardware and algorithms have been proposed.","['Computer Science', 'Image Processing and Computer Vision', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Materials Science, general', 'Computer Vision', 'Image Processing', 'Imaging Techniques']"
Springer,10.1007/978-3-031-71162-6_3,A Local Search Algorithm for MaxSMT(LIA),"MaxSAT modulo theories (MaxSMT) is an important generalization of Satisfiability modulo theories (SMT) with various applications. In this paper, we focus on MaxSMT with the background theory of Linear Integer Arithmetic, denoted as MaxSMT(LIA). We design the first local search algorithm for MaxSMT(LIA) called PairLS, based on the following novel ideas. A novel operator called pairwise operator is proposed for integer variables. It extends the original local search operator by simultaneously operating on two variables, enriching the search space. Moreover, a compensation-based picking heuristic is proposed to determine and distinguish the pairwise operations. Experiments are conducted to evaluate our algorithm on massive benchmarks. The results show that our solver is competitive with state-of-the-art MaxSMT solvers. Furthermore, we also apply the pairwise operation to enhance the local search algorithm of SMT, which shows its extensibility.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-7447-0_8,The Great Leap Development and Outlook of China’s Scientific and Technological Strength (2000–2035),"As the ability to realize the national scientific and technological strategic goals, scientific and technological strength is not only an important part of the country’s comprehensive national strength, but also the basis for scientific and technological innovation to continuously improve the comprehensive national strength. In this work, I have carried out quantitative evaluations of the leapfrog development of China’s scientific and technological strength from 2000 to 2020. Transitioning from catch-up development to innovation-oriented development, from the world’s second phalanx to the first phalanx, from quantitative accumulation to qualitative leap, China’s science and technology has achieved historic leapfrog development. This fully demonstrates the political advantages of the Party’s overall leadership, the strategic advantages of the country’s innovation-driven development, the advantages of the new national system under the socialist market economic system, and the advantages of scientific and technological human resources. Based on China’s insistence on taking self-reliance and self-improvement in science and technology as the support of the national development strategies, as well as the advantages and favorable conditions of future development, policy suggestions have been put forward for the national scientific and technological innovation goals. This has been alongside additional measures for strengthening the national strategic scientific and technological strength and implementing the innovation-driven development strategies. We can look forward to the completion of the long-term goals of China’s science and technology in 2025 and 2035, that is, the strength of science and technology will jump sharply, and China will enter the forefront of the world’s innovative countries.","['Economics', 'Political Economy/Economic Systems', 'Public Policy', 'Political Economy and Economic Systems', 'Public Policy']"
Springer,10.1007/978-3-031-71162-6_12,Introducing SWIRL: An Intermediate Representation Language for Scientific Workflows,"In the ever-evolving landscape of scientific computing, properly supporting the modularity and complexity of modern scientific applications requires new approaches to workflow execution, like seamless interoperability between different workflow systems, distributed-by-design workflow models, and automatic optimisation of data movements. In order to address this need, this article introduces SWIRL, an intermediate representation language for scientific workflows. In contrast with other product-agnostic workflow languages, SWIRL is not designed for human interaction but to serve as a low-level compilation target for distributed workflow execution plans. The main advantages of SWIRL semantics are low-level primitives based on the send/receive programming model and a formal framework ensuring the consistency of the semantics and the specification of translating workflow models represented by Directed Acyclic Graphs (DAGs) into SWIRL workflow descriptions. Additionally, SWIRL offers rewriting rules designed to optimise execution traces, accompanied by corresponding equivalence. An open-source SWIRL compiler toolchain has been developed using the ANTLR Python3 bindings.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-73741-1_13,Future Opportunities for Systematic AI Support in Healthcare,"Artificial Intelligence (AI) holds transformative potential to revolutionize healthcare delivery and outcomes. However, the literature suggests that focusing solely on AI algorithms leads to low adoption rates. AI needs to be introduced systematically into healthcare. This paper builds on this approach and synthesizes existing literature and authors’ insights to critically examine the current landscape and future opportunities for systematic AI support in healthcare. The multifaceted applications of AI, ranging from disease prediction to personalized medicine, are explored with a focus on AI’s potential to optimize employee performance, alleviate healthcare staff burdens, and enhance patient care. However, challenges such as limited access to unbiased data sets, connectivity issues, and ethical concerns pose significant barriers to AI adoption in healthcare.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-74234-7_18,Approximate Distributed Monitoring Under Partial Synchrony: Balancing Speed & Accuracy,"In distributed systems with processes that do not share a global clock, partial synchrony is achieved by clock synchronization that guarantees bounded clock skew among all applications. Existing solutions for distributed runtime verification under partial synchrony against temporal logic specifications are exact but suffer from significant computational overhead. In this paper, we propose an approximate distributed monitoring algorithm for Signal Temporal Logic (STL) that mitigates this issue by abstracting away potential interleaving behaviors. This conservative abstraction enables a significant speedup of the distributed monitors, albeit with a tradeoff in accuracy. We address this tradeoff with a methodology that combines our approximate monitor with its exact counterpart, resulting in enhanced efficiency without sacrificing precision. We evaluate our approach with multiple experiments, showcasing its efficacy in both real-world applications and synthetic examples.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Mathematical Logic and Formal Languages', 'Logics and Meanings of Programs', 'Artificial Intelligence', 'Algorithms', 'Programming Languages, Compilers, Interpreters', 'Software Engineering', 'Formal Languages and Automata Theory', 'Computer Science Logic and Foundations of Programming', 'Artificial Intelligence', 'Algorithms', 'Compilers and Interpreters']"
Springer,10.1007/978-3-031-71177-0_25,Advancing Quantum Computing with Formal Methods,"This tutorial introduces quantum computing with a focus on the applicability of formal methods in this relatively new domain. We describe quantum circuits and convey an understanding of their inherent combinatorial nature and the exponential blow-up that makes them hard to analyze. Then, we show how weighted model counting (#SAT) can be used to solve hard analysis tasks for quantum circuits. This tutorial is aimed at everyone in the formal methods community with an interest in quantum computing. Familiarity with quantum computing is not required, but basic linear algebra knowledge (particularly matrix multiplication and basis vectors) is a prerequisite. The goal of the tutorial is to inspire the community to advance the development of quantum computing with formal methods.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-75434-0_28,Towards Verifying Robotic Systems Using Statistical Model Checking in STORM,"Robust autonomy and interaction of robots with their environment, even in rare or new situations, is an ultimate goal of robotics research. We settle on Statistical Model Checking (SMC) for the benefit of robustness of robot deliberation and base our implementation on STORM, one of the most performant and comprehensive open-source model checkers, so far lacking an SMC extension. The SMC extension introduced in this paper offers various statistical methods, from which the user can choose to find the best trade-off between accuracy of the result and runtime. We demonstrate the efficiency of our SMC implementation by comparing it to other state-of-the-art SMC tools on well-established benchmarks and on a robotics-related example. The results indicate that our implementation, which will be continuously extended in the future to improve support for robotics use cases, is performant enough to bridge the gap between robotic systems and model checking in industry.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-71177-0_22,Code-Level Safety Verification for Automated Driving: A Case Study,"The formal safety analysis of automated driving vehicles poses unique challenges due to their dynamic operating conditions and significant complexity. This paper presents a case study of applying formal safety verification to adaptive cruise controllers. Unlike the majority of existing verification approaches in the automotive domain, which only analyze (potentially imperfect) controller models, employ simulation to find counter-examples or use online monitors for runtime verification, our method verifies controllers at code level by utilizing bounded model checking. Verification is performed against an invariant set derived from formal specifications and an analytical model of the required behavior. For neural network controllers, we propose a scalable three-step decomposition, which additionally uses a neural network verifier. We show that both traditionally implemented as well as neural network controllers are verified within minutes. The dual focus on formal safety and implementation verification provides a comprehensive framework applicable to similar cyber-physical systems.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-72244-8_1,Belenios with Cast-as-Intended: Towards a Usable Interface,"In this work we consider Belenios-CaI, a protocol offering a cast-as-intended mechanism and building upon Belenios, a voting system used in about 7000 elections to date. We modify the design of Belenios-Cai from the user perspective without changing its core cryptographic mechanism. The goal is to increase its usability by letting the voter simply check whether two symbols are equal or different. We conducted a user-study among 165 participants in a research center to evaluate the usability of our implementation of Belenios-CaI. Since the cast-as-intended mechanism assumes that voters make some random choices, we also evaluate whether the choices made by voters are sufficiently “random” to provide verifiability and whether it could affect their privacy. The study shows that, for our population, Belenios-CaI is considered as usable with the random choices of the voters seeming sufficient for verifiability and privacy.","['Computer Science', 'Cryptology', 'Artificial Intelligence', 'Mobile and Network Security', 'Computer Communication Networks', 'Cryptology', 'Artificial Intelligence', 'Mobile and Network Security', 'Computer Communication Networks']"
Springer,10.1007/978-3-031-71162-6_32,The Opacity of Timed Automata,"Opacity serves as a critical security and confidentiality property, which concerns whether an intruder can unveil a system’s secret based on structural knowledge and observed behaviors. Opacity in timed systems presents greater complexity compared to untimed systems, and it has been established that opacity for timed automata is undecidable. However, the original proof cannot be applied to decide the opacity of one-clock timed automata directly. In this paper, we explore three types of opacity within timed automata: language-based timed opacity, initial-location timed opacity, and current-location timed opacity. We begin by formalizing these concepts and establishing transformation relations among them. Subsequently, we demonstrate the undecidability of the opacity problem for one-clock timed automata. Furthermore, we offer a constructive proof for the conjecture regarding the decidability of opacity for timed automata in discrete-time semantics. Additionally, we present a sufficient condition and a necessary condition for the decidability of opacity in specific subclasses of timed automata.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-71162-6_26,Staged Specification Logic for Verifying Higher-Order Imperative Programs,"Higher-order functions and imperative states are language features supported by many mainstream languages. Their combination is expressive and useful, but complicates specification and reasoning, due to the use of yet-to-be-instantiated function parameters. One inherent limitation of existing specification mechanisms is its reliance on only two stages  : an initial stage to denote the precondition at the start of the method and a final stage to capture the postcondition. Such two-stage specifications force abstract properties to be imposed on unknown function parameters, leading to less precise specifications for higher-order methods. To overcome this limitation, we introduce a novel extension to Hoare logic that supports multiple stages for a call-by-value higher-order language with ML-like local references. Multiple stages allow the behavior of unknown function-type parameters to be captured abstractly as uninterpreted relations; and can also model the repetitive behavior of each recursion as a separate stage. In this paper, we define our staged logic with its semantics, prove its soundness and develop a new automated higher-order verifier, called H eifer, for a core ML-like language.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-981-97-6915-5_1,Introduction to Plenoptic Imaging,"Plenoptic is derived from the Latin words plenus (“full”) + optic and was proposed by Edward Adelson in 1991. The plenoptic function is a seven-dimensional function describing the three-dimensional viewing position, two-dimensional visual angle, one-dimensional wavelength, and one-dimensional time of a light ray in space.","['Computer Science', 'Image Processing and Computer Vision', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Materials Science, general', 'Computer Vision', 'Image Processing', 'Imaging Techniques']"
Springer,10.1007/978-981-97-8943-6_8,Quantum Programming Without the Quantum Physics,"We propose a quantum programming paradigm where all data are familiar classical data, and the only non-classical element is a random number generator that can return results with negative probability. Currently, the vast majority of quantum programming languages instead work with quantum data types made up of qubits. The description of their behavior relies on heavy linear algebra and many interdependent concepts and intuitions from quantum physics, which takes dedicated study to understand. We demonstrate that the proposed view of quantum programming explains its central concepts and constraints in more accessible, computationally relevant terms. This is achieved by systematically reducing everything to the existence of that negative-probability random generator, avoiding mention of advanced physics. This makes quantum programming more accessible to programmers without a deep background in physics or linear algebra. The bulk of this paper is written with such an audience in mind. As a working vehicle, we lay out a simple quantum programming language under this paradigm, showing that not only can it express all quantum algorithms, it also naturally captures the semantics of measurement without ever mentioning qubits or collapse.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Programming Techniques', 'Theory of Computation', 'Artificial Intelligence', 'Compilers and Interpreters', 'Programming Techniques', 'Theory of Computation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-66708-4_1,Methodological Resilience Assessment of Smart Cyber Infrastructures,"The race for digitization created a real need to protect smart infrastructures. Environments are becoming highly connected and automated. Their growing complexity and connectivity make it hard to assure and assess their cyber resilience, i.e., protecting them from cyberattacks, failures, and errors. Traditional strategies for ensuring the cyber resilience of smart infrastructures suffer from a lack of holism. Indeed, since smart infrastructures are often structured in layers, traditional protection methods can lead to conflicting and competing goals. For instance, they may increase the resilience of specific layers at the expense of decreasing the performance of others. This chapter reviews existing methods aiming to address this problem. We focus on two leading methodological assessment families: quantitative and qualitative. The former includes numerical metrics to quantify and assist system-dependent decision-making processes. The latter builds upon symbolic modeling to offer a system-agnostic assessment. The chapter provides an in-depth exploration of quantitative and qualitative methodologies with significant potential to enhance the resilience of layered smart infrastructures. Our exploration covers classical technological aspects (e.g., cascading effects) and socio-technical factors (e.g., human-in-the-loop interaction).","['Computer Science', 'Systems and Data Security', 'Computer Communication Networks', 'Computer Applications', 'Software Engineering/Programming and Operating Systems', 'Data and Information Security', 'Computer Communication Networks', 'Computer and Information Systems Applications', 'Software Engineering']"
Springer,10.1007/978-3-031-73030-6_17,Lossy Image Compression with Foundation Diffusion Models,"Incorporating diffusion models in the image compression domain has the potential to produce realistic and detailed reconstructions, especially at extremely low bitrates. Previous methods focus on using diffusion models as expressive decoders robust to quantization errors in the conditioning signals. However, achieving competitive results in this manner requires costly training of the diffusion model and long inference times due to the iterative generative process. In this work we formulate the removal of quantization error as a denoising task, using diffusion to recover lost information in the transmitted image latent. Our approach allows us to perform less than 10% of the full diffusion generative process and requires no architectural changes to the diffusion model, enabling the use of foundation models as a strong prior without additional fine tuning of the backbone. Our proposed codec outperforms previous methods in quantitative realism metrics, and we verify that our reconstructions are qualitatively preferred by end users, even when other methods use twice the bitrate.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Signal, Image and Speech Processing', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Image Processing', 'Computer Communication Networks', 'User Interfaces and Human Computer Interaction', 'Machine Learning', 'Special Purpose and Application-Based Systems']"
Springer,10.1007/978-3-031-76821-7_11,"Serious Games Beyond Entertainment and Learning: An Evaluation Methodology for Assessing Awareness Raising, Empathy, and Social Change","Serious games have emerged as a powerful tool for achieving targeted outcomes beyond entertainment, such as learning or raising awareness about a topic. Considering their pervasiveness and wide adoption in the educational domain, traditional assessment approaches of these games have predominantly focused on their entertainment value and achievement of learning objectives. This paper proposes a comprehensive evaluation framework that goes beyond traditional dimensions to include aspects relevant to empathy raising and attitude change. The proposed framework has been validated through three user studies, assessing entertainment, historical awareness, empathy raising, and attitude change for three games, involving in total 98 high school students. Results from the studies are presented, as well as implications and lessons learned regarding the overall methodological approach, the evaluation instruments used, and the procedures followed.","['Computer Science', 'User Interfaces and Human Computer Interaction', 'User Interfaces and Human Computer Interaction']"
Springer,10.1007/978-3-031-71162-6_17,A Zonotopic Dempster-Shafer Approach to the Quantitative Verification of Neural Networks,"The reliability and usefulness of verification depend on the ability to represent appropriately the uncertainty. Most existing work on neural network verification relies on the hypothesis of either set-based or probabilistic information on the inputs. In this work, we rely on the framework of imprecise probabilities, specifically p-boxes, to propose a quantitative verification of ReLU neural networks, which can account for both probabilistic information and epistemic uncertainty on inputs. On classical benchmarks, including the ACAS Xu examples, we demonstrate that our approach improves the tradeoff between tightness and efficiency compared to related work on probabilistic network verification, while handling much more general classes of uncertainties on the inputs and providing fully guaranteed results.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Logics and Meanings of Programs', 'Special Purpose and Application-Based Systems', 'Professional Computing', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)', 'Software Engineering', 'Computer Science Logic and Foundations of Programming', 'Special Purpose and Application-Based Systems', 'Programming Language', 'Control Structures and Microprogramming', 'Natural Language Processing (NLP)']"
Springer,10.1007/978-3-031-73741-1_12,Towards a Multi-dimensional Health Data Analysis Framework,"Healthcare processes need to be streamlined to offer better healthcare services. Data analysis can be crucial in reducing costs, optimizing processes, and analyzing treatment effectiveness. However, data analysis in healthcare is complex due to the variety and complexity of patient data. This paper proposes a multi-dimensional comparative analysis method that offers healthcare professionals a lens to delve into healthcare datasets from various perspectives. The paper discusses the importance of comparative analysis in healthcare illustrated by two examples on how we can understand the pattern of comorbidity and how we can analyze the effectiveness of internet delivered psychological interventions. The paper presents a multi-dimensional comparative analysis framework covering various use cases in analysing healthcare data. The framework allows healthcare professionals to compare and contrast healthcare data across multiple dimensions, including clinical dimensions such as diagnosis, outcome measures, time dimension, patient dimensions (engagement, involvement), cost dimension, and other relevant factors. This approach offers a more insightful understanding of healthcare data and facilitates informed decision-making in healthcare practices.","['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/978-3-031-78590-0_7,Publishing and Long-Term Archiving 3D Data in Humanities,"We present here the French solution for long-term archiving combined with online publication of 3D research data in the humanities. The focus is on the paradata that document the technical process involved in obtaining the 3D result. Our schema, initially limited to the fields of archaeology and cultural heritage, is now open to other areas of the human sciences.  The choice of data organization, metadata, paradata, standards and infrastructure is in line with the FAIR principles of the semantic web. It is aligned with standard vocabularies and mapped to the Europeana Data Model (EDM). The CINES (Centre Informatique National de l’Enseignement Supérieur.), the Open Archival Information System (OAIS) infrastructure for research data in France, is in charge of archiving. We take care of data documentation and propose to publish part of these documented data at the same time. On the user side, we developed aLTAG3D, a desktop UI software to help research teams to create their OAIS Submission Information Package (SIP). On the publication side, we provide a DOI and a 3D viewer on the online plateform to meet the needs of researchers and public communication. On the archiving side, long-term archiving has given direction to the way our description schema works: it is focused on reproducibility. The content of the SIP is centered on the 3D data, its build process and sources. Paradata describing the process to the 3D file is under development and several options are under study with CIDOC CRM-Dig or W3C prov-O ontologies.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Information Storage and Retrieval', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Information Storage and Retrieval', 'Computer and Information Systems Applications']"
Springer,10.1007/978-3-031-65603-3_3,Civic Knowledge,"In this chapter we report on students’ civic knowledge achievement in the International Civic and Citizenship Education Study (ICCS). The chapter begins with a description of the test instrument used to assess civic knowledge. This is followed by an explanation of the development and content of the ICCS civic knowledge proficiency scale used to measure and describe students’ civic knowledge achievement. The student test and civic knowledge scale are illustrated using example items from each level. We then describe students’ civic knowledge achievement within and across ICCS participants. We first report on students’ civic knowledge in ICCS 2022 and trends in achievement across cycles. While civic knowledge varied across countries, a consistent finding reported across all first three cycles of ICCS was that the variation of civic knowledge within countries exceeded the variation across countries. Furthermore, we observed a pattern of increases in student civic knowledge between 2009 and 2016 followed by decreases between 2016 and 2022. We then report on the associations between aspects of students’ background and their civic knowledge achievement. Consistent with findings from pervious ICCS cycles, female students demonstrated higher civic knowledge than male students, and student socioeconomic status, reported across a range of measures, was positively associated with student civic knowledge.","['Education', 'Educational Policy and Politics', 'Education, general', 'Citizenship Education', 'Educational Policy and Politics', 'Assessment and Testing']"
Springer,10.1007/978-3-031-73741-1_19,From Explanation Correctness to Explanation Goodness: Only Provably Correct Explanations Can Save the World,Explainability Engineering gets evermore important in the era of self-learning and automated systems. We motivate the necessity for interdisciplinary research to engineer verifiably correct and good explanations: Systems engineering research must ensure that correct and machine-understandable explanations can be derived from system specifications and social sciences research must ensure that a context-dependent and stakeholder-tailored explanation can be provided in a fitting manner. We describe our first steps in the direction of a holistic and interdisciplinary explainability engineering process for tackling these challenges.,"['Computer Science', 'Logics and Meanings of Programs', 'Software Engineering/Programming and Operating Systems', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence', 'Computer Science Logic and Foundations of Programming', 'Software Engineering', 'Special Purpose and Application-Based Systems', 'Computer System Implementation', 'Artificial Intelligence']"
Springer,10.1007/s10013-023-00652-0,Computing the Non-properness Set of Real Polynomial Maps in the Plane,"We introduce novel mathematical and computational tools to develop a complete algorithm for computing the set of non-properness of polynomials maps in the plane. In particular, this set, which we call the Jelonek set , is a subset of $$\mathbb {K}^2$$ K 2 , where a dominant polynomial map $$f: \mathbb {K}^2 \rightarrow \mathbb {K}^2$$ f : K 2 → K 2 is not proper; $$\mathbb {K}$$ K could be either $$\mathbb {C}$$ C or $$\mathbb {R}$$ R . Unlike all the previously known approaches we make no assumptions on f whenever $$\mathbb {K} = \mathbb {R}$$ K = R ; this is the first algorithm with this property. The algorithm takes into account the Newton polytopes of the polynomials. As a byproduct we provide a finer representation of the set of non-properness as a union of semi-algebraic curves, that correspond to edges of the Newton polytopes, which is of independent interest. Finally, we present a precise Boolean complexity analysis of the algorithm and a prototype implementation in maple .","['Mathematics', 'Mathematics, general', 'Mathematics']"
Springer,10.1007/s10100-024-00923-2,Curriculum-based university course timetabling considering individual course of studies,"We consider a complex university timetabling problem arising in a four-year study program of teacher education where every student has to choose two subjects. Since any combination of two subjects is feasible, the goal of designing a collision-free timetable for every student seems to be unreachable. However, the task becomes more tractable because parallel groups are offered for most courses, i.e. sectioning of students takes place. Difficulties arise from the individual progress of students who often follow neither the prescribed term of each course nor the prescribed ordering of courses. Under these and other conditions, an optimized timetable can be determined by a multi-stage process, adjusted to the estimated student numbers and their past achievements. Some of the features encountered in this planning task were also part of the well-known ITC-2019 timetabling competition, while others constitute new aspects. After moving main lectures into a regular time grid with minimal changes concerning the previously existing plan, the task of finding a timetable for all lectures with parallel groups is modeled as an integer linear program. At a later time, students with their actual demands are allocated a non-overlapping set of courses that is relevant and feasible for their individual study situation. Besides the maximization of allocated courses, a fairness criterion is also invoked at this stage. Since both optimization tasks are prone to infeasibility, we introduce features to resolve this issue in practice.","['Business and Management', 'Operations Research/Decision Theory', 'Operations Research and Decision Theory', 'Operations Management', 'Operations Research, Management Science', 'Optimization']"
Springer,10.1007/s40534-024-00351-7,High-precision laser monitoring system with enhanced non-uniform scanning for railway safety,"The intrusion of obstacles onto railway tracks presents a significant threat to train safety, characterized by sudden and unpredictable occurrences. With China leading the world in high-speed rail mileage, ensuring railway security is paramount. The current laser monitoring technologies suffer from high false alarm rates and unreliable intrusion identification. This study addresses these issues by investigating high-resolution laser monitoring technology for railway obstacles, focusing on key parameters such as monitoring range and resolution. We propose an enhanced non-uniform laser scanning method, developing a laser monitoring system that reduces the obstacle false alarm rate to 2.00%, significantly lower than the 20% standard (TJ/GW135-2015). This rate is the best record for laser monitoring systems on China Railway. Our system operates seamlessly in all weather conditions, providing superior accuracy, resolution, and identification efficiency. It is the only 3D LiDAR system certified by the China State Railway Group Co., Ltd. (Certificate No. [2023] 008). Over three years, our system has been deployed at numerous points along various lines managed by the China State Railway Group, accumulating a dataset of 300,000 observations. This extensive deployment has significantly enhanced railway safety. The development and implementation of our railway laser monitoring system represent a substantial advancement in railway safety technology. Its low false alarm rate (2.00%), high accuracy (20 cm × 20 cm × 20 cm), and robust performance in diverse conditions underscore its potential for widespread adoption, promising to enhance railway safety in China and internationally.","['Engineering', 'Automotive Engineering', 'Geoengineering, Foundations, Hydraulics', 'Regional/Spatial Science', 'Automotive Engineering', 'Geoengineering', 'Regional and Spatial Economics']"
Springer,10.1007/s11071-024-10244-3,Novel efficient reservoir computing methodologies for regular and irregular time series classification,"Time series is a data structure prevalent in a wide range of fields such as healthcare, finance and meteorology. It goes without saying that analyzing time series data holds the key to gaining insight into our day-to-day observations. Among the vast spectrum of time series analysis, time series classification offers the unique opportunity to classify the sequences into their respective categories for the sake of automated detection. To this end, two types of mainstream approaches, recurrent neural networks and distance-based methods, have been commonly employed to address this specific problem. Despite their enormous success, methods like Long Short-Term Memory networks typically require high computational resources. It is largely as a consequence of the nature of backpropagation, driving the search for some backpropagation-free alternatives. Reservoir computing is an instance of recurrent neural networks that is known for its efficiency in processing time series sequences. Therefore, in this article, we will develop two reservoir computing based methods that can effectively deal with regular and irregular time series with minimal computational cost, both while achieving a desirable level of classification accuracy.","['Physics', 'Applications of Nonlinear Dynamics and Chaos Theory', 'Statistical Physics and Dynamical Systems', 'Classical Mechanics', 'Vibration, Dynamical Systems, Control', 'Multibody Systems and Mechanical Vibrations', 'Classical Mechanics', 'Mechanical Engineering', 'Applied and Technical Physics']"
Springer,10.1007/s40534-024-00355-3,Influence of trains meeting on the ventilation performance of equipment compartment with independent air duct in high-speed train: numerical and experimental study,"During the train meeting events, train equipment compartments are exposed to the worst pressure changes, potentially affecting the ventilation performance of equipment, particularly for electrical facilities equipped with independent air ducts. In this paper, a two-step method is used for numerical computation: (1) obtaining the temporal and spatial transient node data of the flow field sections during the train-passing simulation and (2) using the data as the input data for the equipment compartment simulation. In addition, this paper also compares the difference in equipment ventilation between the single-train and train-passing scenarios in real vehicle tests. The results indicate that the primary factors influencing ventilation effectiveness are the aerodynamic compression and deceleration of airflow induced by the other train’s nose, as well as the instability of the external flow field in the wake of the other train. During train crossing, the air is forced into the air duct, with a maximum ratio of the airflow in-duct to the airflow out-duct reaching 3.2. The average mass flow falls below the rated mass flow for the converter. Compared to the rated air volume of converter, the maximum suppression rates obtained from testing and simulation are – 24.5% and – 16.8%, respectively. Compared to the single-train operation, the maximum suppression rates obtained from testing and simulation are – 15% and – 18%, respectively. These findings provide valuable insights into the design and operation of high-speed trains.","['Engineering', 'Automotive Engineering', 'Geoengineering, Foundations, Hydraulics', 'Regional/Spatial Science', 'Automotive Engineering', 'Geoengineering', 'Regional and Spatial Economics']"
Springer,10.1007/s11071-024-09686-6,A novel method for response probability density of nonlinear stochastic dynamic systems,"This paper presents a novel method for analyzing high-dimensional nonlinear stochastic dynamic systems. In particular, we attempt to obtain the solution of the Fokker–Planck–Kolmogorov (FPK) equation governing the response probability density of the system without using the FPK equation directly. The method consists of several important components including the radial basis function neural networks (RBFNN), Feynman–Kac formula and the short-time Gaussian property of the response process. In the area of solving partial differential equations (PDEs) using neural networks, known as physics-informed neural network (PINN), the proposed method presents an effective alternative for obtaining solutions of PDEs without directly dealing with the equation, thus avoids evaluating the derivatives of the equation. This approach has a potential to make the neural network-based solution more efficient and accurate. Several highly challenging examples of nonlinear stochastic systems are presented in the paper to illustrate the effectiveness of the proposed method in comparison to the equation-based RBFNN approach.","['Physics', 'Applications of Nonlinear Dynamics and Chaos Theory', 'Statistical Physics and Dynamical Systems', 'Classical Mechanics', 'Vibration, Dynamical Systems, Control', 'Multibody Systems and Mechanical Vibrations', 'Classical Mechanics', 'Mechanical Engineering', 'Applied and Technical Physics']"
Springer,10.1007/s10100-024-00924-1,On the identity of two solution algorithms of the ‘improved normalized squared differences’ matrix adjustment model,"The paper is a supplement for an article recently published in this journal. That paper proved that if the sign-preservation requirement is dropped then the solution of the so-called improved normalized squared differences (INSD) two-directional matrix adjustment model is the same as the result of the ‘additive correction iteration algorithm’ which the author has been using successfully for decades. It also argued that if the sign-preservation requirement is dropped then the iteration procedure suggested by the authors of the INSD-model boils down to the same algorithm. Since the formal proof of this statement was not available at the time, in this paper the author duly publishes the three-and-a-half pages long proof he elaborated. In the conclusion the merit of this and similar, yet barely rigorously analysed iteration algorithms and the possible useful extensions are also outlined.","['Business and Management', 'Operations Research/Decision Theory', 'Operations Research and Decision Theory', 'Operations Management', 'Operations Research, Management Science', 'Optimization']"
Springer,10.1038/s41416-024-02916-z,A novel AI-based score for assessing the prognostic value of intra-epithelial lymphocytes in oral epithelial dysplasia,"Background Oral epithelial dysplasia (OED) poses a significant clinical challenge due to its potential for malignant transformation and the lack of reliable prognostic markers. Current OED grading systems do not reliably predict transformation and suffer from considerable observer variability. Recent studies have highlighted that peri-epithelial lymphocytes may play an important role in OED malignant transformation, with indication that intra-epithelial lymphocytes (IELs) may also be important. Methods We propose a novel artificial intelligence (AI) based IEL score from Haematoxylin and Eosin (H&E) stained Whole Slide Images (WSIs) of OED tissue slides. We determine the prognostic value of our IEL score on a digital dataset of 219 OED WSIs (acquired using three different scanners), compared to pathologist-led clinical grading. Results Our IEL scores demonstrated significant prognostic value (C-index = 0.67, p  < 0.001) and were shown to improve both the binary/WHO grading systems in multivariate analyses ( p  < 0.001). Nuclear analyses confirmed the positive association between higher IEL scores, more severe OED and malignant transformation ( p  < 0.05). Conclusions This underscores the potential importance of IELs, and by extension our IEL score, as prognostic indicators in OED. Further validation through prospective multi-centric studies is warranted to confirm the clinical utility of IELs.","['Biomedicine', 'Biomedicine, general', 'Cancer Research', 'Epidemiology', 'Molecular Medicine', 'Oncology', 'Drug Resistance', 'Biomedical Research', 'Cancer Biology', 'Epidemiology', 'Oncology', 'Medical Microbiology']"
Springer,10.1007/s11517-024-03202-z,Evaluating deep learning techniques for optimal neurons counting and characterization in complex neuronal cultures,"Abstract The counting and characterization of neurons in primary cultures have long been areas of significant scientific interest due to their multifaceted applications, ranging from neuronal viability assessment to the study of neuronal development. Traditional methods, often relying on fluorescence or colorimetric staining and manual segmentation, are time consuming, labor intensive, and prone to error, raising the need for the development of automated and reliable methods. This paper delves into the evaluation of three pivotal deep learning techniques: semantic segmentation, which allows for pixel-level classification and is solely suited for characterization; object detection, which focuses on counting and locating neurons; and instance segmentation, which amalgamates the features of the other two but employing more intricate structures. The goal of this research is to discern what technique or combination of those techniques yields the optimal results for automatic counting and characterization of neurons in images of neuronal cultures. Following rigorous experimentation, we conclude that instance segmentation stands out, providing superior outcomes for both challenges. Graphical abstract Identifying the optimal pathway for characterizing neurons in complex cultures through structured experimentation","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Biological Imaging', 'Computer and Information Systems Applications']"
Springer,10.1007/s10763-024-10465-3,Differences in Students’ Computational Thinking Activities when Designing an Algorithm for Drawing Plane Figures,"Computational thinking (CT) is becoming increasingly important as a learning content. Subject-integrated approaches aim to develop CT within other subjects like mathematics. The question is how exactly CT can be integrated and learned in mathematics classrooms. In a case study involving 12 sixth-grade learners, CT activities were explored that learners engage in when drawing geometric figures with a visual programming language. The empirical findings indicate differences not only across various CT activities but also within a single CT activity. Subcategories were developed to describe these differences. Implications for further research and classroom practice are discussed.","['Education', 'Mathematics Education', 'Science Education', 'Mathematics Education', 'Science Education']"
Springer,10.1007/s40299-024-00872-z,Advancing the In-Class Dialogic Quality: Developing an Artificial Intelligence-Supported Framework for Classroom Dialogue Analysis,"The development of artificial intelligence (AI) significantly improves the effectiveness of classroom dialogue systems, but their integration into the learning environment remains challenging. To address this gap, this research presents a framework for automatic intelligent dialogue analysis, intending to promote high-quality classroom dialogue and facilitate teaching and learning. The proposed framework includes two main components: a dialogue-oriented interactive classroom and an artificial intelligence-powered analysis system. We present a synthesis of essential principles that ought to be adhered to in the dialogue-oriented interactive classroom, as viewed through the lens of three key domains: the environment, the community and the teaching–learning. The AI system will analyse the dialogues generated from the interactive classroom. The utilization of feedback obtained from the AI system assists educators who adjust their pedagogical strategies, consequently improving the quality of classroom dialogues. Elevated-quality dialogues will reciprocally boost the performance of the AI system, engendering a sustainable improvement for the entire framework. Moreover, we also propose “Guide of AI”, a union of classroom participants and experts, which serves as the bridge between the classroom and technology to guide the operation of AI system. For the validation of the framework, we conduct an empirical study that mainly investigates the effectiveness of processed essential principles and AI systems. We select 6 pre-service teachers who are randomly divided into three groups. Three groups have different levels of involvement in AI system and each teacher gives three lessons. We record and analyse all teaching dialogue records and also use questionnaires to obtain teachers’ attitudes. The results show that timely feedback from AI system can promote the improvement of dialogue quality, which demonstrates the effectiveness of AI dialogue analysis system. In addition, the proposed essential principles also show a constructive impact.","['Education', 'Education, general', 'Learning and Instruction', 'Sociology of Education', 'Educational Policy and Politics', 'International and Comparative Education', 'Education', 'Instructional Psychology', 'Sociology of Education', 'Educational Policy and Politics', 'International and Comparative Education']"
Springer,10.1007/s11053-024-10423-4,Lateritic Ni–Co Prospectivity Modeling in Eastern Australia Using an Enhanced Generative Adversarial Network and Positive-Unlabeled Bagging,"The surging demand for Ni and Co, driven by the acceleration of clean energy transitions, has sparked interest in the Lachlan Orogen of New South Wales for its potential lateritic Ni–Co resources. Despite recent discoveries, a substantial knowledge gap exists in understanding the full scope of these critical metals in this geological province. This study employed a machine learning-based framework, integrating multidimensional datasets to create prospectivity maps for lateritic Ni–Co deposits within a specific Lachlan Orogen segment. The framework generated a variety of data-driven models incorporating geological (rock units, metamorphic facies), structural, and geophysical (magnetics, gravity, radiometrics, and remote sensing spectroscopy) data layers. These models ranged from comprehensive models that use all available data layers to fine-tuned models restricted to high-ranking features. Additionally, two hybrid (knowledge-data-driven) models distinguished between hypogene and supergene components of the lateritic Ni–Co mineral systems. The study implemented data augmentation methods and tackled imbalances in training samples using the SMOTE–GAN method, addressing common machine learning challenges with sparse training data. The study overcame difficulties in defining negative training samples by translating geological and geophysical data into training proxy layers and employing a positive and unlabeled bagging technique. The prospectivity maps revealed a robust spatial correlation between high probabilities and known mineral occurrences, projecting extensions from these sites and identifying potential greenfield areas for future exploration in the Lachlan Orogen. The high-accuracy models developed in this study utilizing the Random Forest classifier enhanced the understanding of mineralization processes and exploration potential in this promising region.","['Earth Sciences', 'Mineral Resources', 'Fossil Fuels (incl. Carbon Capture)', 'Geography, general', 'Sustainable Development', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Modeling and Industrial Mathematics', 'Mineralogy', 'Fossil Fuel', 'Geography', 'Sustainability', 'Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Modeling and Industrial Mathematics']"
Springer,10.1007/s11947-024-03491-0,Relationship of Cell Size Distribution and Biomechanics of Strawberry Fruit Under Varying Ca and N Supply,"Bruising due to compression of strawberry fruit is the major cause for fruit decay, resulting in food waste, which can be observed macroscopically as the result of texture failures. In the present study, laser light scattering density (LSD) analysis was applied to study the effect of cell size distribution percentages on fruit mechanics, considering three growth stages and added variance by foliar spray application of Ca, N, and Ca-N. The growth rate k considering cell size range 300–450 µm was enhanced in N compared to Ca treatment with 0.8 and 0.5, respectively. Comparison of cell sizes and mechanical fruit tissue properties reveals a strong effect of cell size on fruit mechanics. Based on cell size data, the particles in discrete element models (DEM) were established, applying a simple contact model of Hertz-Mindlin to test the effect of LSD data input, when simulating the compression peak force of strawberry tissue. Comparing measured compression data and DEM simulation, the mean square error was reduced, e.g., for Ca group of over-ripe fruit, from 9.6 to 6.5% when LSD percentages of cell size distribution were considered. Concluding, the newly available cell size distribution data provide valuable information on fruit growth and enable the simulation of fruit tissue compression under varying environmental growth conditions. The enhanced simulation accuracy of LSD-DEM approach makes the approach relevant for plant nutrition, developing robot harvesters, sorting devices, and shelf life assessment of fresh fruit.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Agriculture', 'Biotechnology', 'Food Science', 'Chemistry', 'Agriculture', 'Biotechnology']"
Springer,10.1007/s10763-024-10477-z,Research Trends in STEM Clubs: A Content Analysis,"To identify the research trends in studies related to STEM Clubs, 56 publications that met the inclusion and extraction criteria were identified from the online databases ERIC and WoS in this study. These studies were analysed by using the descriptive content analysis research method based on the Paper Classification Form (PCF), which includes publishing years, keywords, research methods, sample levels and sizes, data collection tools, data analysis methods, durations, purposes, and findings. The findings showed that, the keywords in the studies were used under six different categories: disciplines, technological concepts, academic community, learning experiences, core elements of education, and psychosocial factors (variables). Case studies were frequently employed, with middle school students serving as the main participants in sample groups ranging from 11–15, 16–20, and 201–250. Surveys, questionnaires, and observations were the primary methods of data collection, and descriptive analysis was commonly used for data analysis. STEM Clubs had sessions ranging from 2 to 16 weeks, with each session commonly lasting 60 to 120 min. The study purposes mainly focused on four themes: the impact of participation on various aspects such as attitudes towards STEM disciplines, career paths, STEM major selection, and academic achievement; the development and implementation of a sample STEM Club program, including challenges and limitations; the examination of students' experiences, perceptions, and factors influencing their involvement and choice of STEM majors; the identification of some aspects such as attitudinal effects and non-academic skills; and the comparison of STEM experiences between in-school and out-of-school settings. The study results mainly focused on three themes: the increase in various aspects such as academic achievement, STEM major choice, engagement in STEM clubs, identity, interest in STEM, collaboration-communication skills; the design of STEM Clubs, including sample implementations, design principles, challenges, and factors affecting their success and sustainability; and the identification of factors influencing participation, motivation, and barriers. Overall, this study provides a comprehensive understanding of STEM Clubs, leading the way for more targeted and informed future research endeavours.","['Education', 'Mathematics Education', 'Science Education', 'Mathematics Education', 'Science Education']"
Springer,10.1007/s11053-024-10432-3,Mineral Prospectivity Mapping and Differential Metal Endowment Between Two Greenstone Belts in the Canadian Superior Craton,"Mineral prospectivity maps were produced for gold in two greenstone belts in the Superior geological province in Ontario, Canada, as part of the Metal Earth Project in the Laurentian University, Sudbury, Ontario. These maps, created using the random forest machine learning algorithm, cover the well-endowed Matheson area, which is in the Abitibi sub-province, and the less fertile Dryden area, which is in the Wabigoon sub-province. Newly identified areas for follow-up gold exploration are associated with major faults and 3D geophysical data comprising resistivity, density and susceptibility data. In addition, observations not used in mineral prospectivity mapping based on magnetotelluric, seismic and isotopic data may in part describe why the Matheson greenstone belt is more fertile with respect to gold mineralization than the Dryden greenstone belt. These observations suggest that the Matheson area has major transcurrent faults associated with conductive zones that reach the surface, many of which are associated with deeply penetrating, vertical faults. The isotopic signature of the Matheson crust also suggests it is juvenile, whereas the Dryden area is older.","['Earth Sciences', 'Mineral Resources', 'Fossil Fuels (incl. Carbon Capture)', 'Geography, general', 'Sustainable Development', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Modeling and Industrial Mathematics', 'Mineralogy', 'Fossil Fuel', 'Geography', 'Sustainability', 'Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Modeling and Industrial Mathematics']"
Springer,10.1007/s11947-024-03539-1,"Investigation of Chemical Composition, Antioxidant Properties, and Molecular Docking in Different Roasting Stages of Coffee Beans","This study investigates the chemical composition and antioxidant properties of coffee beans at different roasting stages, namely green coffee, filter-roasted coffee, and espresso-roasted coffee. Using a Golden Roaster machine, specific roasting profiles were developed to achieve distinct flavor characteristics: an intense flavor and balanced acidity for espresso, and a balanced, complex taste for filter coffee. Results indicate that filter-roasted coffee exhibits the highest radical scavenging activity, as evidenced by its lowest IC_50 value for 2,2-diphenyl-1-picrylhydrazyl (DPPH) inhibition. Green coffee demonstrates superior iron chelation activity, while filter-roasted coffee contains the highest flavonol content and espresso-roasted coffee has the highest flavonoid content. Bacterial sensitivity tests show that both filter-roasted and espresso-roasted coffee are effective against certain strains, including Klebsiella pneumoniae ATCC 13883. Gas chromatography-mass spectrometry (GC–MS) analysis identifies key compounds such as caffeine and 4,4-dimethyl-3-(3-methylbut-3-enylidene)-2-methylenebicyclo [4.1.0] heptane in filter-roasted coffee, and 2-(2-hydroxyphenyl) buta-1,3-diene in espresso-roasted coffee. Molecular docking and in silico molecule’s absorption, distribution, metabolism, excretion, and toxicity (ADME) studies suggest potential pharmaceutical applications for coffee compounds. These findings provide valuable insights into coffee’s complex chemistry and its health-related properties. Additionally, the importance of coffee profiling in bioprocesses is highlighted by the need to carefully analyze the profiling process to optimize the biological effects and health benefits of these compounds. Coffee profiling not only enhances consumer taste experiences but also contributes to a better understanding of coffee’s potential health benefits by effectively identifying biomolecules and nutrients for use in bioprocesses. Graphical Abstract ","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Agriculture', 'Biotechnology', 'Food Science', 'Chemistry', 'Agriculture', 'Biotechnology']"
Springer,10.1007/s00222-024-01306-9,Dimension-free discretizations of the uniform norm by small product sets,"Let f $f$ be an analytic polynomial of degree at most K − 1 $K-1$ . A classical inequality of Bernstein compares the supremum norm of f $f$ over the unit circle to its supremum norm over the sampling set of the K $K$ -th roots of unity. Many extensions of this inequality exist, often understood under the umbrella of Marcinkiewicz–Zygmund-type inequalities for L p , 1 ≤ p ≤ ∞ $L^{p},1\le p\leq \infty $ norms. We study dimension-free extensions of these discretization inequalities in the high-dimension regime, where existing results construct sampling sets with cardinality growing with the total degree of the polynomial. In this work we show that dimension-free discretizations are possible with sampling sets whose cardinality is independent of deg ( f ) $\deg (f)$ and is instead governed by the maximum individual degree of f $f$ ; i.e. , the largest degree of f $f$ when viewed as a univariate polynomial in any coordinate. For example, we find that for n $n$ -variate analytic polynomials f $f$ of degree at most d $d$ and individual degree at most K − 1 $K-1$ , ∥ f ∥ L ∞ ( D n ) ≤ C ( X ) d ∥ f ∥ L ∞ ( X n ) $\|f\|_{L^{\infty }(\mathbf{D}^{n})}\leq C(X)^{d}\|f\|_{L^{\infty }(X^{n})}$ for any fixed X $X$ in the unit disc D $\mathbf{D}$ with | X | = K $|X|=K$ . The dependence on d $d$ in the constant is tight for such small sampling sets, which arise naturally for example when studying polynomials of bounded degree coming from functions on products of cyclic groups. As an application we obtain a proof of the cyclic group Bohnenblust–Hille inequality with an explicit constant O ( log K ) 2 d $\mathcal{O}(\log K)^{2d}$ .","['Mathematics', 'Mathematics, general', 'Mathematics']"
Springer,10.1007/s11517-024-03215-8,Generation of a virtual cohort of TAVI patients for in silico trials: a statistical shape and machine learning analysis,"Purpose In silico trials using computational modeling and simulations can complement clinical trials to improve the time-to-market of complex cardiovascular devices in humans. This study aims to investigate the significance of synthetic data in developing in silico trials for assessing the safety and efficacy of cardiovascular devices, focusing on bioprostheses designed for transcatheter aortic valve implantation (TAVI). Methods A statistical shape model (SSM) was employed to extract uncorrelated shape features from TAVI patients, enabling the augmentation of the original patient population into a clinically validated synthetic cohort. Machine learning techniques were utilized not only for risk stratification and classification but also for predicting the physiological variability within the original patient population. Results By randomly varying the statistical shape modes within a range of ± 2σ, a hundred virtual patients were generated, forming the synthetic cohort. Validation against the original patient population was conducted using morphological measurements. Support vector machine regression, based on selected shape modes (principal component scores), effectively predicted the peak pressure gradient across the stenosis ( R -squared of 0.551 and RMSE of 11.67 mmHg). Multilayer perceptron neural network accurately predicted the optimal device size for implantation with high sensitivity and specificity (AUC = 0.98). Conclusion The study highlights the potential of integrating computational predictions, advanced machine learning techniques, and synthetic data generation to improve predictive accuracy and assess TAVI-related outcomes through in silico trials. Graphical Abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Biological Imaging', 'Computer and Information Systems Applications']"
Springer,10.1007/s11947-024-03591-x,Synchronous Fluorescence Spectroscopy as a Green Method for the Prediction of Antioxidant Activity in Wine Brandy and Sweet Wine,"The official method for the determination of antioxidant activity in beverages is 2,2-diphenyl-1-picrylhydrazyl (DPPH) assay, which requires toxic reagents, is laborious, and produces waste. The aim of this work was to develop a more eco‑friendly method for the prediction of antioxidant activity in wine brandy and sweet wine using synchronous fluorescence spectra (SFS). In scanning of bulk and diluted samples, the excitation wavelength was varied from 250 to 500 nm and the wavelength interval was ranged from 20 to 100 nm. Partial least squares (PLS) regression was done on individual SFS, on unfolded SFS and on variables selected by the variable importance in the projection (VIP) algorithm, while the DPPH assay was the reference method. VIP-PLS modeling of the SFS of diluted samples led to better performance characteristics of the regression models. The best VIP-PLS model for wine brandy with relative predictive deviation (RPD) of 3.9 was based on 62 variables (the wavelength interval from 80 to 100 nm and the excitation wavelength from 290 to 320 nm). The best VIP-PLS model for sweet wine with RPD of 4.2 was calculated on 108 variables (the wavelength interval from 60 to 100 nm and the excitation wavelength from 260 to 290 nm). RPD values above 3.5 indicated very good prediction accuracy obtained by VIP-PLS models. Analytical GREEnness (AGREE) score 0.74 confirmed a high level of greenness of the proposed method.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Agriculture', 'Biotechnology', 'Food Science', 'Chemistry', 'Agriculture', 'Biotechnology']"
Springer,10.1007/s11947-024-03555-1,Lacticaseibacillus casei as Anti-blowing Agents: Impact on the Evolution of Ripening and Sensory Profile of Montasio Cheese,"Recently, the Lacticaseibacillus casei group strains have been gaining growing interest due to their potential to be used as secondary adjunct cultures in cheese. This work aimed to test autochthonous Lb. casei strains as anti-blowing agents and to evaluate their impact on the evolution of the ripening and the sensory profile of Montasio cheese. The cheesemaking trial included a control production without lysozyme (C1), a control with lysozyme (C2), and four experimental productions, each containing a different pool of autochthonous Lb. casei strains (EX1-4). Samples were taken during ripening, and physicochemical, microbiological, and sensory analyses were carried out. Lb. casei counts indicate that the selected strains survived the cheesemaking and maintained their viability of about 9 log cfu g^−1 at the end of the ripening. Only EX3 showed a significant slowdown of the proteolytic index compared to controls over time. Furthermore, from the principal component analysis, it emerged that at the end of the 120-day-long ripening, C2 and the experimental samples were characterized by similar profiles of volatile compounds. The late-blowing defect (LBD) was observed exclusively in C1, whereas it was not detected in the control sample with lysozyme or in any experimental samples. These findings not only confirmed the efficacy of lysozyme in preventing LBD but also supported the effectiveness of the selected Lb. casei strains as anti-blowing agents with the ability to contribute to the final volatile profile without compromising the typicality of the product.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Agriculture', 'Biotechnology', 'Food Science', 'Chemistry', 'Agriculture', 'Biotechnology']"
Springer,10.1007/s10443-024-10268-3,Efficient Approximation of Varying Fiber Orientation States in Injection Molded Parts Under Consideration of Multiple Manufacturing Uncertainties,"The production of high-quality fiber reinforced polymer parts is an important aspect in several industrial areas. However, due to unavoidable uncertainties in material and manufacturing processes, the part quality scatters. One important aspect here is the fiber orientation, being crucial for the thermo-mechanical properties of the part and being influenced by the uncertain material state and process conditions. Process simulations are an important tool for predicting the fiber orientation, but state-of-the-art simulations are normally deterministic and represent only one specific case. Performing enough deterministic simulations to model manufacturing uncertainties requires high numerical effort. Therefore, this work presents methods to quickly and efficiently approximate the fiber orientation under varying material and process parameters, requiring only a few simulations as input. Different schemes for approximation are evaluated and compared with each other and with 3D process simulations.","['Materials Science', 'Characterization and Evaluation of Materials', 'Classical Mechanics', 'Polymer Sciences', 'Industrial Chemistry/Chemical Engineering', 'Characterization and Analytical Technique', 'Classical Mechanics', 'Polymers', 'Industrial Chemistry']"
Springer,10.1007/s10750-024-05717-w,Invasive dynamics of the signal crayfish Pacifastacus leniusculus in a protected area,"Invasive species have been drivers of biodiversity loss and functional changes in aquatic ecosystems, including in protected areas. Therefore, monitoring population invasion dynamics and biological traits is fundamental to better understand their ecological and economic impacts and for management actions development. We followed signal crayfish ( Pacifastacus leniusculus ) invasion in Rabaçal River upper reach at Montesinho Natural Park, Portugal. We collected information on the spread and biological traits (abundance, size, weight, physical condition, sex ratio, and aggressiveness) to assess differences between invasion core and front areas and among years. Signal crayfish population remained restricted since first reports in 2013 in the invasion core until 2017. After 2019, signal crayfish population has been spreading downstream, decreasing abundance at invasion core but increasing at invasion front. Significant higher number of crayfish with claw loss indicate potential higher signs of aggressiveness in the invasion front. Results also demonstrate a significant dominance of females although sex ratio is closer to 1:1 at the invasion front. Overall, results indicate signal crayfish is spreading and increasing their abundance at Rabaçal River highlighting the need for immediate management actions to hold dispersion and mitigate possible impacts.","['Life Sciences', 'Freshwater & Marine Ecology', 'Ecology', 'Zoology', 'Freshwater and Marine Ecology', 'Ecology', 'Zoology']"
Springer,10.1007/s43440-024-00679-1,Assessing the effects of 5-HT_2A and 5-HT_5A receptor antagonists on DOI-induced head-twitch response in male rats using marker-less deep learning algorithms,"Background Serotonergic psychedelics, which display a high affinity and specificity for 5-HT_2A receptors like 2,5-dimethoxy-4-iodoamphetamine (DOI), reliably induce a head-twitch response in rodents characterized by paroxysmal, high-frequency head rotations. Traditionally, this behavior is manually counted by a trained observer. Although automation could simplify and facilitate data collection, current techniques require the surgical implantation of magnetic markers into the rodent’s skull or ear. Methods This study aimed to assess the feasibility of a marker-less workflow for detecting head-twitch responses using deep learning algorithms. High-speed videos were analyzed using the DeepLabCut neural network to track head movements, and the Simple Behavioral Analysis (SimBA) toolkit was employed to build models identifying specific head-twitch responses. Results In studying DOI (0.3125–2.5 mg/kg) effects, the deep learning algorithm workflow demonstrated a significant correlation with human observations. As expected, the preferential 5-HT_2A receptor antagonist ketanserin (0.625 mg/kg) attenuated DOI (1.25 mg/kg)-induced head-twitch responses. In contrast, the 5-HT_5A receptor antagonists SB 699,551 (3 and 10 mg/kg), and ASP 5736 (0.01 and 0.03 mg/kg) failed to do so. Conclusions Previous drug discrimination studies demonstrated that the 5-HT_5A receptor antagonists attenuated the interoceptive cue of a potent hallucinogen LSD, suggesting their anti-hallucinatory effects. Nonetheless, the present results were not surprising and support the head-twitch response as selective for 5-HT_2A and not 5-HT_5A receptor activation. We conclude that the DeepLabCut and SimBA toolkits offer a high level of objectivity and can accurately and efficiently identify compounds that induce or inhibit head-twitch responses, making them valuable tools for high-throughput research.","['Pharmacy', 'Pharmacy', 'Drug Safety and Pharmacovigilance', 'Pharmacotherapy', 'Pharmacy', 'Drug Safety and Pharmacovigilance', 'Therapeutics']"
Springer,10.1007/s11075-024-01809-9,Numerical solution of the boundary value problems for the biharmonic equations via quasiseparable representations,"The paper incorporates new methods of numerical linear algebra for the approximation of the biharmonic equation with potential, namely, numerical solution of the Dirichlet problem for $$ \left( \frac{d}{dx}\right) ^4u(x)+c(x)u(x)=\phi (x),\quad 0<x<1. $$ d dx 4 u ( x ) + c ( x ) u ( x ) = ϕ ( x ) , 0 < x < 1 . High-order discrete finite difference operators are presented, constructed on the basis of discrete Hermitian derivatives, and the associated Discrete Biharmonic Operator (DBO). It is shown that the matrices associated with the discrete operator belong to a class of quasiseparable matrices of low rank matrices. The application of quasiseparable representation of rank structured matrices yields fast and stable algorithm for variable potentials c ( x ). Numerical examples corroborate the claim of high order accuracy of the algorithm, with optimal complexity O ( N ).","['Computer Science', 'Numeric Computing', 'Algorithms', 'Algebra', 'Theory of Computation', 'Numerical Analysis', 'Numerical Analysis', 'Algorithms', 'Algebra', 'Theory of Computation']"
Springer,10.1007/s13369-024-09179-z,"Predicting Mechanical Properties in Geopolymer Mortars, Including Novel Precursor Combinations, Through XGBoost Method","Concrete is the most widely used material in the building industry due to its affordability, durability, and strength. However, considering carbon emissions, it is believed that concrete will be replaced by geopolymers in the future. As numerous parameters significantly affect the strength of geopolymers, the performance of potential algorithms for strength prediction needs to be evaluated for different binders to select an appropriate algorithm. This study employs machine learning approaches to provide the best prediction method for the flexural strength and compressive strength of geopolymers. A new dataset containing 533 compressive strength and 533 flexural strength values of geopolymers with different binders such as waste glass (GW), obsidian (OB), and fly ash was created. The best prediction solution, with R ^2 = 0.981 for compressive strength and R ^2 = 0.898 for flexural strength, was obtained from the extreme gradient boosting (XGBoost) algorithm. Additionally, several other machine learning models were employed, including linear regression, k-nearest neighbors, deep neural network, and random forest, with corresponding determination coefficient ( R ^2) values of 0.763, 0.804, 0.93, and 0.96, respectively. These models were trained and evaluated using a dataset encompassing features such as binder types, age, and heat, to forecast the mechanical properties of geopolymers. Among these models, XGBoost demonstrated the highest R ^2 value, indicating superior performance in predicting both compressive and flexural strengths. The findings of this study provide valuable insights into the selection of appropriate machine learning algorithms for predicting mechanical properties in geopolymers, thus contributing to advancements in sustainable construction materials.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary', 'Technology and Engineering', 'Humanities and Social Sciences']"
Springer,10.1007/s40299-024-00873-y,Changes in Learning Outcomes of Students Participating in Problem-Based Learning for the First Time: A Case Study of a Financial Management Course,"The aim of this study was to explore how first-time problem-based learning (PBL) participants can improve their learning outcomes. Empirical results showed that students with higher academic performance and attendance rates significantly enhanced their critical thinking and problem-solving skills through PBL compared to traditional lecture-based methods. However, PBL was less effective for students with lower academic performance or attendance rates compared to traditional lectures, highlighting the impact of student characteristics on PBL outcomes. Tutors should focus on students with lower academic or attendance rates, encouraging their active participation to improve overall learning outcomes. Results of path model analysis revealed that tutor performance significantly influences both the learning process and the development of critical thinking and problem-solving skills. Self-directed learning greatly influences critical thinking, while the functionality of tutorial groups significantly affects problem-solving skills. These findings emphasized the importance of tutor performance in enhancing the learning outcomes of students new to PBL. Therefore, educational institutions should invest in PBL teaching seminars to boost tutor performance and ultimately improve student learning outcomes.","['Education', 'Education, general', 'Learning and Instruction', 'Sociology of Education', 'Educational Policy and Politics', 'International and Comparative Education', 'Education', 'Instructional Psychology', 'Sociology of Education', 'Educational Policy and Politics', 'International and Comparative Education']"
Springer,10.1007/s11661-024-07667-3,Correction: Comparative Quantitative Analysis of the Evolution of Precipitates in Inconel 625 Superalloy Manufactured by Laser Powder Bed Fusion Subjected to High-Temperature Creep and Annealing,,"['Materials Science', 'Metallic Materials', 'Characterization and Evaluation of Materials', 'Structural Materials', 'Surfaces and Interfaces, Thin Films', 'Nanotechnology', 'Metals and Alloys', 'Characterization and Analytical Technique', 'Structural Materials', 'Surfaces, Interfaces and Thin Film', 'Nanotechnology']"
Springer,10.1038/s41380-024-02683-6,"Cognitive and psychiatric relevance of dynamic functional connectivity states in a large (N > 10,000) children population","Children’s brains dynamically adapt to the stimuli from the internal state and the external environment, allowing for changes in cognitive and mental behavior. In this work, we performed a large-scale analysis of dynamic functional connectivity (DFC) in children aged 9~11 years, investigating how brain dynamics relate to cognitive performance and mental health at an early age. A hybrid independent component analysis framework was applied to the Adolescent Brain Cognitive Development (ABCD) data containing 10,988 children. We combined a sliding-window approach with k-means clustering to identify five brain states with distinct DFC patterns. Interestingly, the occurrence of a strongly connected state with the most within-network synchrony and the anticorrelations between networks, especially between the sensory networks and between the cerebellum and other networks, was negatively correlated with cognitive performance and positively correlated with dimensional psychopathology in children. Meanwhile, opposite relationships were observed for a DFC state showing integration of sensory networks and antagonism between default-mode and sensorimotor networks but weak segregation of the cerebellum. The mediation analysis further showed that attention problems mediated the effect of DFC states on cognitive performance. This investigation unveils the neurological underpinnings of DFC states, which suggests that tracking the transient dynamic connectivity may help to characterize cognitive and mental problems in children and guide people to provide early intervention to buffer adverse influences.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s10346-024-02347-0,Stochastic hazard assessment framework of landslide blocking river by depth-integrated continuum method and random field theory,"Landslide-induced barrier dams pose a threat to the safety of humans, livestock and nearby infrastructures. The efficient assessment of landslide blocking river is crucial for disaster prevention and mitigation solutions. This study proposes a novel stochastic assessment framework to evaluate the landslide blocking river through the prediction of their deposition depths and considering the heterogeneity of shear strength parameters on the potential sliding surface. The depth-integrated continuum method (DICM) is used to simulate the landslide runout process. Using an enhanced Karhunen-Loève expansion (KLE) method, the spatial variations in soil's shear strength parameters are modeled by random fields to incorporate the effects of soil's spatial heterogeneity on the landslide deposition pattern. Subsequently, the multi-response surrogate model is constructed to relate the random field variables to the deposition depths based on extreme gradient boosting (XGBoost). To improve the performance of the surrogate model, principal component analysis (PCA) and sliced inverse regression (SIR) methods are employed for the dimension reduction of output and input variables, respectively. Furthermore, the algorithm for river blockage identification is developed to search for the deposition ridges. To demonstrate the capability of the stochastic assessment framework, an example of the first Baige landslide in Tibet, China is simulated, and the affected region and deposition depths of the landslide are predicted to calculate the probability of river damming. The presented methodology provides a practical means for improving the landslide blocking river prediction and new insights for early warning and risk mitigation.","['Earth Sciences', 'Natural Hazards', 'Geography, general', 'Agriculture', 'Civil Engineering', 'Natural Hazards', 'Geography', 'Agriculture', 'Civil Engineering']"
Springer,10.1007/s11517-024-03199-5,Cancer-on-chip: a breakthrough organ-on-a-chip technology in cancer cell modeling,"Cancer remains one of the leading causes of death worldwide. The unclear molecular mechanisms and complex in vivo microenvironment of tumors make it difficult to clarify the nature of cancer and develop effective treatments. Therefore, the development of new methods to effectively treat cancer is urgently needed and of great importance. Organ-on-a-chip (OoC) systems could be the breakthrough technology sought by the pharmaceutical industry to address ever-increasing research and development costs. The past decade has seen significant advances in the spatial modeling of cancer therapeutics related to OoC technology, improving physiological exposition criteria. This article aims to summarize the latest achievements and research results of cancer cell treatment simulated in a 3D microenvironment using OoC technology. To this end, we will first discuss the OoC system in detail and then demonstrate the latest findings of the cancer cell treatment study by Ooc and how this technique can potentially optimize better modeling of the tumor. The prospects of OoC systems in the treatment of cancer cells and their advantages and limitations are also among the other points discussed in this study. Graphical Abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Biological Imaging', 'Computer and Information Systems Applications']"
Springer,10.1007/s00125-024-06318-x,The metabolic and circadian signatures of gestational diabetes in the postpartum period characterised using multiple wearable devices,"Aims/hypothesis Gestational diabetes mellitus (GDM) affects 14% of all pregnancies worldwide and is associated with cardiometabolic risk. We aimed to exploit high-resolution wearable device time-series data to create a fine-grained physiological characterisation of the postpartum GDM state in free-living conditions, including clinical variables, daily glucose dynamics, food and drink consumption, physical activity, sleep patterns and heart rate. Methods In a prospective observational study, we employed continuous glucose monitors (CGMs), a smartphone food diary, triaxial accelerometers and heart rate and heart rate variability monitors over a 2 week period to compare women who had GDM in the previous pregnancy (GDM group) and women who had a pregnancy with normal glucose metabolism (non-GDM group) at 1–2 months after delivery (baseline) and 6 months later (follow-up). We integrated CGM data with ingestion events recorded with the smartphone app MyFoodRepo to quantify the rapidity of returning to preprandial glucose levels after meal consumption. We inferred the properties of the underlying 24 h rhythm in the baseline glucose. Aggregating the baseline and follow-up data in a linear mixed model, we quantified the relationships between glycaemic variables and wearable device-derived markers of circadian timing. Results Compared with the non-GDM group ( n =15), the GDM group ( n =22, including five with prediabetes defined based on fasting plasma glucose [5.6–6.9 mmol/l (100–125 mg/dl)] and/or HbA_1c [39–47 mmol/mol (5.7–6.4%)]) had a higher BMI, HbA_1c and mean amplitude of glycaemic excursion at baseline (all p ≤0.05). Integrating CGM data and ingestion events showed that the GDM group had a slower postprandial glucose decrease ( p =0.01) despite having a lower proportion of carbohydrate intake, similar mean glucose levels and a reduced amplitude of the underlying glucose 24 h rhythm ( p =0.005). Differences in CGM-derived variables persisted when the five women with prediabetes were removed from the comparison. Longitudinal analysis from baseline to follow-up showed a significant increase in fasting plasma glucose across both groups. The CGM-derived metrics showed no differences from baseline to follow-up. Late circadian timing (i.e. sleep midpoint, eating midpoint and peak time of heart rate) was correlated with higher fasting plasma glucose and reduced amplitudes of the underlying glucose 24 h rhythm (all p ≤0.05). Conclusions/interpretation We reveal GDM-related postpartum differences in glucose variability and 24 h rhythms, even among women clinically considered to be normoglycaemic. Our results provide a rationale for future interventions aimed at improving glucose variability and encouraging earlier daily behavioural patterns to mitigate the long-term cardiometabolic risk of GDM. Trial registration ClinicalTrials.gov no. NCT04642534 Graphical Abstract ","['Medicine & Public Health', 'Internal Medicine', 'Metabolic Diseases', 'Human Physiology', 'Internal Medicine', 'Diseases', 'Human Physiology']"
Springer,10.1007/s12119-024-10270-8,Correlates of the Short Form Love Attitudes Scale among Portuguese People,"In this paper, we analyze the psychometric characteristics of the Love Attitudes Scale: Short Form (LAS-SF), and the relationship between love style and other romantic relationships constructs in a Portuguese population. The LAS-SF measures six love styles: Eros, Ludus, Storge, Pragma, Mania and Agape, which are grounded on Lee’s theory. This tool is one of the most utilized assessments of love in the literature. There were 1153 Portuguese participants (554 women, mean age = 38 years). Confirmatory factor analyses evidenced that the six latent dimensions of the LAS-SF confirmed an acceptable fit to the data. The internal consistency of the Portuguese form of the LAS-SF was evaluated utilizing Cronbach’s Alpha (for subscales 0.71 to 0.78) and McDonald’s Omega (for subscales 0.71 to 0.79). Eros was positively related to satisfaction with love life, satisfaction with sex life, sexual desire, commitment, and negatively associated with romantic loneliness. Ludic orientation was positively correlated with romantic loneliness and negatively correlated with commitment. Storgic orientation was positively related to commitment. Pragma was positively related to romantic loneliness. Mania was positively related to commitment. Agape was positively correlated with satisfaction with love life and commitment, and negatively correlated with romantic loneliness. Notably, commitment mediated the relationship of the agapic love style and romantic loneliness.","['Social Sciences', 'Social Sciences, general', 'Personality and Social Psychology', 'Psychology, general', 'Regional and Cultural Studies', 'Personality and Differential Psychology', 'Behavioral Sciences and Psychology', 'Regional Cultural Studies']"
Springer,10.1007/s12239-024-00129-0,Local Reinforcement of a Fuel Cell End Plate for Package Improvements Using Steel–Aluminium Hybrid-Casting Technology,"In this research work, a method for integrating a local reinforcement structure in a medium-pressure plate (MPP) for fuel cell electric vehicle (FCEV) applications was developed using steel–aluminium hybrid-casting technology. Using this technology, it is possible to create a bonded enclosure of a steel reinforcement patch with the cast aluminium pressure plate to increase its stiffness and achieve 15% package space savings. A load-compliant, manufacturable patch was chosen and optimised for maximum stiffness gains using non-linear static finite-element (FE) calculations. Special form and process requirements due to hybrid-casting technology were examined and secured with casting simulations. The reinforcement patch was manufactured and coated with a unique aluminium–silicon coating enabling a ductile material connection between the steel and aluminium, and casting trials were conducted to create prototypes. Additionally, the insulating plastic layer on top of the metallic pressure plate carrier was substituted from costly short-fibre-reinforced high-performance plastic to cheaper and stiffer glass-mat reinforced thermoplastic material. Finally, the new hybrid MPP was tested mechanically, and the FE-Model was verified. In summary, through the package gain, 2.1 kW more power output and 11% less weight could be achieved while remaining stiffness neutral.","['Engineering', 'Automotive Engineering', 'Automotive Engineering']"
Springer,10.1007/s11224-024-02377-3,"Molecular insights into genistein-NSAID hybrids—synthesis, characterisation and DFT study","Genistein (GEN) is one of the pharmaceutically valuable phenolic compounds, which belongs to the isoflavone group of flavonoids and is a natural phytohormone found mainly in soybeans and red clover. It affects estrogen receptors, functioning as a selective estrogen receptor modulator (SERM) with anti-inflammatory and antioxidant activity. The presence of reactive phenolic groups in genistein provides an opportunity to expand its structure by introducing components responsible for anti-inflammatory properties. Such an innovative combination of a compound with anticancer and antioxidant potential with an anti-inflammatory compound (NSAID) may lead to interesting new derivatives with dual mechanisms of biological action. The synthesis and characterisation of genistein-NSAID hybrid compounds (ibuprofen, ketoprofen, naproxen, flurbiprofen) was conducted, together with a comprehensive structural and quantum chemistry DFT (density functional theory) computational analysis allowing the description of ^1H-NMR and ^13C-NMR spectroscopic properties of the starting compounds and the resulting hybrids. The study resulted in the formation of seven hybrid GEN-NSAID derivatives. In the case of ibuprofen, ketoprofen and flurbiprofen, a mixture of isomeric hybrid GEN-4’-NSAID and GEN-7-NSAID derivatives was obtained, whereas, for naproxen, only GEN-4’-NSAID was formed. The structural characteristics of the resulting compounds were determined using MS, IR, ^1H-NMR and ^13C-NMR spectroscopic methods. The most accurate DFT computational methods for predicting ^1H-NMR and ^13C-NMR spectra were also established with statistical parameters to assess their accuracy.","['Chemistry', 'Computer Applications in Chemistry', 'Physical Chemistry', 'Theoretical and Computational Chemistry', 'Computational Chemistry', 'Physical Chemistry', 'Theoretical Chemistry']"
Springer,10.1007/s40299-024-00859-w,"Early Childhood Visual Arts Education: Teachers’ Content Knowledge, Pedagogical Content Knowledge, and Challenges","In the past, visual arts education in Hong Kong was not considered an important area of early childhood education. While the Hong Kong kindergarten curriculum has recently been updated to encourage creativity, there remains a lack of adequate visual arts education for young children. This deficiency stems from the fact that the visual arts receive minimal attention within Hong Kong teacher education programs. Little research has been conducted on how visual arts education is actually delivered in local kindergarten classrooms in Hong Kong and what kinds of artistic knowledge and skills kindergarten teachers need. Therefore, this study aimed to investigate kindergarten teachers’ content knowledge and pedagogical content knowledge in early visual arts education (EVAE) and to identify the challenges they faced in teaching visual arts to children. The study surveyed 342 in-service kindergarten teachers in Hong Kong and conducted individual interviews with 12 participants. The findings revealed that Hong Kong kindergarten teachers generally performed well in terms of their pedagogical content knowledge, but they lacked content knowledge in various forms of early visual arts (EVA) and faced challenges in teaching visual arts effectively. This study has the potential to change how early childhood visual arts teaching is conceptualized and taught in Hong Kong and other Asian regions.","['Education', 'Education, general', 'Learning and Instruction', 'Sociology of Education', 'Educational Policy and Politics', 'International and Comparative Education', 'Education', 'Instructional Psychology', 'Sociology of Education', 'Educational Policy and Politics', 'International and Comparative Education']"
Springer,10.1038/s41386-024-02018-7,A sleepy cannabis constituent: cannabinol and its active metabolite influence sleep architecture in rats,"Medicinal cannabis is being used worldwide and there is increasing use of novel cannabis products in the community. Cannabis contains the major cannabinoids, Δ^9-tetrahydrocannabinol (Δ^9-THC) and cannabidiol (CBD), but also an array of minor cannabinoids that have undergone much less pharmacological characterization. Cannabinol (CBN) is a minor cannabinoid used in the community in “isolate’ products and is claimed to have pro-sleep effects comparable to conventional sleep medications. However, no study has yet examined whether it impacts sleep architecture using objective sleep measures. The effects of CBN on sleep in rats using polysomnography were therefore examined. CBN increased total sleep time, although there was evidence of biphasic effects with initial sleep suppression before a dramatic increase in sleep. CBN increased both non-rapid eye movement (NREM) and rapid eye movement (REM) sleep. The magnitude of the effect of CBN on NREM was comparable to the sleep aid zolpidem, although, unlike CBN, zolpidem did not influence REM sleep. Following CBN dosing, 11-hydroxy-CBN, a primary metabolite of CBN surprisingly attained equivalently high brain concentrations to CBN. 11-hydroxy-CBN was active at cannabinoid CB_1 receptors with comparable potency and efficacy to Δ^9-THC, however, CBN had much lower activity. We then discovered that the metabolite 11-hydroxy-CBN also influenced sleep architecture, albeit with some subtle differences from CBN itself. This study shows CBN affects sleep using objective sleep measures and suggests an active metabolite may contribute to its hypnotic action.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s11224-024-02395-1,Aromatic terminology. Highlighting the keywords polycyclic aromatic hydrocarbons (PAHs) and polycyclic aromatic compounds (PACs),"The present Review on aromatic terminology was prompted by a recent series of articles exploring the chemical space of polycyclic aromatic systems and introducing new related keywords. The keywords polycyclic aromatic hydrocarbons (PAHs) and polycyclic aromatic compounds (PACs) are highlighted. Keywords and other entities relevant to PAHs and PACs are outlined. The article calls the community of polycyclic aromatic hydrocarbons and polycyclic aromatic compounds to use these two leading keywords and their acronyms PAHs and PACs and the accompanying keyword benzenoid hydrocarbons, but to refrain from using and coining related new and previously proposed similar general keywords. This recommendation does not rule out keywords combining ‘polycyclic aromatic’ and a specific functionality, e.g., polycyclic aromatic ketones.","['Chemistry', 'Computer Applications in Chemistry', 'Physical Chemistry', 'Theoretical and Computational Chemistry', 'Computational Chemistry', 'Physical Chemistry', 'Theoretical Chemistry']"
Springer,10.1038/s41380-024-02651-0,Single-cell spatial transcriptomics reveals distinct patterns of dysregulation in non-neuronal and neuronal cells induced by the Trem2^R47H Alzheimer’s risk gene mutation,"The R47H missense mutation of the TREM2 gene is a known risk factor for development of Alzheimer’s Disease. In this study, we analyze the impact of the Trem2 ^R47H mutation on specific cell types in multiple cortical and subcortical brain regions in the context of wild-type and 5xFAD mouse background. We profile 19 mouse brain sections consisting of wild-type, Trem2 ^R47H, 5xFAD and Trem2 ^R47H; 5xFAD genotypes using MERFISH spatial transcriptomics, a technique that enables subcellular profiling of spatial gene expression. Spatial transcriptomics and neuropathology data are analyzed using our custom pipeline to identify plaque and Trem2 ^R47H-induced transcriptomic dysregulation. We initially analyze cell type-specific transcriptomic alterations induced by plaque proximity. Next, we analyze spatial distributions of disease associated microglia and astrocytes, and how they vary between 5xFAD and Trem2 ^R47H; 5xFAD mouse models. Finally, we analyze the impact of the Trem2 ^R47H mutation on neuronal transcriptomes. The Trem2 ^R47H mutation induces consistent upregulation of Bdnf and Ntrk2 across many cortical excitatory neuron types, independent of amyloid pathology. Spatial investigation of genotype enriched subclusters identified spatially localized neuronal subpopulations reduced in 5xFAD and Trem2 ^R47H; 5xFAD mice. Overall, our MERFISH spatial transcriptomics analysis identifies glial and neuronal transcriptomic alterations induced independently by 5xFAD and Trem2 ^R47H mutations, impacting inflammatory responses in microglia and astrocytes, and activity and BDNF signaling in neurons.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s11517-024-03200-1,Investigation of inert gas washout methods in a new numerical model based on an electrical analogy,"Inert gas washout methods have been shown to detect pathological changes in the small airways that occur in the early stages of obstructive lung diseases such as asthma and COPD. Numerical lung models support the analysis of characteristic washout curves, but are limited in their ability to simulate the complexity of lung anatomy over an appropriate time period. Therefore, the interpretation of patient-specific washout data remains a challenge. A new numerical lung model is presented in which electrical components describe the anatomical and physiological characteristics of the lung as well as gas-specific properties. To verify that the model is able to reproduce characteristic washout curves, the phase 3 slopes (S_3) of helium washouts are simulated using simple asymmetric lung anatomies consisting of two parallel connected lung units with volume ratios of $$\frac{1.25}{\mathrm{0.75}}$$ 1.25 0.75 , $$\frac{1.50}{\mathrm{0.50}}$$ 1.50 0.50 , and $$\frac{1.75}{\mathrm{0.25}}$$ 1.75 0.25 and a total volume flow of 250 ml/s which are evaluated for asymmetries in both the convection- and diffusion-dominated zone of the lung. The results show that the model is able to reproduce the S_3 for helium and thus the processes underlying the washout methods, so that electrical components can be used to model these methods. This approach could form the basis of a hardware-based real-time simulator. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Biological Imaging', 'Computer and Information Systems Applications']"
Springer,10.1007/s11553-023-01084-x,"Einsatz von Präventionsprogrammen an Grundschulen während der Coronapandemie (COVID-19, „coronavirus disease 2019“)","Background To date, few systematic studies of school-based prevention activities during the coronavirus pandemic are available. During the pandemic, there were massive cuts in school operations such as school closures and distance learning, suggesting that there were changes in the implementation of prevention activities. Therefore, we investigated (1) whether and which prevention programs were implemented in Thuringian primary schools in grades 1 to 4 from the beginning of the pandemic in March 2020 to the school year 2022/23, (2) whether the coronavirus pandemic had an impact on the implementation of prevention programs in schools, (3) which individuals, resources, and information sources were involved in the implementation of prevention programs, and (4) which factors need to be considered for the implementation of future measures so that more prevention programs can be implemented. Methods A total of 105 schools participated in the cross-sectional survey using a mixed-mode design. In addition to the use of prevention programs in schools, respondents were asked about individuals involved in the planning and decision-making process, sources of information and resources for prevention programs, and pandemic-related implementation difficulties and school contextual conditions relevant to implementation. Results The majority of schools implemented prevention measures during the pandemic, although fewer than during prepandemic periods. At the same time, school administrators reported a perceived increase in prevention needs. In terms of implementation, pandemic-related difficulties were reported, particularly due to school closures, hygiene regulations, and lack of staff. School contextual conditions such as funding and external support were criticized as barriers to (future) use of prevention programs. Conclusions The reported decline in the number of prevention programs implemented during the pandemic emphasizes the need for promoting prevention programs at schools. There is still a need for action to introduce (only) evidence-based programs with sustainable effectiveness into school practice. The proportion of non-evaluated programs remains widespread in schools. Hintergrund Bisher liegen nur wenige systematische Bestandsaufnahmen schulischer Präventionsprogramme während der Coronapandemie (COVID-19, „coronavirus disease 2019“) vor. Während dieser Zeit kam es zu massiven Einschnitten im Schulbetrieb, die zu Veränderungen in der Durchführung von Präventionsprogrammen geführt haben könnten. Daher wurde untersucht, (1) ob und welche Präventionsprogramme in Thüringer Grund- und Gemeinschaftsschulen in den Klassen 1 bis 4 von Pandemiebeginn im März 2020 bis zum Schuljahr 2022/23 eingesetzt wurden, (2) ob die Coronapandemie einen Einfluss auf die Durchführung von Präventionsprogrammen in Schulen hatte, (3) welche an Entscheidungs- und Auswahlprozessen beteiligte Personen, welche Informationsquellen und Ressourcen für die Durchführung von Präventionsprogrammen berücksichtigt wurden und (4) welche Faktoren für die Implementierung zukünftiger Maßnahmen berücksichtigt werden müssen, sodass mehr Präventionsprogramme eingesetzt werden können. Methodik Es handelt sich um ein querschnittliches Studiendesign. Die Studie wurde als Fragebogenstudie im Mixed-mode-Ansatz durchgeführt. Es nahmen 105 Thüringer Grund- und Gemeinschaftsschulen (1. bis 4. Klasse) an der Studie teil. Neben dem Einsatz von Präventionsprogrammen an den Schulen wurden am Entscheidungs- und Auswahlprozess beteiligte Personen, sowie Informationsquellen und Ressourcen für die Durchführung von Präventionsprogrammen erfasst. Des Weiteren wurden pandemiebedingte Durchführungsschwierigkeiten und für die Implementierung relevante schulische Kontextbedingungen von den Schulleitungen erfragt. Ergebnisse Die Mehrheit der Schulen führte während der Pandemie Präventionsprogramme durch, wenn auch weniger als zu präpandemischen Zeiten. In der Durchführung wurden pandemiebedingte Schwierigkeiten berichtet, die besonders mit Schulschließungen, Hygienevorschriften und fehlendem Personal begründet wurden. Zeitgleich berichteten die Schulleitungen einen wahrgenommen gestiegenen Präventionsbedarf. Im Planungs- und Entscheidungsprozess waren mehrheitlich Schulleitungen und Lehrkräfte beteiligt, während die Ressourcen vorrangig von externen Projektträgern zur Verfügung gestellt wurden. Schulische Kontextbedingungen wie Finanzierung und externe Unterstützung wurden als Hindernisse für den (zukünftigen) Einsatz von Präventionsprogrammen bemängelt. Schlussfolgerung Angesichts der berichteten rückläufigen Anzahl an durchgeführten Präventionsprogrammen während der Coronapandemie, sollte eine gezielte Förderung des Einsatzes von Präventionsprogrammen in Schulen in verschiedenen Bereichen forciert werden. Es besteht weiterhin Handlungsbedarf, (ausschließlich) evidenzbasierte Programme mit nachhaltiger Wirksamkeit in die schulische Praxis einzuführen. Der Anteil an nicht evaluierten Programmen ist weiterhin in Schulen weit verbreitet.","['Medicine & Public Health', 'Public Health', 'Medicine/Public Health, general', 'Health Promotion and Disease Prevention', 'Public Health', 'Health Promotion and Disease Prevention']"
Springer,10.1007/s12119-024-10266-4,Currying Favour with the Algorithm: Online Sex Workers’ Efforts To Satisfy Patriarchal Expectations,"The rise of the online sex work industry is reshaping how people conceptualise and negotiate sexual encounters across digital and offline spaces. This article analyses content from an online sex work forum (AmberCutie Forum (ACF)) to examine how online sex workers establish boundaries between their online and offline lives to manage competing expectations from their partners and viewers. Our analysis reveals a misogynistic double standard whereby workers are seen to threaten monogamous values, while viewers escape the same level of moral culpability. We argue that the cultural logics of monogamy function to delegitimise the labour involved with online sex work and increase the risk posed to online sex workers through retributive misogyny, including cyber-harassment toward sex workers. This impacts sex workers’ emotional and financial wellbeing and reinforces gendered power relations by prioritising stereotypically masculine pleasure over workers’ economic interests.","['Social Sciences', 'Social Sciences, general', 'Personality and Social Psychology', 'Psychology, general', 'Regional and Cultural Studies', 'Personality and Differential Psychology', 'Behavioral Sciences and Psychology', 'Regional Cultural Studies']"
Springer,10.1007/s11661-024-07637-9,Comparative Quantitative Analysis of the Evolution of Precipitates in Inconel 625 Superalloy Manufactured by Laser Powder Bed Fusion Subjected to High-Temperature Creep and Annealing,"The aim of the study is to contribute to understanding how prolonged exposure to high temperature and stress affects the morphology, size, and distribution of secondary phase precipitates in Inconel 625 manufactured by laser powder bed fusion. Creep tests were performed under a constant stress of 100 MPa at temperatures of 600 °C, 700 °C, and 800 °C. Samples creep tested at 600 °C and 700 °C were terminated after 2000 hours, while at 800 °C, the creep test was carried out until rupture. Isothermal annealing was carried out at temperatures of 700 °C and 800 °C for the same duration as creep tests. Microstructural analysis of creep-deformed and annealed samples was performed with the use of light microscopy, scanning, and transmission electron microscopy. The first-ever comparison of the evolution of precipitates in Inconel 625 LPBF after creep tests and annealing at 700 °C and 800 °C was shown. The qualitative and quantitative analysis revealed that the stress applied during creep tests leads to more pronounced nucleation of δ phase precipitates, although the growth of δ particles occurs more slowly compared to annealing. Furthermore, in creep-tested samples, the growth and coalescence of carbides and Laves phase particles were more intensive, compared to stress-free annealing, particularly at a temperature of 800 °C. Moreover, we show that the evolution of carbides and Laves phase particles along grain boundaries contributes to cavity and microcrack formation during high-temperature creep of Inconel 625 LPBF.","['Materials Science', 'Metallic Materials', 'Characterization and Evaluation of Materials', 'Structural Materials', 'Surfaces and Interfaces, Thin Films', 'Nanotechnology', 'Metals and Alloys', 'Characterization and Analytical Technique', 'Structural Materials', 'Surfaces, Interfaces and Thin Film', 'Nanotechnology']"
Springer,10.1007/s13369-024-09101-7,Deep Learning-Based Acoustic Emission Signal Filtration Model in Reinforced Concrete,"Acoustic emission is a nondestructive testing (NDT) technique, widely used to monitor the condition of structures for safety reasons especially in real time. The method utilizes the electrical signals generated by the elastic waves in a material under load to detect and locate damage in structures. However, identifying the sources of AE signals in concrete or composite materials can be challenging due to the anisotropic properties of materials and interpreting a large amount of AE data, leading to data misinterpretation and inaccurate detection of damage. Hence, the need for filtering out noise-induced signals from recorded data and emphasizing the actual AE source is crucial for monitoring and source localization of damage in real time. This study proposed a one-dimensional convolutional neural network (1D-CNN) deep learning approach to filter around 22,000 AE data in a reinforced concrete (RC) beam. The model utilizes significant AE parameters identified through neighborhood component analysis (NCA) to classify true AE signals from noise-induced signals. By using the optimized network parameters, a high classification accuracy of 97% and 96.29% was achieved during the training and testing phases, respectively. To check the reliability of the proposed AE filtering model in the real world, it was evaluated and verified using source location AE activities collected during a four-point bending test on a shear-deficient beam. The outcomes suggest that the proposed AE filtration model has the potential to accurately classify AE signals with an accuracy of 92.8% and proved that the filtration model provides accurate and valuable insight into source location determination.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary', 'Technology and Engineering', 'Humanities and Social Sciences']"
Springer,10.1038/s41386-024-02016-9,PACAP regulates neuroendocrine and behavioral stress responses via CRF-containing neurons of the rat hypothalamic paraventricular nucleus,"Pituitary adenylate cyclase-activating polypeptide (PACAP) is a neuropeptide widely distributed in the brain including the hypothalamic paraventricular nucleus (PVN) implying a regulatory role in stress function. Recent evidence indicates that one of the main targets of PACAP within the PVN are corticotropin-releasing factor (CRF) neurons, which are key regulators of the hypothalamic-pituitary-adrenal (HPA) axis. However, the neural correlates that mediate PACAP effects on stress function are not fully understood. In the present study, we characterized the neuronal mechanism by which PACAP regulates neuroendocrine and behavioral stress responses in rats. We found that intracerebroventricular administration of PACAP increased the swim stress-induced c-Fos expression in distinct brain areas of the stress and anxiety circuitry including the parvocellular part of the PVN and changed behavioral stress coping during forced swimming to a more passive coping style (i.e., indicated by increased floating and reduced struggling behavior). Subsequently, PACAP administration directly into the PVN mimicked these behavioral effects and potentiated the plasma ACTH response to forced swim stress suggesting an excitatory role of PACAP on HPA stress axis reactivity. In addition, immunohistochemical analysis revealed a considerable portion of stress-activated CRF neurons in the medial parvocellular part of the PVN that co-localized PAC1 receptors suggesting that PACAP-induced effects on stress function are likely mediated directly by activation of CRF neurons in the PVN. Thus, these findings suggest that the PVN may represent one of the key areas where PACAP regulates the neuroendocrine and behavioral stress response.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s13369-024-09024-3,Assessing the Influences of Noise Suppression Filters on Ultrasonic Concrete Images Generated by an Innovative CMU-SAFT Algorithm,"Ultrasonic imaging is a useful nondestructive testing technology for visualizing internal structural defects in structures. Despite its utility, since synthetic aperture focusing technique (SAFT) algorithm demands using advanced equipment to superimpose the measurements, this paper introduces a novel approach named CMU-SAFT to broaden applicability of SAFT on conventional ultrasonic data obviating the necessity for matrix antenna array-equipped devices and preventing hyperbolic patterns. To validate the feasibility of the proposed algorithm, experimental tests were conducted on a laboratory-produced concrete specimen including delamination defects at varying depths. Since other reflectives causing distortion in the ultrasonic image can interfere with the signal, the study also evaluated the influences of different noise suppression filters along with their combinations (band pass, wavelet transform, Wiener, and Savitzky–Golay). CMU-SAFT images were constructed using eleven filter combinations, and their performances were quantitatively assessed using signal-to-noise ratio, signal-to-noise and distortion ratio, total harmonic distortion, root mean square, mean squared ratio, mean absolute error and cross-correlation. The most effective filters and performance indices aligning with CMU-SAFT images considering defect depth and scanning width were suggested. The findings of the study revealed the leading potential of CMU-SAFT algorithm to overcome the need for specialized equipment by utilizing recommended filters and indicators under specific conditions.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary', 'Technology and Engineering', 'Humanities and Social Sciences']"
Springer,10.1007/s13369-024-09323-9,"Investigation of the Dewatering Process with Geotextile Tubes by Sedimentation, One- and Two-Dimensional Filtration Test Methods","Dewatering applications are carried out with geotextile tubes for the disposal or reuse of industrial wastes with high water content. Class F Seyitomer thermal power plant fly ash, an industrial waste, was selected in this study. Turbidity, sedimentation and filtration experiments were carried out using anionic and cationic polymers and polypropylene synthetic fiber to investigate the effect of polymers and fibers on the dewatering of fly ash. The use of polymers was determined to significantly accelerate filtration and soil sedimentation speed while leading to a slight increase in the volume of the filter cake. When effective polymer and dosage are used, slurry filtration time can be reduced up to one-eighth of the time and dewatering can be achieved much faster. The addition of synthetic fiber accelerated the sedimentation of the slurry and increased the filtration in the vertical direction, while it did not show a significant effect on the total filtration in two-dimensional filtration. In geotextile tube applications, although one-dimensional filtration experiments might give misleading results in terms of estimating the effectiveness of the polymers used in solid–liquid separation and dewatering times, the jar test, sedimentation and two-dimensional filtration experiments were determined to give compatible and more realistic results. In two-dimensional filtration experiments, approximately 75% of the filtration occurred in the radial direction and the dewatering time was approximately 21–55% of the time estimated by one-dimensional filtration experiment. Geotextile tube dewatering design can be made more predictable and cost-effective in the field by performing small-scale laboratory experiments with the two-dimensional filtration test system designed for this study and various dewatering applications.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary', 'Technology and Engineering', 'Humanities and Social Sciences']"
Springer,10.1007/s00769-024-01614-w,Value assignment and uncertainty evaluation for certified reference gas mixtures,"The procedures used to assign values to certified reference gas mixtures and to evaluate their associated uncertainties, which are described in ISO 6143, and that were variously improved by Guenther and Possolo (Anal Bioanal Chem 399:489–500, 2011. 10.1007/s00216-010-4379-z), are further enhanced by the following developments: (i) evaluating and propagating uncertainty contributions derived from comparisons with historical reference gas mixtures of similar nominal composition; (ii) recognizing and quantifying mutual inconsistency (dark uncertainty) between primary standard gas mixtures used for calibration; (iii) employing Bayesian procedures for calibration, value assignment, and uncertainty evaluations; and (iv) employing state-of-the-art methods of meta-analysis to combine cylinder-specific measurement results. These developments are illustrated in examples of certification of two gas mixture Standard Reference Materials developed by the National Institute of Standards and Technology (NIST, USA). These examples serve only to demonstrate the methods described in this contribution and do not replace any official measurement results delivered in the certificates of any reference materials developed by NIST.","['Chemistry', 'Analytical Chemistry', 'Food Science', 'Biochemistry, general', 'Commercial Law', 'Ecotoxicology', 'Marketing', 'Analytical Chemistry', 'Food Science', 'Chemical Biology', 'Commercial Law', 'Environmental Chemistry', 'Marketing']"
Springer,10.1038/s41386-024-02008-9,Deep phenotyping reveals CRH and FKBP51-dependent behavioral profiles following chronic social stress exposure in male mice,"The co-chaperone FKBP51, encoded by FKBP5 gene, is recognized as a psychiatric risk factor for anxiety and depressive disorders due to its crucial role in the stress response. Another key modulator in stress response regulation is the corticotropin releasing hormone (CRH), which is co-expressed with FKBP51 in many stress-relevant brain-regions and cell-types. Together, they intricately influence the balance of the hypothalamic-pituitary-adrenal (HPA) axis, one of the primary stress response systems. Previous research underscores the potential moderating effects these genes have on the regulation of the stressful life events towards the vulnerability of major depressive disorder (MDD). However, the specific function of FKBP51 in CRH-expressing neurons remains largely unexplored. Here, through deep behavioral phenotyping, we reveal heightened stress effects in mice lacking FKBP51 in CRH co-expressing neurons ( CRH ^ FKBP5−/− ), particularly evident in social contexts. Our findings highlight the importance of considering cell-type specificity and context in comprehending stress responses and advocate for the utilization of machine-learning-driven phenotyping of mouse models. By elucidating these intricacies, we lay down the groundwork for personalized interventions aimed at enhancing stress resilience and individual well-being.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s43440-024-00658-6,c-Myc inhibition and p21 modulation contribute to unsymmetrical bisacridines-induced apoptosis and senescence in pancreatic cancer cells,"Background Pancreatic cancer (PC) is one of the most aggressive cancers and is the seventh leading cause of cancer-related death worldwide. PC is characterized by rapid progression and resistance to conventional treatments. Mutations in KRAS , CDKN2A , TP53 , SMAD4/DPC4 , and MYC are major genetic alterations associated with poor treatment outcomes in patients with PC. Therefore, optimizing PC therapy is a tremendous challenge. Unsymmetrical bisacridines (UAs), synthesized by our group, are new promising compounds that have exhibited high cytotoxicity and antitumor activity against several solid tumors, including pancreatic cancer. Methods The cellular effects induced by UAs in PC cells were evaluated by MTT assay (cell growth inhibition), flow cytometry, and fluorescence and light microscopy (cell cycle distribution, apoptosis, and senescence detection). Analysis of the effects of UAs on the levels of proteins (c-Myc, p53, SMAD4, p21, and p16) was performed by Western blotting. Results Apoptosis was the main triggered mechanism of death after UAs treatment, and induction of the SMAD4 protein can facilitate this process. c-Myc, which is one of the molecular targets of UAs, can participate in the induction of cell death in a p53-independent manner. Moreover, UAs can also induce accelerated senescence through the upregulation of p21. Notably, senescent cells can die via apoptosis after prolonged exposure to UAs. Conclusions UAs have emerged as potent anticancer agents that induce apoptosis by inhibiting c-Myc protein and triggering cellular senescence in a dose-dependent manner by increasing p21 levels. Thus, UAs exhibit desirable features as promising candidates for future pancreatic anticancer therapies.","['Pharmacy', 'Pharmacy', 'Drug Safety and Pharmacovigilance', 'Pharmacotherapy', 'Pharmacy', 'Drug Safety and Pharmacovigilance', 'Therapeutics']"
Springer,10.1007/s00208-024-02953-2,Hecke algebra action on twisted motivic Chern classes and K-theoretic stable envelopes,"Let G be a linear semisimple algebraic group and B its Borel subgroup. Let $${\mathbb {T}}\subset B$$ T ⊂ B be the maximal torus. We study the inductive construction of Bott–Samelson varieties to obtain recursive formulas for the twisted motivic Chern classes of Schubert cells in G / B . To this end we introduce two families of operators acting on the equivariant K-theory $${\text {K}}_{\mathbb {T}}(G/B)[y]$$ K T ( G / B ) [ y ] , the right and left Demazure–Lusztig operators depending on a parameter. The twisted motivic Chern classes coincide (up to normalization) with the K-theoretic stable envelopes. Our results imply wall-crossing formulas for a change of the weight chamber and slope parameters. The right and left operators generate a twisted double Hecke algebra. We show that in the type A this algebra acts on the Laurent polynomials. This action is a natural lift of the action on $${\text {K}}_{\mathbb {T}}(G/B)[y]$$ K T ( G / B ) [ y ] with respect to the Kirwan map. We show that the left and right twisted Demazure–Lusztig operators provide a recursion for twisted motivic Chern classes of matrix Schubert varieties.","['Mathematics', 'Mathematics, general', 'Mathematics']"
Springer,10.1007/s00125-024-06319-w,Simplified meal announcement study (SMASH) using hybrid closed-loop insulin delivery in youth and young adults with type 1 diabetes: a randomised controlled two-centre crossover trial,"Aims/hypothesis The majority of hybrid closed-loop systems still require carbohydrate counting (CC) but the evidence for its justification remains limited. Here, we evaluated glucose control with simplified meal announcement (SMA) vs CC in youth and young adults with type 1 diabetes using the mylife CamAPS FX system. Methods We conducted a two-centre, randomised crossover, non-inferiority trial in two University Hospitals in Switzerland in 46 participants (aged 12–20 years) with type 1 diabetes using multiple daily injections ( n =35), sensor-augmented pump ( n =4) or hybrid closed-loop ( n =7) therapy before enrolment. Participants underwent two 3 month periods with the mylife CamAPS FX system (YpsoPump, Dexcom G6) to compare SMA (individualised carbohydrate meal sizes) with CC, in a randomly assigned order using computer-generated sequences. The primary endpoint was the proportion of time glucose was in target range (3.9–10.0 mmol/l) with a non-inferiority margin of 5 percentage points. Secondary endpoints were other sensor glucose and insulin metrics, usability and safety endpoints. Results Forty-three participants (18 women and girls) completed the trial. In the intention-to-treat analysis, time in range (mean±SD) was 69.9±12.4% with SMA and 70.7±13.0% with CC (estimated mean difference −0.6 percentage points [95% CI −2.4, 1.1], demonstrating non-inferiority). Time <3.9 mmol/l (median [IQR] 1.8 [1.2–2.2]% vs 1.9 [1.6–2.5]%) and >10.0 mmol/l (28.2±12.6% vs 27.2±13.4%) was similar between periods. Total daily insulin dose was higher with SMA (54.0±14.7 U vs 51.7±12.1 U, p =0.037). Three participants experienced serious adverse events, none of which were intervention-related. Conclusions/interpretation Glucose control using the CamAPS FX algorithm with SMA was non-inferior to its use with CC in youth and young adults with type 1 diabetes. Trial registration ClinicalTrials.gov NCT05481034. Funding The study was supported by the Swiss Diabetes Foundation and by a YTCR grant from the Bangerter-Rhyner Foundation and the Swiss Academy of Medical Sciences. Dexcom and Ypsomed provided product support. Graphical Abstract ","['Medicine & Public Health', 'Internal Medicine', 'Metabolic Diseases', 'Human Physiology', 'Internal Medicine', 'Diseases', 'Human Physiology']"
Springer,10.1038/s41380-024-02676-5,The role of body image dissatisfaction in the relationship between body size and disordered eating and self-harm: complimentary Mendelian randomization and mediation analyses,"Disordered eating and self-harm commonly co-occur in young people suggesting potential for shared underlying causes. Body image dissatisfaction (BID) has been recognised as a psychological correlate of body size, associated with both disordered eating and self-harm. However, the investigation into etiological pathways early in the lifecourse to provide detail on how body size and BID may foster disordered eating and self-harm remains largely unexplored. Employing data from two large population-based cohorts, the UK Biobank and the Avon Longitudinal Study of Parents And Children (ALSPAC), we conducted bidirectional Mendelian randomization (MR) to determine the causal direction of effect between genetically predicted prepubertal body size and two measures of BID indicating (i) desire to be smaller, and (ii) desire to be larger. We then used multivariable regression followed by counterfactual mediation analyses. Bidirectional MR indicated robust evidence that increased genetically predicted prepubertal body size increased desire to be smaller and decreased desire to be larger. Evidence for the reverse causal direction was negligible. These findings remained very similar across sensitivity analyses. In females and males, multivariable regression analyses demonstrated that being overweight increased the risk of disordered eating (risk ratio (RR), 95% confidence interval (CI): 1.19, 1.01 to 1.40 and 1.98, 1.28 to 3.05, respectively) and self-harm (RR, 95% CI: 1.35, 1.04 to 1.77 and 1.55, 0.86 to 2.81, respectively), while being underweight was protective against disordered eating (RR, 95% CI: 0.57, 0.40 to 0.81 and 0.81, 0.38 to 1.73, respectively). There was weak evidence of an increase in the risk of self-harm among underweight individuals. Mediation analyses indicated that the relationship between being overweight and subsequent disordered eating was largely mediated by the desire to be smaller. Our research carries important public health implications, suggesting distinct risk profiles for self-harm and disordered eating in relation to weight and body image. In addition, a better understanding of genetically predicted prepubertal BID may be valuable in the prevention and treatment of disordered eating and self-harm in adolescence.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s11096-024-01822-x,Model-informed precision dosing of vancomycin in clinical practice: an intervention development study,"Background Current guidelines recommend dosing vancomycin based on the area under the concentration time curve (AUC) to maximise efficacy and minimise the risk of nephrotoxicity. The preferred approach to AUC-guided therapy is to apply model-informed precision dosing (MIPD). However, the adoption in clinical practice has been slow. Aim We aimed to develop an intervention, including a standardised MIPD workflow and an implementation plan for vancomycin AUC-guided dosing, in a Swedish tertiary hospital. Method The intervention was developed in a framework-guided process. The design phase included stakeholder feedback (nurses, pharmacists, physicians), local data collection and feasibility testing of intervention components with parallel consideration of implementation aspects. The hypothesised relationships between the different components, implementation strategies and the mechanism of action resulting in expected outcomes were represented by a logic model. Results The final intervention consisted of a workflow for MIPD, with defined roles and responsibilities, as well as processes for data and information transfer. Details were provided in supportive documents; an instruction on therapeutic drug monitoring (TDM) sampling and documentation for nurses, and a detailed dosing software instruction for MIPD consultants and clinical pharmacists. Activities to facilitate implementation included the development of a local clinical routine for vancomycin dosing, staff training and recurring MIPD rounds. Conclusion An intervention for MIPD, with an implementation plan for AUC-guided dosing of vancomycin, was developed for a tertiary hospital setting. The process can be used as guidance for other institutions with similar context wishing to initiate MIPD.","['Medicine & Public Health', 'Internal Medicine', 'Pharmacy', 'Internal Medicine', 'Pharmacy']"
Springer,10.1007/s11060-024-04874-1,The impact of intraoperative mapping during re-resection in recurrent gliomas: a systematic review,"Purpose Previous evidence suggests that glioma re-resection can be effective in improving clinical outcomes. Furthermore, the use of mapping techniques during surgery has proven beneficial for newly diagnosed glioma patients. However, the effects of these mapping techniques during re-resection are not clear. This systematic review aimed to assess the evidence of using these techniques for recurrent glioma patients. Methods A systematic search was performed to identify relevant studies. Articles were eligible if they included adult patients with recurrent gliomas (WHO grade 2–4) who underwent re-resection. Study characteristics, application of mapping, and surgical outcome data on survival, patient functioning, and complications were extracted. Results The literature strategy identified 6372 articles, of which 125 were screened for eligibility. After full-text evaluation, 58 articles were included in this review, comprising 5311 patients with re-resection for glioma. Of these articles, 17% (10/58) reported the use of awake or asleep intraoperative mapping techniques during re-resection. Mapping was applied in 5% (280/5311) of all patients, and awake craniotomy was used in 3% (142/5311) of the patients. Conclusion Mapping techniques can be used during re-resection, with some evidence that it is useful to improve clinical outcomes. However, there is a lack of high-quality support in the literature for using these techniques. The low number of studies reporting mapping techniques may, next to publication bias, reflect limited application in the recurrent setting. We advocate for future studies to determine their utility in reducing morbidity and increasing extent of resection, similar to their benefits in the primary setting.","['Medicine & Public Health', 'Oncology', 'Neurology', 'Oncology', 'Neurology']"
Springer,10.1007/s00208-024-02957-y,Local connectivity of boundaries of tame Fatou components of meromorphic functions,"We prove the local connectivity of the boundaries of invariant simply connected attracting basins for a class of transcendental meromorphic maps. The maps within this class need not be geometrically finite or in class $${\mathcal {B}}$$ B , and the boundaries of the basins (possibly unbounded) are allowed to contain an infinite number of post-singular values, as well as the essential singularity at infinity. A basic assumption is that the unbounded parts of the basins are contained in regions which we call ‘repelling petals at infinity’, where the map exhibits a kind of ‘parabolic’ behaviour. In particular, our results apply to a wide class of Newton’s methods for transcendental entire maps. As an application, we prove the local connectivity of the Julia set of Newton’s method for $$\sin z$$ sin z , providing the first non-trivial example of a locally connected Julia set of a transcendental map outside class $${\mathcal {B}}$$ B , with an infinite number of unbounded Fatou components.","['Mathematics', 'Mathematics, general', 'Mathematics']"
Springer,10.1007/s00208-024-02981-y,Extensions of k-regulous functions from two-dimensional varieties,We prove that a k -regulous function defined on a two-dimensional non-singular affine variety can be extended to an ambient variety. Additionally we derive some results concerning sums of squares of k -regulous functions; in particular we show that every positive semi-definite regular function on a non-singular affine variety can be written as a sum of squares of locally Lipschitz regulous functions.,"['Mathematics', 'Mathematics, general', 'Mathematics']"
Springer,10.1007/s11060-024-04885-y,TTF-1 negativity in synchronous M1b/M1c wildtype lung adenocarcinoma brain metastases predicts worse survival with increased risk of intracranial progression,"Background Thyroid Transcription Factor-1 (TTF-1) expression in lung adenocarcinoma (LUAD) has been studied for its prognostic value in early-stage and metastatic disease. Its role in brain metastasis remains unexplored. This study investigates the predictive value and association of TTF-1 status with clinicopathological variables in patients with synchronous LUAD brain metastases. Material and methods In this bicentric retrospective study, 245 patients with newly diagnosed, treatment-naïve brain metastasis undergoing resection were included. Patient data were retrieved from electronic records. Outcomes included overall and progression-free survival. Statistical analysis included Kaplan–Meier estimates and Cox proportional hazards regression. Results Mean Ki67 index in TTF-1 negative patients was 43% [95% CI 38–48%] compared to 32% [95% CI 29–35%] in TTF-1 positive (TTF-1 +) patients (p < 0.001). Tumor volume was significantly larger in TTF-1 negative (TTF-1-) patients (mean volume 24 mL [95% CI 18–31 mL]) vs. 15 mL [95% CI 12–17 mL] in TTF-1 + patients (padjust = 0.003). Perifocal edema was smaller in TTF-1- patients (mean volume: 58 mL [95% CI 45–70 mL]) vs. 84 mL [95% CI 73–94 mL] in TTF-1 + patients (padjust = 0.077). Tumor and edema volume did not correlate. TTF-1- patients showed worse overall, intracranial, and extracranial progression-free survival. In a multivariable Cox model, positive TTF-1 status was independently associated with improved outcomes. Negative TTF-1 status was associated with increased hazard for intracranial disease progression compared to extracranial progression. Conclusion In synchronous LUAD brain metastases, TTF-1 negativity reflects an aggressive phenotype with larger proliferation capacity and tumor volume. Future research should explore the underlying cellular and molecular alterations of this phenotype.","['Medicine & Public Health', 'Oncology', 'Neurology', 'Oncology', 'Neurology']"
Springer,10.1007/s11060-024-04891-0,"Patient-derived glioma organoids real time identification of IDH mutation, 1p/19q-codeletion and CDKN2A/B homozygous deletion with differential ion mobility spectrometry","Purpose Extent of brain tumor resection continues to be one of the central decisions taken during standard of care in glioma patients. Here, we aimed to evaluate the most essential molecular factors, such as IDH (isocitrate dehydrogenase) mutation in gliomas classification with patient-derived glioma organoids (PGOs) using differential mobility spectrometry (DMS). Methods we prospectively recruited 12 glioma patients, 6 IDH-mutated and 6 IDH wild-type tumors, from which PGOs were generated ex-vivo . Altogether, 320 PGOs DMS spectra were analyzed with a classifier algorithm based on linear discriminant analysis (LDA). Results LDA model classification accuracy (CA) obtained between IDH-mutant and IDH wild-type PGOs was 90% (91% sensitivity and 89% specificity). Furthermore, 1p/19q codeletion classification within IDH mutant PGOs reached 98% CA (93% sensitivity and 99% specificity), while CDKN2A/B homozygous loss status had 86% CA (63% sensitivity 93% specificity). Conclusion DMS suitability to differentiate IDH-mutated PGOs was thus validated in ex vivo cultured samples, PGOs. Preliminary results regarding 1p/19q codeleted PGOs and CDKN2A/B loss PGOs identification endorse testing in a prospective intraoperative glioma patient cohort. Our results reveal a sample classification set-up that is compatible with real-time intraoperative surgery guidance.","['Medicine & Public Health', 'Oncology', 'Neurology', 'Oncology', 'Neurology']"
Springer,10.1007/s40615-023-01876-z,Strategies to Improve Care in the Emergency Department for Culturally and Linguistically Diverse Adults: a Systematic Review,"Background The emergency department (ED) is an important gateway into the health system for people from culturally and linguistically diverse (CALD) backgrounds; their experience in the ED is likely to impact the way they access care in the future. Our review aimed to describe interventions used to improve ED health care delivery for adults from a CALD background. Methods An electronic search of four databases was conducted to identify empirical studies that reported interventions with a primary focus of improving ED care for CALD adults (aged ≥ 18 years), with measures relating to ED system performance, patient outcomes, patient experience, or staff experience. Studies published from inception to November 2022 were included. We excluded non-empirical studies, studies where an intervention was not provided in ED, papers where the full text was unavailable, or papers published in a language other than English. The intervention strategies were categorised thematically, and measures were tabulated. Results Following the screening of 3654 abstracts, 89 articles underwent full text review; 16 articles met the inclusion criteria. Four clear strategies for targeting action tailored to the CALD population of interest were identified: improving self-management of health issues, improving communication between patients and providers, adhering to good clinical practice, and building health workforce capacity. Conclusions The four strategies identified provide a useful framework for targeted action tailored to the population and outcome of interest. These detailed examples show how intervention design must consider intersecting socio-economic barriers, so as not to perpetuate existing disparity. Registration PROSPERO registration number: CRD42022379584.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Epidemiology', 'Quality of Life Research', 'Social Structure, Social Inequality', 'Public Health', 'Epidemiology', 'Quality of Life Research', 'Social Structure']"
Springer,10.1038/s41388-024-03199-7,Chimeric protein EWS::FLI1 drives cell proliferation in Ewing Sarcoma via aberrant expression of KCNN1/SK1 and dysregulation of calcium signaling,"Ewing sarcoma (ES) is characterized by EWS::FLI1 or EWS::ERG fusion proteins. Knowing that ion channels are involved in tumorigenesis, this work aimed to study the involvement of the KCNN1 gene, which encodes the SK1 potassium channel, in ES development. Bioinformatics analyses from databases were used to study KCNN1 expression in patients and cell lines. Molecular approaches and in vitro assays were used to study the transcriptional regulation of KCNN1 and its involvement in the regulation of ES cell proliferation. KCNN1 is overexpressed in ES patient biopsies, and its expression is inversely correlated with patient survival. EWS::FLI1, like EWS::ERG, promotes KCNN1 and SK1 expression, binding to GGAA microsatellites near the promoter of KCNN1 isoforms. KCNN1 is involved in the regulation of ES cell proliferation, with its silencing being associated with a slowing of the cell cycle, and its expression modulates membrane potential and therefore calcium flux. These results highlight that KCNN1 is a direct target of EWS::FLI1 and EWS::ERG and demonstrate that KCNN1 is involved in the regulation of intracellular calcium activity and ES cell proliferation, making it a promising therapeutic target in ES.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Internal Medicine', 'Cell Biology', 'Human Genetics', 'Oncology', 'Apoptosis', 'Public Health', 'Internal Medicine', 'Cell Biology', 'Medical Genetics', 'Oncology', 'Cell Death']"
Springer,10.1186/s41239-024-00501-1,Enhancing peer assessment with artificial intelligence,"This paper surveys research and practice on enhancing peer assessment with artificial intelligence. Its objectives are to give the structure of the theoretical framework underpinning the study, synopsize a scoping review of the literature that illustrates this structure, and provide a case study which further illustrates this structure. The theoretical framework has six areas: (i) Assigning Peer Assessors, (ii) Enhancing Individual Reviews, (iii) Deriving Peer Grades/Feedback, (iv) Analyzing Student Feedback, (v) Facilitating Instructor Oversight and (vi) Peer Assessment Systems. The vast majority of the 79 papers in the review found that artificial intelligence improved peer assessment. However, the focus of many papers was on diversity in grades and feedback, fuzzy logic and the analysis of feedback with a view to equalizing its quality. Relatively few papers focused on automated assignment, automated assessment, calibration, teamwork effectiveness and automated feedback and these merit further research. This picture suggests AI is making inroads into peer assessment, but there is still a considerable way to go, particularly in the under-researched areas. The paper incorporates a case study of the RIPPLE peer-assessment tool, which harnesses student wisdom, insights from the learning sciences and AI to enable time-constrained educators to immerse their students in deep and personalized learning experiences that effectively prepare them to serve as assessors. Once trained, they use a comprehensive rubric to vet learning resources submitted by other students. They thereby create pools of high-quality learning resources which can be used to recommend personalized content to students. RIPPLE engages students in a trio of intertwined activities: creation, review and personalized practice, generating many resource types. AI-driven real-time feedback is given but students are counseled to assess whether it is accurate. Affordances and challenges for researchers and practitioners were identified.","['Computer Science', 'Computers and Education', 'Educational Technology', 'Higher Education', 'Computer Appl. in Social and Behavioral Sciences', 'Statistics for Social Sciences, Humanities, Law', 'Information Systems Applications (incl.Internet)', 'Computers and Education', 'Digital Education and Educational Technology', 'Higher Education', 'Computer Application in Social and Behavioral Sciences', 'Statistics in Social Sciences, Humanities, Law, Education, Behavorial Sciences, Public Policy', 'Computer and Information Systems Applications']"
Springer,10.1007/s10586-024-04715-w,Ml assisted techniques in power side channel analysis for trojan classification,"Hardware Trojans (HTs) are one of the emerging malicious hardware modification attacks that have become a critical threat to the integrity, reliability, security, and trustworthiness of integrated circuits (ICs) applications. These deliberately hidden malicious entities can be inserted into the IC during manufacturing or design, potentially leading to the leakage of secret information or the deactivation or destruction of the entire system that relies on the IC hardware chips. Localizing and detecting hardware Trojans is becoming increasingly challenging as these threats are deeply embedded and electronic systems continue to grow in complexity. Traditional methods, including physical inspections and functional testing, are increasingly inadequate. They are limited in scope and often fall short when confronted with the advanced designs of modern hardware Trojans.This study aims to improve the identification of hardware Trojans (HTs) in integrated circuits by using advanced machine learning algorithms with a unique dataset of power side-channel signals. Various machine learning techniques, such as Support Vector Machines (SVM), neural networks, and decision trees, are applied to classify and identify HTs accurately. This method combines comprehensive feature extraction and model validation to provide high accuracy and reliability in HT identification. This research makes significant contributions to cybersecurity by providing improved techniques for detecting minor anomalies associated with HTs using advanced machine learning algorithms. The results show potential advances in securing electronic systems against these hidden threats, highlighting the practical significance and necessity of our work in maintaining a wide range of applications from consumer electronics to national infrastructure.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
Springer,10.1007/s00778-024-00893-6,How good are multi-dimensional learned indexes? An experimental survey,"Efficient indexing is fundamental to managing and analyzing multi-dimensional data. A growing trend is to directly learn the storage layout of multi-dimensional data using simple machine learning models, leading to the concept of Learned Index . Compared to conventional indexing methods that have been used for decades (e.g., k d-tree and R-tree variants), learned indexes have demonstrated empirical advantages in both space and time efficiency on modern architectures. However, there is a lack of comprehensive evaluation across existing multi-dimensional learned indexes under a standardized benchmark, making it challenging to identify the most suitable index for specific data types and query patterns. This gap also hinders the widespread adoption of learned indexes in practical applications. In this paper, we present the first in-depth empirical study to answer the question: how good are multi-dimensional learned indexes? We evaluate ten recently published indexes under a unified experimental framework, which includes standardized implementations, datasets, query workloads, and evaluation metrics. We thoroughly investigate the evaluation results and discuss the findings that may provide insights for future learned index design.","['Computer Science', 'Database Management', 'Database Management']"
Springer,10.1007/s10586-024-04969-4,A new approach for fire and non-fire aerosols discrimination based on multilayer perceptron trained by modified bonobo optimizer,"Fire smoke detection plays a pivotal role in our life. For the optical fire smoke detector, it is easy to produce false alarms due to the misidentification of non-fire aerosols, such as dust and water mist. In this research, four multilayer perceptron (MLP) models are established and applied to discriminate the fire smokes and non-fire aerosols. A new hybrid algorithm (HBOBES) is proposed to optimize the parameters of MLP, which incorporates three search phases, including space selecting, promiscuous and restrictive mating, and consortship and extra-group mating, which are derived from the basic bonobo optimizer (BO) and bald eagle search algorithm (BES). Moreover, a adaptive tent chaos mapping technique is introduced into the first two phases to increase the population diversity. In the experiments, eight standard classification datasets and sixteen self-established aerosol classification datasets are applied to evaluate HBOBES’s optimization performance on training the MLP models. The results show that the proposed HBOBES ranks first overall, showing the merits in training MLP models compared to eight other algorithms. For the aerosol classification problem, it is found that both the optimized 3-7-2 and 3-7-3 MLP models have achieved the highest classification accuracy of approximately 95%. Black smokes can be identified with 100% classification accuracy. Therefore, this paper provides an effective and feasible approach to distinguish fire and non-fire aerosols by using the MLP classifier optimized by optimization algorithm, which has practical significance for the development of intelligent optical smoke detector.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
Springer,10.1007/s42484-025-00237-9,Transformer models for quantum gate set tomography,"Quantum computation represents a promising frontier in the domain of high-performance computing, blending quantum information theory with practical applications to overcome the limitations of classical computation. This study investigates the challenges of manufacturing high-fidelity and scalable quantum processors. Quantum gate set tomography (QGST) is a critical method for characterizing quantum processors and understanding their operational capabilities and limitations. This paper introduces Ml4Qgst as a novel approach to QGST by integrating machine learning techniques, specifically utilizing a transformer neural network model. Adapting the transformer model for QGST addresses the computational complexity of modeling quantum systems. Advanced training strategies, including data grouping and curriculum learning, are employed to enhance model performance, demonstrating significant congruence with ground-truth values. We benchmark this training pipeline on the constructed learning model, to successfully perform QGST for 2 and 3 gates on single-qubit and two-qubit systems, with over-rotation error and depolarizing noise estimation with comparable accuracy to pyGSTi. This research marks a pioneering step in applying deep neural networks to the complex problem of quantum gate set tomography, showcasing the potential of machine learning to tackle nonlinear tomography challenges in quantum computing.","['Engineering', 'Computational Intelligence', 'Quantum Information Technology, Spintronics', 'Artificial Intelligence', 'Computational Intelligence', 'Spintronics', 'Artificial Intelligence']"
Springer,10.1038/s41598-024-82402-x,Minimal sourced and lightweight federated transfer learning models for skin cancer detection,"One of the most fatal diseases that affect people is skin cancer. Because nevus and melanoma lesions are so similar and there is a high likelihood of false negative diagnoses challenges in hospitals. The aim of this paper is to propose and develop a technique to classify type of skin cancer with high accuracy using minimal resources and lightweight federated transfer learning models. Here minimal resource based pre-trained deep learning models including EfficientNetV2S, EfficientNetB3, ResNet50, and NasNetMobile have been used to apply transfer learning on data of shape $$\:\:224\times\:224\times\:3$$ . To compare with applied minimal resource transfer learning, same methodology has been applied using best identified model i.e. EfficientNetV2S for images of shape $$\:\:32\times\:32\times\:3$$ . The identified minimal and lightweight resource based EfficientNetV2S with images of shape $$\:32\times\:32\times\:3$$ have been applied for federated learning ecosystem. Both, identically and non-identically distributed datasets of shape $$\:32\times\:32\times\:3$$ have been applied and analyzed through federated learning implementations. The results have been analyzed to show the impact of low-pixel images with non-identical distributions over clients using parameters such as accuracy, precision, recall and categorical losses. The classification of skin cancer shows an accuracy of IID 89.83% and Non-IID 90.64%.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1038/s41598-025-86371-7,Research on credit risk of listed companies: a hybrid model based on TCN and DilateFormer,"The ability to assess and manage corporate credit risk enables financial institutions and investors to mitigate risk, enhance the precision of their decision-making, and adapt their strategies in a prompt and effective manner. The growing quantity of data and the increasing complexity of indicators have rendered traditional machine learning methods ineffective in enhancing the accuracy of credit risk assessment. Consequently, academics have begun to explore the potential of models based on deep learning. In this paper, we apply the concept of combining Transformer and CNN to the financial field, building on the traditional CNN-Transformer model’s capacity to effectively process local features, perform parallel processing, and handle long-distance dependencies. To enhance the model’s ability to capture financial data over extended periods and address the challenge of high-dimensional financial data, we propose a novel hybrid model, TCN-DilateFormer. This integration improves the accuracy of corporate credit risk assessment. The empirical study demonstrates that the model exhibits superior prediction accuracy compared to traditional machine learning assessment models, thereby offering a novel and efficacious tool for corporate credit risk assessment.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1038/s41467-024-55762-1,Taming large-scale genomic analyses via sparsified genomics,"Searching for similar genomic sequences is an essential and fundamental step in biomedical research. State-of-the-art computational methods performing such comparisons fail to cope with the exponential growth of genomic sequencing data. We introduce the concept of sparsified genomics where we systematically exclude a large number of bases from genomic sequences and enable faster and memory-efficient processing of the sparsified, shorter genomic sequences, while providing comparable accuracy to processing non-sparsified sequences. Sparsified genomics provides benefits to many genomic analyses and has broad applicability. Sparsifying genomic sequences accelerates the state-of-the-art read mapper (minimap2) by 2.57-5.38x, 1.13-2.78x, and 3.52-6.28x using real Illumina, HiFi, and ONT reads, respectively, while providing comparable memory footprint, 2x smaller index size, and more correctly detected variations compared to minimap2. Sparsifying genomic sequences makes containment search through very large genomes and large databases 72.7-75.88x (1.62-1.9x when indexing is preprocessed) faster and 723.3x more storage-efficient than searching through non-sparsified genomic sequences (with CMash and KMC3). Sparsifying genomic sequences enables robust microbiome discovery by providing 54.15-61.88x (1.58-1.71x when indexing is preprocessed) faster and 720x more storage-efficient taxonomic profiling of metagenomic samples over the state-of-the-art tool (Metalign). Searching for similar genomic DNA sequences remains challenging, partly due to the large size of the analyzed data. Here the authors introduce the concept of ‘sparsified genomics’, where they systematically exclude a significant proportion of bases from genomic sequences, enabling faster and memory-efficient processing, while providing comparable accuracy to processing non-sparsified sequences.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Humanities and Social Sciences']"
Springer,10.1186/s42234-024-00163-4,Next generation bioelectronic medicine: making the case for non-invasive closed-loop autonomic neuromodulation,"The field of bioelectronic medicine has advanced rapidly from rudimentary electrical therapies to cutting-edge closed-loop systems that integrate real-time physiological monitoring with adaptive neuromodulation. Early innovations, such as cardiac pacemakers and deep brain stimulation, paved the way for these sophisticated technologies. This review traces the historical and technological progression of bioelectronic medicine, culminating in the emerging potential of closed-loop devices for multiple disorders of the brain and body. We emphasize both invasive techniques, such as implantable devices for brain, spinal cord and autonomic regulation, while we introduce new prospects for non-invasive neuromodulation, including focused ultrasound and newly developed autonomic neurography enabling precise detection and titration of inflammatory immune responses. The case for closed-loop non-invasive autonomic neuromodulation (incorporating autonomic neurography and splenic focused ultrasound stimulation) is presented through its applications in conditions such as sepsis and chronic inflammation, illustrating its capacity to revolutionize personalized healthcare. Today, invasive or non-invasive closed-loop systems have yet to be developed that dynamically modulate autonomic nervous system function by responding to real-time physiological and molecular signals; it represents a transformative approach to therapeutic interventions and major opportunity by which the bioelectronic field may advance. Knowledge gaps remain and likely contribute to the lack of available closed loop autonomic neuromodulation systems, namely, (1) significant exogenous and endogenous noise that must be filtered out, (2) potential drift in the signal due to temporal change in disease severity and/or therapy induced neuroplasticity, and (3) confounding effects of exogenous therapies (e.g., concurrent medications that dysregulate autonomic nervous system functions). Leveraging continuous feedback and real-time adjustments may overcome many of these barriers, and these next generation systems have the potential to stand at the forefront of precision medicine, offering new avenues for individualized and adaptive treatment.","['Biomedicine', 'Biomedical Engineering/Biotechnology', 'Biotechnology']"
Springer,10.1007/s11063-024-11703-z,Fixed-time Synchronization of Caputo/Conformable Fractional-Order Inertial Cohen-Grossberg Neural Networks via Event-triggered One/Two-phase Hybrid Impulsive Control,"This paper is concerned with the fixed-time synchronization (FXS) problems of Caputo/conformable fractional-order inertial Cohen-Grossberg neural networks (FOICGNNs) by one/two-phase hybrid impulsive control (HIC) through event-triggered update strategies. By utilizing the properties of fractional calculus, several novel inequalities regarding the fixed-time convergence of hybrid impulsive systems (HIS) are obtained. We especially discuss and compare the cases of Caputo and conformable fractional order to gain deep insight into fractional calculus. By applying the Lyapunov stability theory, two hybrid controllers, which consist of event-triggered continuous controllers and impulsive controllers, are designed to realize the FXS of FOICGNNs. It’s worth pointing out that, we unprecedentedly study and compare the differences of the one-phase HIC and two-phase HIC, where a novel nonlinear impulsive controller is proposed and designed to obtain fixed-time convergence in the impulsive control phase. In addition, the exclusion of Zeno behavior is proved for the designed event-triggered strategy. Finally, several numerical examples are provided to illustrate the feasibility of the proposed control approach and the correctness of the theoretical results.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
Springer,10.1007/s00778-024-00892-7,Optimizing navigational graph queries,"We study the optimization of navigational graph queries in the form of the Regular Queries, i.e., queries which combine recursive and pattern-matching fragments. Current approaches to their evaluation are not effective in practice. Towards addressing this, we present a number of novel powerful optimization techniques which aim to constrain the intermediate results during query evaluation. We show how these techniques can be planned effectively and executed efficiently towards the first practical evaluation solution for complex navigational queries on real-world workloads. Indeed, our experimental results show several orders of magnitude improvement in query evaluation performance over state-of-the-art techniques on a wide range of queries on diverse datasets.","['Computer Science', 'Database Management', 'Database Management']"
Springer,10.1038/s41598-025-86886-z,Efficient and rapid identification of tropical maize inbred lines tolerant to waterlogging stress,"Waterlogging (WL) is an important abiotic stress, severely affecting plant growth and development, inhibiting root respiration and degradation of chlorophyll, senescence of leaves and chlorosis leading to substantial yield loss. These intensities of yield losses generally depend on the duration of WL and crop growth stages. Maize being a dry land crop is particularly sensitive to WL. Systematic screening techniques to identify parameters linked with tolerance are not well established which serves as a major bottleneck in the identification of promising genotypes. In this study, 120 maize inbred lines belonging to diverse genetic backgrounds were evaluated for WL tolerance both at pre-emergence as well as the seedling stage. Results based on percentage germination at pre-emergence and percentage survival at the seedling stage under WL established that pre-germination tolerance is independent of seedling stage tolerance. Membership function value based on WL tolerance coefficient of shoot and root fresh weights, dry weights, lengths, root surface area, shoot area and root volume was used to identify tolerant lines. Established mathematical models were used and identified root dry weight as a single reliable parameter to judge the tolerance level of genotypes. The use of BLPSI and ESIM selection indices as well as MTSI to judge the stability as well as genetic worth of genotypes further strengthens the selection efficiency. Lines thus performing best across all the models included I 185, I 172 and SE 503 and were identified as tolerant lines for WL. A combination of these different selection approaches would further strengthen selection efficiency and is believed to be a rapid and effective selection approach.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1038/s41378-024-00825-y,3D printable and myoelectrically sensitive hydrogel for smart prosthetic hand control,"Surface electromyogram (sEMG) serves as a means to discern human movement intentions, achieved by applying epidermal electrodes to specific body regions. However, it is difficult to obtain high-fidelity sEMG recordings in areas with intricate curved surfaces, such as the body, because regular sEMG electrodes have stiff structures. In this study, we developed myoelectrically sensitive hydrogels via 3D printing and integrated them into a stretchable, flexible, and high-density sEMG electrodes array. This electrode array offered a series of excellent human-machine interface (HMI) features, including conformal adherence to the skin, high electron-to-ion conductivity (and thus lower contact impedance), and sustained stability over extended periods. These attributes render our electrodes more conducive than commercial electrodes for long-term wearing and high-fidelity sEMG recording at complicated skin interfaces. Systematic in vivo studies were used to investigate its efficacy to control a prosthetic hand by decoding sEMG signals from the human hand via a multiple-channel readout circuit and a sophisticated artificial intelligence algorithm. Our findings demonstrate that the 3D printed gel myoelectric sensing system enables real-time and highly precise control of a prosthetic hand.","['Engineering', 'Engineering, general', 'Technology and Engineering', 'Nanotechnology']"
Springer,10.1038/s41598-025-86731-3,Clinical application of 3D slicer reconstruction and 3D printing localization combined with neuroendoscopy technology in VPS surgery,"To explore techniques, advantages and disadvantages of 3D Slicer reconstruction and 3D printing localization technology combined with transcranial neuroendoscopy in ventriculoperitoneal shunt surgery. Retrospective analysis of clinical data of patients with hydrocephalus treated by ventriculoperitoneal shunt surgery using 3D Slicer reconstruction and 3D printing positioning technology combined with transcranial neuroendoscopy in our hospital from October 2021 to March 2023. A total of 33 patients with complete data were collected, including 19 males and 14 females, aged 10–81 years. Pre operative use of 3D Slicer reconstruction and 3D printing localization, and intraoperative use of neuroendoscopy assisted catheterization to complete ventriculoperitoneal shunt surgery. The drainage tube position was confirmed by brain CT and 3D Slicer reconstruction after operation, of which 30 cases were located in the frontal horn or center of the ipsilateral lateral ventricle, and 3 cases were located in the frontal horn or center of the contralateral lateral ventricle. All patients were successfully catheterized and well positioned. According to the unique ventricular system characteristics of each hydrocephalus patient, the 3D Slicer reconstruction technology was used to determine the individualized puncture point and direction, measure the puncture depth, accurately locate the puncture through the 3D printing guide plate, and accurately send the tip of the ventricular catheter into the frontal or central part of the lateral ventricle with the assistance of neuroendoscopic visualization, which improved the success rate of the operation and reduced the risk of tube blockage. At the same time, our team has newly developed a puncture point (“Cai’s point”), which has a puncture path in a non-vascular area and can reduce the risk of puncture bleeding. However, further prospective clinical research is needed to determine its routine location.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1186/s40814-024-01581-6,Rapid Adaptation to Prevent Drug Use (RAPD): protocol of a pilot randomized trial to enhance the impact of an evidence-based intervention for youth,"Background Drug use trends change rapidly among youth, leaving intervention experts struggling to respond promptly. Delays in responses can lead to preventable morbidity and mortality. The COVID-19 pandemic underscored the need for implementation science to facilitate rapid, equitable responses using existing treatment and prevention efforts. Existing, widely adopted evidence-based interventions (EBIs; e.g., the Michigan Model for Health™: MMH) are well suited to address emerging drug trends. We have a critical need to advance implementation strategies to optimize system responsiveness to these emerging drug issues. This research aims to design and test implementation strategies to (1) improve the responsiveness of school-based EBIs in addressing urgent issues and (2) find ways to support teachers in implementing updated EBIs, attending to unique considerations of schools serving economically disadvantaged students. Methods The research aims are as follows: aim 1: identify implementation gaps and best practices using After Action Review (a reflective process used by health organizations in responding to emergent public health events) using qualitative methods. Aim 2: design and pilot test RAPD (Rapid Adaptation to Prevent Drug use) based on aim 1 findings. RAPD refers to a novel set of implementation strategies designed to enhance the capacity of an existing, widely adopted evidence-based universal prevention curriculum (MMH) to respond to emerging drug issues among youth. We will pilot test RAPD in ten middle schools serving diverse student populations using a two-group, mixed method, cluster randomized controlled trial design. Aim 3: assess the costs and benefits of RAPD from multiple partner perspectives using a mixed methods approach. Discussion This study focuses on designing and deploying implementation strategies to reduce the detrimental impact of emerging drugs and provide an infrastructure to make future adaptations that can be applied in other contexts. After Action Review (AAR) provides a valuable opportunity to review the statewide response to past drug use events, specifically the vaping crisis, using the MMH curriculum, which can systematically guide implementation strategy selection and deployment to meet identified gaps. The rationale for the proposed research is that designing and testing RAPD will advance implementation science in responding to urgent public health events and ensure equitable responses across youth populations. Trial registration ClinicalTrials.gov NCT05806840 .","['Medicine & Public Health', 'Medicine/Public Health, general', 'Biomedicine, general', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Public Health', 'Biomedical Research', 'Biostatistics', 'Clinical Research', 'Clinical Medicine', 'Health Care']"
Springer,10.1007/s13320-025-0750-8,Optical Fiber Hydrogen Sensor Based on π-Phase-Shifted Grating and Sputtered Pd/Hf Composite Film,"A novel optical fiber hydrogen sensor based on the π -phase-shifted grating and partial coated Pd/Hf composite film is proposed and experimentally demonstrated in this paper. The hydrogen sensitive Pd/Hf film with the length of 4 mm is successfully deposited in the π -phase-shifted grating region by the magnetron sputtering process and rotating fixture technology. Since the hydrogen sensitivity between the notch and flank wavelengths of the π -phase-shifted grating is different due to the partial coating only on the π -phase-shifted grating region, the relative shift between the notch and flank wavelengths is employed to characterize the hydrogen concentration in this paper. The hydrogen calibration results show that the sensor shows the good response and repeatability. At the temperature of 20 °C and the hydrogen concentration of 2%, the wavelength distance shifts of 200 nm and 500 nm Pd/Hf coatings are 12.6 pm and 33.5 pm, respectively.","['Physics', 'Optics, Lasers, Photonics, Optical Devices', 'Measurement Science and Instrumentation', 'Microwaves, RF and Optical Engineering', 'Optics and Photonics', 'Photonics and Optical Engineering', 'Microwaves, RF Engineering and Optical Communications']"
Springer,10.1007/s13320-025-0744-6,Two-Dimensional Reconstructed Image of a Subsurface Structure Using Continuous Scanned Photothermal Imaging,"This study presents the two-dimensional (2D) image of a subsurface structure reconstructed using an imaging method based on the photothermal effect. The photothermal imaging method is based on the deflection method using two lasers: pump and probe lasers. A continuous scanning technique is proposed for 2D ( x - and y -directions) surface scanning. The continuous scanning method is compared with the conventional point-by-point scanning technique, and a low-pass fast Fourier transform filter and a Marr-Hildreth detector are found to produce significant results. The photothermal imaging method with continuous 2D surface scanning is performed on three copper-resin double-layer samples with different subsurface structures. The subsurface structures of the copper-resin double-layer samples comprise a square block of 5×5 mm^2 area and blocks shaped as the alphabet letters “T” and “F”. The letters are 3 mm wide and 10×13 mm^2 in area. All three shapes are 1 mm thick and located at a depth of 0.5 mm from the surface of the copper block. The reconstructed photothermal images show an absolute error within 0.122 mm compared with the actual subsurface structure, equivalent to a 2.3% relative error.","['Physics', 'Optics, Lasers, Photonics, Optical Devices', 'Measurement Science and Instrumentation', 'Microwaves, RF and Optical Engineering', 'Optics and Photonics', 'Photonics and Optical Engineering', 'Microwaves, RF Engineering and Optical Communications']"
Springer,10.1007/s00707-025-04229-5,Thorough investigation of exact wave solutions in nonlinear thermoelasticity theory under the influence of gravity using advanced analytical methods,"This work aims to develop a deeper understanding of wave propagation in thermoelastic materials by deriving exact wave solutions for governing equations that account for gravity and temperature-dependent material properties. The study employs the improved modified extended tanh-function method (IMETFM) to address the coupled thermal and mechanical behaviorsin these materials, enabling the formulation of analytical solutions that capture complex wave phenomena. By extending the traditional tanh-function approach, the IMETFM allows for the derivation of diverse wave structures, including hyperbolic, polynomial, exponential, combo dark soliton, bright soliton, singular soliton, rational, and Jacobi elliptic solutions. These solutions are characterized by free parameters, offering thermoelastic in analyzing various physical scenarios. The study provides detailed graphical representations of key results, including stress tensors, displacement fields, and temperature distributions, offering visual insights into the intricate interactions within thermoelastic systems. The study’s findings emphasize the critical role of gravity and temperature dependence in shaping wave propagation and aim to advance theoretical understanding while offering potential applications in material science and engineering.","['Engineering', 'Theoretical and Applied Mechanics', 'Classical and Continuum Physics', 'Solid Mechanics', 'Engineering Fluid Dynamics', 'Vibration, Dynamical Systems, Control', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Mechanical Engineering', 'Classical and Continuum Physics', 'Solid Mechanics', 'Engineering Fluid Dynamics', 'Multibody Systems and Mechanical Vibrations', 'Engineering Thermodynamics, Heat and Mass Transfer']"
Springer,10.1007/s12564-025-10035-4,Three paradigms of inquiry into self-regulated learning (SRL): a critical analysis and ways to transformative and integrated practices,"This paper aims to review and conceptualize how researchers with different human interests understand and approach self-regulated learning (SRL) for their specific purposes. In this narrative literature review, three paradigms of inquiry into SRL guided by Habermas's three human interests (i.e., technical interests, practical interests, and critical/emancipatory interests) are presented in an interpretive way. It was found that, with various human interests, researchers' definitions of SRL, the understandings of the roles of students and social contexts, inquiry interests and intentions, and the methodologies adopted vary. By incorporating Habermas's three human interests into SRL inquiries, this article offers a comprehensive and critical narrative review of the three paradigms in SRL. Discussions about the extension of the established paradigm and the alternative approaches to SRL, emancipatory transformative practices, and the strengths and weaknesses of the three paradigms and their integrated application provide insights and implications for SRL research and practices.","['Education', 'Education, general', 'Education']"
Springer,10.1007/s42452-025-06462-y,"Fingerprint localization based on hybrid TOA, AOA, and RSS measurements","This paper presents a novel joint TOA-AOA-RSS fingerprint localization method aimed at reducing the necessity for collecting extensive fingerprint data for database construction. Initially, The Time-of-Arrival(TOA) is employed for coarse localization to narrow down the fingerprint database. Subsequently, Angle-of-Arrivals(AOAs) and Received Signal Strengths(RSSs) were used as feature parameters to construct the fingerprint database together with the coarse localization coordinates obtained from the TOA. Finally, a matching algorithm is utilized to determine the coordinate of the localization point. Additionally, we introduce an improved self adaptive weighted K nearest neighbor (ISAWKNN) algorithm is proposed based on Hybrid TOA,AOA, and RSS measurements. Simulation results illustrate the proposed algorithm improving the localization accuracy.","['Engineering', 'Engineering, general', 'Materials Science, general', 'Earth Sciences, general', 'Applied and Technical Physics', 'Chemistry/Food Science, general', 'Environment, general', 'Life Sciences', 'Chemistry', 'Earth and Environmental Sciences', 'Technology and Engineering', 'Materials Science', 'Applied and Technical Physics']"
Springer,10.1038/s41598-025-86524-8,Distribution changes of Ormosia microphylla under different climatic scenarios,"Ormosia microphylla is a nationally prioritized wild plant in China but effects of likely future climate change have been poorly studied. Here distribution data of O. microphylla and environmental data with an optimized MaxEnt maximum entropy model were used to predict potentially suitable areas under current and future climate scenarios. The results showed that with future climate warming, the total area suitable for O. microphylla might gradually increase. In the three future periods (2030s (2021–2040), 2050s (2041–2060) and 2090s (2081–2100)), the medium and high suitable areas under different climate scenarios generally showed an expanding trend, while the low suitable areas mostly showed a decreasing trend. At the same time, the potential suitable areas for O. microphylla in China have shown a certain degree of migration trend towards higher latitudes in the north and northwest, as well as a trend towards higher altitudes. The research results will provide data support for the protection of germplasm resources and the development of artificial cultivation techniques for O. microphylla , and provide a theoretical basis for the protection of other rare and endangered plants.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1007/s00414-025-03421-5,"iForensic, multicentric validation of digital whole slide images (WSI) in forensic histopathology setting according to the College of American Pathologists guidelines","Pathology has benefited from the rapid progress of image-digitizing technology during the last decade. However, the application of digital whole slide images (WSI) in forensic pathology still needs to be improved. WSI validation is crucial to ensure diagnostic performance, at least equivalent to glass slides and light microscopy. The College of American Pathologists Pathology and Laboratory Quality Center recently updated internal digital pathology system validation recommendations. Following these guidelines, this pilot study aimed to validate the performance of a digital approach for forensic histopathological diagnosis. Six independent skilled forensic pathologists from different forensic medicine institutes evaluated 100 glass slides of forensic interest (80 stained with standard hematoxylin and eosin, 20 with special staining), including different organs and tissues, with light microscopy (Olympus BX51, Tokyo, Japan). Glass slides were scanned using the Aperio GT 450 DX Digital Slides Scanner (Leica Biosystems, Nussloch, Germany). After two wash-out weeks, forensic pathologists evaluated WSIs in front of a widescreen using computer devices with dedicated software (O3 viewer, O3 Enterprise, Zucchetti, Trieste, Italy). Side-by-side comparisons between diagnoses performed on tissue glass slides versus WSIs were above the threshold stated in the validation guidelines (mean concordance of 97.8%). CSUQ Version 3 questionnaire showed high satisfaction for all pathologists (mean result: 6.6/7). Our institutional digital forensic pathology system has been validated for practical casework application. This approach opens new scenarios in practical forensic casework investigations, such as sharing live histological ex-glass slides online, as well as educational and research perspectives, with improving impacts on the whole daily workflow.","['Medicine & Public Health', 'Forensic Medicine', 'Medical Law', 'Medicine/Public Health, general', 'Forensic Medicine', 'Medical Law', 'Public Health']"
Springer,10.1038/s41698-025-00811-1,A multi-modal deep learning model for prediction of Ki-67 for meningiomas using pretreatment MR images,"This study developed and validated a deep learning network using baseline magnetic resonance imaging (MRI) to predict Ki-67 status in meningioma patients. A total of 1239 patients were retrospectively recruited from three hospitals between January 2010 and December 2023, forming training, internal validation, and two external validation cohorts. A representation learning framework was utilized for modeling, and performance was assessed against existing methods. Furthermore, Kaplan–Meier survival analysis was conducted to investigate whether the model could be used for tumor growth prediction. The model achieved superior results, with areas under the curve (AUCs) of 0.797 for internal testing and 0.808 for generalization, alongside 0.756 and 0.727 for 3- and 5-year tumor growth predictions, respectively. The prediction was significantly associated with the growth of asymptomatic small meningiomas. Overall, the model provides an effective tool for early prediction of Ki-67 and tumor volume growth, aiding in individualized patient management.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Internal Medicine', 'Cancer Research', 'Human Genetics', 'Oncology', 'Gene Therapy', 'Public Health', 'Internal Medicine', 'Cancer Biology', 'Medical Genetics', 'Oncology', 'Clinical Genetics']"
Springer,10.1007/s40520-025-02927-7,Viral infections of the central nervous system increase the risk of knee osteoarthritis: a two-sample mendelian randomization study,"Objective Osteoarthritis (OA) represents a condition under the influence of central nervous system (CNS) regulatory mechanisms. This investigation aims to examine the causal association between viral infections of the central nervous system (VICNS) and inflammatory diseases of the central nervous system (IDCNS) and knee osteoarthritis (KOA) at the genetic level. Methods In this investigation, VICNS and IDCNS were considered as primary exposure variables, while KOA served as the primary outcome. Employing a two-sample mendelian randomization (MR) approach, we conducted an analysis utilizing summary data derived from genome-wide association studies (GWAS). The GWAS summary data pertaining to VICNS and IDCNS were procured from the Finnish consortium, whereas the IEU OpenGWAS database furnished the requisite data for KOA. To ensure the robustness of our genetic causal assessment, a comprehensive array of sensitivity analyses was undertaken, encompassing evaluations of heterogeneity, horizontal pleiotropy, outlier identification, leave-one-out analyses, and assessment of the normal distribution. Results The results of the MR analyses revealed a suggestive positive genetic causal relationship between VICNS and KOA ( P  = 0.012, odds ratio [OR] with a 95% confidence interval [CI] of 1.033 [1.007–1.059]). Conversely, the MR analyses did not indicate any evidence of genetic causation between IDCNS and KOA ( P  = 0.575, OR 95% CI = 0.986 [0.940–1.035]). Importantly, the genetic causal assessment of the exposure and outcome variables did not demonstrate any indications of heterogeneity, horizontal pleiotropy, or outliers. Furthermore, this assessment remained robust against the influence of individual single nucleotide polymorphisms (SNPs) and exhibited adherence to a normal distribution. Conclusion The result of this study has elucidated a suggestive positive genetic causal link between the VICNS and KOA. However, no such genetic causal relationship was observed between the IDCNS and KOA. These findings substantiate the genetic underpinnings supporting the association between the CNS and OA.","['Medicine & Public Health', 'Geriatrics/Gerontology', 'Geriatrics']"
Springer,10.1186/s40658-024-00714-3,Quantitative accuracy of preclinical in ovo PET/MRI: influence of attenuation and quantification methods,"Aim The combination of positron emission tomography (PET) and magnetic resonance imaging (MRI) provides an innovation leap in the use of fertilized chicken eggs ( in ovo model) in preclinical imaging as PET/MRI enables the investigation of the chick embryonal organ-specific distribution of PET-tracers. However, hybrid PET/MRI inheres technical challenges in quantitative in ovo PET such as attenuation correction (AC) for the object as well as for additional hardware parts present in the PET field-of-view, which potentially contribute to quantification biases in the PET images if not accounted for. This study aimed to investigate the influence of the different sources of attenuation on in ovo PET/MRI and assess the accuracy of MR-based AC for in ovo experiments. Method An in-house made chicken egg phantom was used to investigate the magnitude of self-attenuation and the influence of the MRI hardware on the PET signal. The phantom was placed in a preclinical PET/MRI system and PET acquisitions were performed without, and after subsequently adding the different hardware parts to the setup. Reconstructions were performed without any AC for the different setups and with subsequently incorporating the hardware parts into the AC. In addition, in ovo imaging was performed using [^18F]FDG and [^68Ga]Ga-Pentixafor, and PET data was reconstructed with the different AC combinations. Quantitative accuracy was assessed for the phantom and the in ovo measurements. Results In general, not accounting for the self-attenuation of the egg and the hardware parts caused an underestimation of the PET signal of around 49% within the egg. Accounting for all sources of attenuation allowed a proper quantification with global offsets of 2% from the true activity. Quantification based on % injected dose per cc (%ID/cc) was similar for the in ovo measurements, regardless of whether hardware parts were included in AC or not, when the injected activity was extracted from the PET images. However, substantial quantification biases were found when the self-attenuation of the egg was not taken into account. Conclusion Self-attenuation of the egg and PET signal attenuation within the hardware parts of the MRI substantially influence quantitative accuracy in in ovo measurements. However, when compensating for the self-attenuation of the egg by a respective AC, a reliable quantification using %ID/cc can be performed even if not accounting for the attenuation of the hardware parts.","['Medicine & Public Health', 'Nuclear Medicine', 'Imaging / Radiology', 'Applied and Technical Physics', 'Computational Mathematics and Numerical Analysis', 'Engineering, general', 'Nuclear Medicine', 'Biological Imaging', 'Applied and Technical Physics', 'Computational Mathematics and Numerical Analysis', 'Technology and Engineering']"
Springer,10.1038/s41398-025-03247-0,Developing multifactorial dementia prediction models using clinical variables from cohorts in the US and Australia,"Existing dementia prediction models using non-neuroimaging clinical measures have been limited in their ability to identify disease. This study used machine learning to re-examine the diagnostic potential of clinical measures for dementia. Data was sourced from the Australian Imaging, Biomarkers, and Lifestyle Flagship Study of Ageing (AIBL) and the Alzheimer’s Disease Neuroimaging Initiative (ADNI). Clinical variables included 21 measures across medical history, hematological and other blood tests, and APOE genotype. Tree-based machine learning algorithms and artificial neural networks were used. APOE genotype was the best predictor of dementia cases and healthy controls. Our results, however, demonstrated that there are limitations when using publicly accessible cohort data that may limit the generalizability and interpretability of such predictive models. Future research should examine the use of routine APOE genetic testing for dementia diagnostics. It should also focus on clearly unifying data across clinical cohorts.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Psychiatry', 'Neurosciences', 'Behavioral Sciences', 'Pharmacotherapy', 'Biological Psychology', 'Public Health', 'Psychiatry', 'Neuroscience', 'Behavioral Neuroscience', 'Therapeutics', 'Biological Psychology']"
Springer,10.1007/s10237-024-01923-6,Impact of lesion preparation-induced calcified plaque defects in vascular intervention for atherosclerotic disease: in silico assessment,"Percutaneous coronary interventions in highly calcified atherosclerotic lesions are challenging due to the high mechanical stiffness that significantly restricts stent expansion. Intravascular lithotripsy (IVL) is a novel vessel preparation technique with the potential to improve interventional outcomes by inducing microscopic and macroscopic cracks to enhance stent expansion. However, the exact mechanism of action for IVL is poorly understood, and it remains unclear whether the improvement in-stent expansion is caused by either the macro-cracks allowing the vessel to open or the micro-cracks altering the bulk material properties. In silico models offer a robust means to examine (a) diverse lesion morphologies, (b) a range of lesion modifications to address these deficiencies, and (c) the correlation between calcium morphology alteration and improved stenting outcomes. These models also help identify which lesions would benefit the most from IVL. In this study, we develop an in silico model of stent expansion to study the effect of macro-crack morphology on interventional outcomes in clinically inspired geometries. Larger IVL-induced defects promote more post-stent lumen gain. IVL seems to induce better stenting outcomes for large calcified lesions. IVL defects that split calcified plaque in two parts are the most beneficial for stenting angioplasty, regardless of the calcified plaque size. Location of the IVL defect does not seem to matter with respect to lumen gain. These findings underscore the potential of IVL to enhance lesion compliance and improve clinical outcomes in PCI. The macroscopic defects induced by IVL seem to have a substantial impact on post-stent outcomes.","['Engineering', 'Theoretical and Applied Mechanics', 'Biomedical Engineering and Bioengineering', 'Biological and Medical Physics, Biophysics', 'Mechanical Engineering', 'Biomedical Engineering and Bioengineering', 'Biophysics']"
Springer,10.1186/s13063-025-08724-x,Large simple randomized controlled trials—from drugs to medical devices: lessons from recent experience,"Randomized controlled trials (RCTs) are the cornerstone of modern evidence-based medicine. They are considered essential to establish definitive evidence of efficacy and safety for new drugs, and whenever possible they should also be the preferred method for investigating new high-risk medical devices. Well-designed studies robustly inform clinical practice guidelines and decision-making, but administrative obstacles have made it increasingly difficult to conduct informative RCTs. The obstacles are compounded for RCTs of high-risk medical devices by extra costs related to the interventional procedure that is needed to implant the device, challenges with willingness to randomize patients throughout a trial, and difficulties in ensuring proper blinding even with sham procedures. One strategy that may help is to promote the wider use of simpler and more streamlined RCTs using data that are collected routinely during healthcare delivery. Recent large simple RCTs have successfully compared the performance of drugs and of high-risk medical devices, against alternative treatments; they enrolled many patients in a short time, limited costs, and improved efficiency, while also achieving major impact. From a task conducted within the CORE-MD project, we report from our combined experience of designing and conducting large pharmaceutical trials during the COVID-19 pandemic, and of planning and coordinating large registry-based RCTs of cardiovascular devices. We summarize the essential principles and utility of large simple RCTs, likely applicable to all interventions but especially in order to promote their wider adoption to evaluate new medical devices.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Biomedicine, general', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Public Health', 'Biomedical Research', 'Biostatistics', 'Clinical Research', 'Clinical Medicine', 'Health Care']"
Springer,10.1007/s12351-024-00897-8,A hybrid decision-making conceptual framework based on generalized information quality under neutrosophic evidence theory: a comparative analysis,"Simulating uncertainty for dealing with actual events is one of Artificial Intelligence's key difficulties and challenges. The ultimate objective for decision-makers is to manage uncertainty, particularly in indeterminate scenarios when it is not the case that the solution of the problem can be expressed with true or false values. As a result, new techniques to facilitate the interpretation of indeterminacy are currently under development. Neutrosophic logic (NL), which addresses the concept of neutralities, extends classic logic, fuzzy logic, paraconsistent logic, intuitionistic logic, and so on. The single-valued neutrosophic set (SVN) is a subclass of neutrosophic sets that has recently been presented. Solving multicriteria decision-making problems is an essential use case for SVNs to be applied. The research objectives of the article are twofold. First, we examine the potential of utilizing single-valued neutrophilic sets in a more efficient manner to address the issue of multicriteria decision-making. Within this framework, our aim is to explore and extend the concept of information quality as an uncertainty measure by comparing it to neutrosophic Dempster–Shafer (D–S) evidence theory in the context of decision-making. As a proposed solution to the aforementioned research objectives, this manuscript suggests and implements a novel conceptual framework for determining and quantifying the similarity measure between SVNSs in a multicriteria decision-making context under the principles of D–S evidence theory. Finally, illustrative case studies are given to support the logic and practicality of the suggested methodology compared to current methodologies.","['Business and Management', 'Operations Research/Decision Theory', 'Operations Research, Management Science', 'Computational Intelligence', 'Operations Research and Decision Theory', 'Operations Research, Management Science', 'Optimization', 'Operations Management']"
Springer,10.1186/s12913-025-12229-3,How do staff work in NHS hospital operations management meetings to support resilience in everyday service delivery? A qualitative study,"Background Operations Management meetings in NHS hospitals provide an opportunity for operational and clinical staff to monitor demand and capacity and manage patient flow. These meetings play an important role in the achievement of resilient performance over time. However, little is known about the work that takes place within these meetings in the United Kingdom’s National Health Service. Methods We conducted a qualitative study observing 29 Operations Management meetings across three English hospitals between June and October 2023. The observations focused on: who was present; how meetings were organised and conducted; what data were used; how decision-making took place; and the types of work that were undertaken. We also conducted 17 semi-structured interviews with divisional leads and meeting chairs. A grounded theory analytic approach involved exploring the data in two sites to identify key themes, and then testing these themes through a third comparator site. Results We identified the type and extent of work that took place in these meetings to maintain flow and enable resilient service delivery. Operations Management meetings provided an opportunity for staff to come together to engage in collective sense-making, to develop a shared mental model of the state of the hospital and to build a collective understanding of where action was needed. Review of centralised data, formally encoded and recorded in numerical form, played an important role, but staff also drew on local intelligence to make sense of and adapt to often pressurised situations. We identified three types of work: Sense-making and Interpretation, and Risk work (which together contributed to maintaining organisational function) and Maintaining morale (which supported individual staff resilience). Conclusions The work that went on in Operations Management meetings functioned to support organisational and individual resilience, through staff repeatedly sharing and assessing information on capacity and demand, taking action to address these continually changing pressures, and having their efforts recognised.","['Medicine & Public Health', 'Public Health', 'Health Administration', 'Health Informatics', 'Nursing Research', 'Health Care', 'Health Care Management', 'Health Economics', 'Health Policy', 'Medical and Health Technologies']"
Springer,10.1186/s40561-024-00340-7,Impact of an adaptive environment based on learning analytics on pre-service science teacher behavior and self-regulation,"Learning analytics provides valuable data to inform the best decisions for each learner. This study, based on adaptive environment (AE) learning analytics dashboards, examines how instructor interventions affect student self-regulation abilities and academic performance. It identifies the self-regulation categories requiring the most support to correct learning paths. Little is known about how interventions in an AE can influence learners' self-regulation based on performance indicators, particularly in science education. The study included 95 Faculty of Education and the Department of Science students. Using a longitudinal clustering approach, researchers identified three unique self-regulated learning (SRL) profiles: oriented, adaptive, and minimally self-regulated learners. While the results showed that the learning analyses were useful in guiding the process of appropriate interventions through an adaptive environment for each student by providing indicators and raising the level of self-regulation for each group separately, the results also showed that there was no change in the classification of self-regulation into groups and that no students moved between groups. These findings highlight the complexity of SRL, suggesting that while interventions can impact engagement and behavior, they may not be sufficient to change the learner's underlying profile. In academic performance, statistically significant differences were found, with the oriented self-regulation group outperforming the adaptive and minimally self-regulated groups. The findings underscore the importance of learning analytics and their indicators for timely interventions in adaptive environments. Additionally, the AE was highly effective, offering students opportunities to review material, which improved their study techniques, test-taking strategies, and overall learning experience.","['Education', 'Education, general', 'Computers and Education', 'Education', 'Computers and Education']"
Springer,10.1007/s10639-025-13355-5,Teaching of the subject ‘Biomolecules in Living Organisms’ using 3D printing models,"In recent years, 3D printing technology or 3D printing models have become a powerful educational tool used in many fields such as medicine, engineering and science. However, research on the integration of these technologies into formal educational environments and the researches examining their effect on students’ learning biology is quite limited. The aim of this study is to examine the effect of using 3D printing models on students’ learning the subject ‘Biomolecules in Living Organisms’. In the study, the students were also interviewed, and the usefulness of 3D printing models was evaluated. For this aim, a quasi-experimental design was used in the study. The study group consisted of 61 ninth grade students (37 females, 24 males), 32 students in the control group and 29 students in the experimental group, attending a public high school during the academic year 2023–2024. The findings indicated that the achievement levels of the students in the experimental group (3D printing models) were statistically higher than those in the control groups (non-3D printing models) after a 9-week instruction. The findings also indicated that there is a statistically important difference in students’ scores on retention tests in favour of the experimental group. Similarly, as a result of student interviews, it was determined that 3D models were evaluated by most of the students as materials that were interesting, arousing curiosity, increased the memorability of information, concretized the subject and facilitated learning. Despite these positive effects of 3D printing models on learning, some limitations, such as small sample selection, reduced the generalizability of the study results. Considering these limitations, some suggestions for future research were made.","['Computer Science', 'Computers and Education', 'Educational Technology', 'User Interfaces and Human Computer Interaction', 'Education, general', 'Information Systems Applications (incl.Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Computers and Education', 'Digital Education and Educational Technology', 'User Interfaces and Human Computer Interaction', 'Education', 'Computer and Information Systems Applications', 'Computer Application in Social and Behavioral Sciences']"
Springer,10.1007/s10915-024-02785-x,A Rigorous Integrator and Global Existence for Higher-Dimensional Semilinear Parabolic PDEs via Semigroup Theory,"In this paper, we introduce a general constructive method to compute solutions of initial value problems of semilinear parabolic partial differential equations on hyper-rectangular domains via semigroup theory and computer-assisted proofs. Once a numerical candidate for the solution is obtained via a finite dimensional projection, Chebyshev series expansions are used to solve the linearized equations about the approximation from which a solution map operator is constructed. Using the solution operator (which exists from semigroup theory), we define an infinite dimensional contraction operator whose unique fixed point together with its rigorous bounds provide the local inclusion of the solution. Applying this technique for multiple time steps leads to constructive proofs of existence of solutions over long time intervals. As applications, we study the 3D/2D Swift–Hohenberg, where we combine our method with explicit constructions of trapping regions to prove global existence of solutions of initial value problems converging asymptotically to nontrivial equilibria. A second application consists of the 2D Ohta–Kawasaki equation, providing a framework for handling derivatives in nonlinear terms.","['Mathematics', 'Algorithms', 'Computational Mathematics and Numerical Analysis', 'Mathematical and Computational Engineering', 'Theoretical, Mathematical and Computational Physics', 'Algorithms', 'Computational Mathematics and Numerical Analysis', 'Mathematical and Computational Engineering Applications', 'Theoretical, Mathematical and Computational Physics']"
Springer,10.1007/s11097-024-10044-5,Can communication Brain-Computer Interfaces read minds?,"Recent developments in the domain of communication Brain-Computer Interface (BCI) technology have raised questions about the ability for communication BCIs to read minds. How those questions are answered depends on how we theorize the mind and mindreading in the first place. Thus, in this paper, I ask (1) what does it mean to read minds? (2) can a communication BCI do this? (3) what does this mean for potential users of this technology? and (4) what is at stake morally in light of this? I show that current answers to these questions are conceptually unclear and committed to a Cartesian picture of the mind and its relation to the brain, questionably informing how debates about BCIs as mindreading devices are framed. I offer an alternative perspective on these questions by turning to an enactive perspective on mindedness. I argue that this perspective can offer conceptual as well as ethical clarification about what is at stake in the domain of communication BCIs. From this perspective, the concerns raised about BCIs as mindreading machines are demystified. Instead, concerns are raised about BCIs as enabling users to flourish as authentic communicators.","['Philosophy', 'Phenomenology', 'Philosophy of Mind', 'Cognitive Psychology', 'Phenomenology', 'Philosophy of Mind', 'Cognitive Psychology']"
Springer,10.1007/s11263-024-02207-3,"R
              
                
              
              $$^{2}$$
              
                
                  
                  
                  2
                
              
            S100K: Road-Region Segmentation Dataset for Semi-supervised Autonomous Driving in the Wild","Semantic understanding of roadways is a key enabling factor for safe autonomous driving. However, existing autonomous driving datasets provide well-structured urban roads while ignoring unstructured roadways containing distress, potholes, water puddles, and various kinds of road patches i.e., earthen, gravel etc. To this end, we introduce Road Region Segmentation dataset (R^2S100K)—a large-scale dataset and benchmark for training and evaluation of road segmentation in aforementioned challenging unstructured roadways. R^2S100K comprises 100K images extracted from a large and diverse set of video sequences covering more than 1000 km of roadways. Out of these 100K privacy respecting images, 14,000 images have fine pixel-labeling of road regions, with 86,000 unlabeled images that can be leveraged through semi-supervised learning methods. Alongside, we present an Efficient Data Sampling based self-training framework to improve learning by leveraging unlabeled data. Our experimental results demonstrate that the proposed method significantly improves learning methods in generalizability and reduces the labeling cost for semantic segmentation tasks. Our benchmark will be publicly available to facilitate future research at https://r2s100k.github.io/ .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Computer Vision', 'Automated Pattern Recognition']"
Springer,10.1007/s10869-024-09947-6,Navigating Through the Digital Workplace: Measuring Leader Digital Competence,"In a modern digital workplace, leaders must have the necessary skills to lead employees virtually. Despite its high practical and theoretical relevance, a consensus on crucial digital competencies for virtual leaders is lacking, hindering a systematic exploration of the leader’s role in facilitating technology use. In the present article, we propose a new concept and instrument to assess leader digital competence (LDC). After reviewing the literature, we establish three dimensions of LDC, centering around the leader’s ability and inclination to select, promote, and enable technology and digital media among their employees. We provide support for the scale's convergent, discriminant, criterion-related, and incremental validity using four independent samples ( N _ 1  = 156, N _ 2  = 309, N _ 3  = 201, N _ 4 employee = 452, N _ 4 leader = 93). Furthermore, results support the reliability and factor structure with the three proposed dimensions of the 10-item LDC scale. The findings demonstrate that the scale represents a psychometrically sound instrument, useful for further examining conditions for effectiveness in the virtual environment. Future research should aim to advance the understanding of antecedents and situational factors that influence the relevance of LDC and its impact on employee, team, and organizational outcomes.","['Psychology', 'Industrial and Organizational Psychology', 'Community and Environmental Psychology', 'Personality and Social Psychology', 'Business and Management, general', 'Social Sciences, general', 'Work and Organizational Psychology', 'Community Psychology', 'Personality and Differential Psychology', 'Business and Management', 'Society']"
Springer,10.1007/s11263-024-02199-0,Learning General and Specific Embedding with Transformer for Few-Shot Object Detection,"Few-shot object detection (FSOD) studies how to detect novel objects with few annotated examples effectively. Recently, it has been demonstrated that decent feature embeddings, including the general feature embeddings that are more invariant to visual changes and the specific feature embeddings that are more discriminative for different object classes, are both important for FSOD. However, current methods lack appropriate mechanisms to sensibly cooperate both types of feature embeddings based on their importance to detecting objects of novel classes, which may result in sub-optimal performance. In this paper, to achieve more effective FSOD, we attempt to explicitly encode both general and specific feature embeddings using learnable tensors and apply a Transformer to help better incorporate them in FSOD according to their relations to the input object features. We thus propose a Transformer-based general and specific embedding learning (T-GSEL) method for FSOD. In T-GSEL, learnable tensors are employed in a three-stage pipeline, encoding feature embeddings in general level, intermediate level, and specific level, respectively. In each stage, we apply a Transformer to first model the relations of the corresponding embedding to input object features and then apply the estimated relations to refine the input features. Meanwhile, we further introduce cross-stage connections between embeddings of different stages to make them complement and cooperate with each other, delivering general, intermediate, and specific feature embeddings stage by stage and utilizing them together for feature refinement in FSOD. In practice, a T-GSEL module is easy to inject. Extensive empirical results further show that our proposed T-GSEL method achieves compelling FSOD performance on both PASCAL VOC and MS COCO datasets compared with other state-of-the-art approaches.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Computer Vision', 'Automated Pattern Recognition']"
Springer,10.1007/s11263-024-02210-8,Out-of-Distribution Detection with Virtual Outlier Smoothing,"Detecting out-of-distribution (OOD) inputs plays a crucial role in guaranteeing the reliability of deep neural networks (DNNs) when deployed in real-world scenarios. However, DNNs typically exhibit overconfidence in OOD samples, which is attributed to the similarity in patterns between OOD and in-distribution (ID) samples. To mitigate this overconfidence, advanced approaches suggest the incorporation of auxiliary OOD samples during model training, where the outliers are assigned with an equal likelihood of belonging to any category. However, identifying outliers that share patterns with ID samples poses a significant challenge. To address the challenge, we propose a novel method, V irtual O utlier S m o othing (VOSo), which constructs auxiliary outliers using ID samples, thereby eliminating the need to search for OOD samples. Specifically, VOSo creates these virtual outliers by perturbing the semantic regions of ID samples and infusing patterns from other ID samples. For instance, a virtual outlier might consist of a cat’s face with a dog’s nose, where the cat’s face serves as the semantic feature for model prediction. Meanwhile, VOSo adjusts the labels of virtual OOD samples based on the extent of semantic region perturbation, aligning with the notion that virtual outliers may contain ID patterns. Extensive experiments are conducted on diverse OOD detection benchmarks, demonstrating the effectiveness of the proposed VOSo. Our code will be available at https://github.com/junz-debug/VOSo .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Computer Vision', 'Automated Pattern Recognition']"
Springer,10.1007/s11263-024-02206-4,AROID: Improving Adversarial Robustness Through Online Instance-Wise Data Augmentation,"Deep neural networks are vulnerable to adversarial examples. Adversarial training (AT) is an effective defense against adversarial examples. However, AT is prone to overfitting which degrades robustness substantially. Recently, data augmentation (DA) was shown to be effective in mitigating robust overfitting if appropriately designed and optimized for AT. This work proposes a new method to automatically learn online, instance-wise, DA policies to improve robust generalization for AT. This is the first automated DA method specific for robustness. A novel policy learning objective, consisting of Vulnerability, Affinity and Diversity, is proposed and shown to be sufficiently effective and efficient to be practical for automatic DA generation during AT. Importantly, our method dramatically reduces the cost of policy search from the 5000 h of AutoAugment and the 412 h of IDBH to 9 h, making automated DA more practical to use for adversarial robustness. This allows our method to efficiently explore a large search space for a more effective DA policy and evolve the policy as training progresses. Empirically, our method is shown to outperform all competitive DA methods across various model architectures and datasets. Our DA policy reinforced vanilla AT to surpass several state-of-the-art AT methods regarding both accuracy and robustness. It can also be combined with those advanced AT methods to further boost robustness. Code and pre-trained models are available at: https://github.com/TreeLLi/AROID .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Computer Vision', 'Automated Pattern Recognition']"
Springer,10.1007/s11263-024-02187-4,SplitNet: Learnable Clean-Noisy Label Splitting for Learning with Noisy Labels,"Annotating the dataset with high-quality labels is crucial for deep networks’ performance, but in real-world scenarios, the labels are often contaminated by noise. To address this, some methods were recently proposed to automatically split clean and noisy labels among training data, and learn a semi-supervised learner in a Learning with Noisy Labels (LNL) framework. However, they leverage a handcrafted module for clean-noisy label splitting, which induces a confirmation bias in the semi-supervised learning phase and limits the performance. In this paper, for the first time, we present a learnable module for clean-noisy label splitting, dubbed SplitNet, and a novel LNL framework which complementarily trains the SplitNet and main network for the LNL task. We also propose to use a dynamic threshold based on split confidence by SplitNet to optimize the semi-supervised learner better. To enhance SplitNet training, we further present a risk hedging method. Our proposed method performs at a state-of-the-art level, especially in high noise ratio settings on various LNL benchmarks.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Computer Vision', 'Automated Pattern Recognition']"
Springer,10.1007/s00259-024-06961-x,Dopaminergic PET to SPECT domain adaptation: a cycle GAN translation approach,"Purpose Dopamine transporter imaging is routinely used in Parkinson’s disease (PD) and atypical parkinsonian syndromes (APS) diagnosis. While [^11C]CFT PET is prevalent in Asia with a large APS database, Europe relies on [^123I]FP-CIT SPECT with limited APS data. Our aim was to develop a deep learning-based method to convert [^11C]CFT PET images to [^123I]FP-CIT SPECT images, facilitating multicenter studies and overcoming data scarcity to promote Artificial Intelligence (AI) advancements. Methods A CycleGAN was trained on [^11C]CFT PET ( n  = 602, 72%PD) and [^123I]FP-CIT SPECT ( n  = 1152, 85%PD) images from PD and non-parkinsonian control (NC) subjects. The model generated synthetic SPECT images from a real PET test set ( n  = 67, 75%PD). Synthetic images were quantitatively and visually evaluated. Results Fréchet Inception Distance indicated higher similarity between synthetic and real SPECT than between synthetic SPECT and real PET. A deep learning classification model trained on synthetic SPECT achieved sensitivity of 97.2% and specificity of 90.0% on real SPECT images. Striatal specific binding ratios of synthetic SPECT were not significantly different from real SPECT. The striatal left-right differences and putamen binding ratio were significantly different only in the PD cohort. Real PET and real SPECT had higher contrast-to-noise ratio compared to synthetic SPECT. Visual grading analysis scores showed no significant differences between real and synthetic SPECT, although reduced diagnostic performance on synthetic images was observed. Conclusion CycleGAN generated synthetic SPECT images visually indistinguishable from real ones and retained disease-specific information, demonstrating the feasibility of translating [^11C]CFT PET to [^123I]FP-CIT SPECT. This cross-modality synthesis could enhance further AI classification accuracy, supporting the diagnosis of PD and APS.","['Medicine & Public Health', 'Nuclear Medicine', 'Imaging / Radiology', 'Orthopedics', 'Cardiology', 'Oncology', 'Nuclear Medicine', 'Biological Imaging', 'Orthopaedics', 'Cardiology', 'Oncology']"
Springer,10.1007/s11600-024-01383-7,Azimuthal crustal variations and their implications on the seismic impulse response in the Valley of Mexico,"Previous studies have suggested prominent variations in the seismic wave behavior at the 5 s period when traveling across the Valley of Mexico, associating them with the crustal structure and contributing to the anomalous seismic wave patterns observed each time an earthquake hits Mexico City. This article confirms the variations observed at 0.2 Hz by analyzing the Green tensor diagonal retrieved from empirical Green functions (EGF) calculated using seismic noise data cross-correlations of the vertical and horizontal components. We observe time and phase shifts between the east and north EGF components and show that they can be explained by the crustal structure from the surface up to 20 km depth; we also observe that the time and phase shifts are more significant if the distance between the source and the station increases. Additionally, the article presents an updated version of the velocity model from receiver functions and dispersion curves (VMRFDC v2.0) for the crustal structure under the Valley of Mexico. To validate this model, we compare the EGFs with synthetic Green functions determined numerically. To do so, we adaptatively meshed this model using an iterative algorithm to numerically simulate the impulse response up to 0.5 Hz. Finally, the comparisons between noise and synthetic EGF showed that the VMRFDC v2.0 model explains the time shifts and phase differences at 0.2 Hz, previously observed by independent studies, suggesting it correctly characterizes the crustal structure anomalies beneath the Valley of Mexico.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences', 'Geophysics', 'Geology', 'Geotechnical Engineering and Applied Earth Sciences']"
Springer,10.1007/s13346-024-01679-7,3D printed microneedles: revamping transdermal drug delivery systems,,"['Biomedicine', 'Pharmaceutical Sciences/Technology', 'Pharmaceutics']"
Springer,10.1038/s41433-024-03460-z,Artificial intelligence in assessing progression of age-related macular degeneration,"The human population is steadily growing with increased life expectancy, impacting the prevalence of age-dependent diseases, including age-related macular degeneration (AMD). Health care systems are confronted with an increasing burden with rising patient numbers accompanied by ongoing developments of therapeutic approaches. Concurrent advances in imaging modalities provide eye care professionals with a large amount of data for each patient. Furthermore, with continuous progress in therapeutics, there is an unmet need for reliable structural and functional biomarkers in clinical trials and practice to optimize personalized patient care and evaluate individual responses to treatment. A fast and objective solution is Artificial intelligence (AI), which has revolutionized assessment of AMD in all disease stages. Reliable and validated AI-algorithms can aid to overcome the growing number of patients, visits and necessary treatments as well as maximize the benefits of multimodal imaging in clinical trials. Therefore, there are ongoing efforts to develop and validate automated algorithms to unlock more information from datasets allowing automated assessment of disease activity and disease progression. This review aims to present selected AI algorithms, their development, applications and challenges regarding assessment and prediction of AMD progression.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Ophthalmology', 'Laboratory Medicine', 'Surgery', 'Surgical Oncology', 'Pharmaceutical Sciences/Technology', 'Public Health', 'Ophthalmology', 'Biomedical Research', 'Surgery', 'Cancer Therapy', 'Pharmaceutics']"
Springer,10.1007/s10750-024-05744-7,Warming of shallow temperate lakes: consequences for rotifer community composition and population dynamics,"Taxa specific responses to climate warming may shape aquatic communities, dominance patterns, biotic interactions, and related ecosystem processes and functions. As climate warming effects on smaller zooplankton are less understood than larger zooplankton, we focused on rotifers to study their response to a future climate warming scenario in outdoor mesocosms. Our year-long experiment (14 July 2020 to 13 July 2021) included present temperature conditions as controls and a treatment simulating a future warmer climate involving occasional heatwaves. Total rotifer abundance increased with warming, with Keratella spp. and Polyarthra spp. benefiting the most, while the Kellicottia spp. population collapsed. Filinia spp. were negatively affected by warming in the summer of 2020, but increased during winter and the following summer. Our findings suggest that thermophilic or eurytherm rotifers such as Keratella and Polyarthra may increase in a warmer future, while heat-sensitive Kellicottia may be negatively affected in the temperate region. Milder winters may allow some rotifer genera to proliferate while allowing others to recover from high summer temperatures, thereby considerably changing the composition and dominance patterns of rotifer assemblages.","['Life Sciences', 'Freshwater & Marine Ecology', 'Ecology', 'Zoology', 'Freshwater and Marine Ecology', 'Ecology', 'Zoology']"
Springer,10.1007/s10869-024-09946-7,Validation and Measurement Invariance of the German and Spanish Gender Bias Scale for Women Leaders,"Gender bias still appears to contribute significantly to the gender disparity observed in upper leadership positions. The present study presents the translation, modification, and validation of the Gender Bias Scale for Women Leaders (GBSWL, Diehl et al., 2020 ) into German and Spanish. Using data from four samples of full-time employed women from Germany and Spain with and without leadership responsibility ( N  = 870), we conducted confirmatory factor analyses to establish factorial validity, tested measurement invariance across the different job levels and countries, and tested construct validity. Our results indicate that the original factor structure does not hold in Germany and Spain. Therefore, utilizing the German leader sample as a construction sample, we modified the factor structure and validated the modified version using the remaining three samples. The modified version demonstrated good model fit, had metric measurement invariance across all samples, and resulted in a correlational pattern consistent with theory and the original study. Overall, results suggest that the German and Spanish versions of the GBSWL are reliable and valid instruments that scholars and practitioners can use to advance theory, research, and human resource practice in Germany and Spain.","['Psychology', 'Industrial and Organizational Psychology', 'Community and Environmental Psychology', 'Personality and Social Psychology', 'Business and Management, general', 'Social Sciences, general', 'Work and Organizational Psychology', 'Community Psychology', 'Personality and Differential Psychology', 'Business and Management', 'Society']"
Springer,10.1038/s41433-024-03433-2,Decolonising the ‘red’ reflex test: transitioning from terminology based on colour to anatomy,,"['Medicine & Public Health', 'Medicine/Public Health, general', 'Ophthalmology', 'Laboratory Medicine', 'Surgery', 'Surgical Oncology', 'Pharmaceutical Sciences/Technology', 'Public Health', 'Ophthalmology', 'Biomedical Research', 'Surgery', 'Cancer Therapy', 'Pharmaceutics']"
Springer,10.1007/s40122-024-00702-6,Influence of Intraoperative Pain Management on Postoperative Delirium in Elderly Patients: A Prospective Single-Center Randomized Controlled Trial,"Introduction Intraoperative analgesia and sedation are closely related to postoperative delirium. Depth of sedation based on bispectral index (BIS) guidance has been shown to reduce the occurrence of postoperative delirium (POD). However, the correlation between intraoperative analgesia levels and POD is unclear. The aim of this study was to investigate the effect of intraoperative analgesic management guided by the nociceptive stimulus index (NOX) on postoperative delirium. Methods In this prospective single-center randomized controlled study, elderly patients aged 65 and above, who are scheduled to undergo unilateral total knee arthroplasty (TKA), were allocated into two groups: the routine monitoring group (group R), which solely monitored patient sedation levels using BIS; and the NOX monitoring group (group N), which monitored patient analgesic levels using NOX based on BIS-monitored sedation levels. The primary outcome was the incidence of postoperative delirium within 3 days after surgery, using the confusion assessment method (CAM). Results From May 2022 to December 2022, a total of 240 patients were randomized; 12 were excluded because of failure to meet experimental conditions or were lost to follow-up. Patients in group N had a lower incidence rate (%) of POD on the first day compared to those in group R (8 (7%) vs 18 (16%), P  = 0.041). The dosage of remifentanil administered in group N was significantly higher than that in group R (927.07 ± 268.09 vs 882.32 ± 187.91 mg, P  = 0.002). Conclusions Appropriate intraoperative analgesia guided by NOX is associated with POD. When sedation levels were consistent, the incidence of POD was significantly reduced in older patients with NOX-guided analgesic management during unilateral TKA surgery.","['Medicine & Public Health', 'Internal Medicine', 'Pain Medicine', 'Internal Medicine', 'Pain Medicine']"
Springer,10.1007/s11600-024-01398-0,"Interactive analysis of the results of NET-VISA, a Bayesian inference system, in CTBTO’s International Data Centre bulletin production","The Global Association model is a crucial tool in seismic data analysis at the International Data Centre (IDC) of the Comprehensive Nuclear-Test-Ban Treaty Organization. However, it faces challenges due to its limitations in accurately associating seismic events on a global scale. Over the past years, attempts have been undertaken to tackle these issues by introducing the Network Processing Vertically Integrated Seismic Analysis (NET-VISA) algorithm, specifically designed to enhance seismic event association across the globe. NET-VISA uses a machine learning Bayesian approach to solve the automatic association problem. NET-VISA has been implemented in operation as an additional automatic event scanner tool since January 2018. In this study, we assess the effect of the NET-VISA automatic scanner on the IDC output REB and LEB bulletins. We used three distinct time periods to evaluate the NET-VISA performance. The results show a 4.6% increase in the number of LEB events after including the NET-VISA scanner in operation, with an average of 7 additional events per day, and an increase of 17.90% in the number of scanned events. A comparison between the different bulletins in distinct periods shows NET-VISA is beneficial to build more valid events, providing opportunities to improve nuclear-test-ban monitoring. However, NET-VISA exhibits slightly reduced performance for events occurring at depths exceeding 300 km.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences', 'Geophysics', 'Geology', 'Geotechnical Engineering and Applied Earth Sciences']"
Springer,10.1007/s40122-024-00681-8,Study on the Analgesic Efficacy of Femoral Nerve Block for Post-Hip Arthroscopy Pain,"Introduction Postoperative pain management is challenging for hip arthroscopy, and the effectiveness and specific protocols of femoral nerve block (FNB) in hip surgeries remain insufficient. Therefore, we designed this study to investigate the analgesic efficacy and optimal drug concentrations of FNB after hip arthroscopy. Methods A total of 148 patients undergoing hip arthroscopy were included and randomly divided into three groups: 0.3% ropivacaine FNB group, 0.4% ropivacaine FNB group, and 0.4% ropivacaine intra-articular injection (IAI) group (positive control). The main outcomes included dynamic and static visual analog scale (VAS) scores at various time points postoperatively, total intraoperative remifentanil consumption, and cumulative consumption of morphine within 24 h postoperatively. Secondary outcomes included total intraoperative dexmedetomidine consumption, RASMAY sedation scores, and patients’ satisfaction scores postoperatively. Results Both FNB and IAI anesthesia were shown to be safe for post-hip arthroscopy analgesia. Compared with IAI anesthesia, FNB showed no significant differences in analgesic effect within 12 h postoperatively but had a better analgesic effect after 24 h and lower remifentanil consumption intraoperatively. Group 0.4% ropivacaine showed lower dynamic VAS scores within the first 12 h compared with 0.3% ropivacaine for FNB, however, there were no significant differences in patient satisfaction and sedation, and postoperative ambulation was delayed, indicating that the higher concentration of ropivacaine correlated with a longer time to ambulation. The IAI group had greater intraoperative opioid consumption and more side effects. Conclusions Compared with IAI anesthesia, FNB can better alleviate post-hip arthroscopy pain and reduce opioid consumption. However, it requires specialized equipment and technical support and carries a certain risk of puncture. Trial Registration Chinese Clinical Trials Registry (ChiCTR2400091579).","['Medicine & Public Health', 'Internal Medicine', 'Pain Medicine', 'Internal Medicine', 'Pain Medicine']"
Springer,10.1038/s41433-024-03435-0,Clinical outcomes with a new diffractive multifocal intraocular lens optimized by the dynamic light utilization algorithm,"Background/objectives To evaluate the refractive outcomes, optical performance, and the quality of vision in patients implanted with a new diffractive intraocular lens (IOL), the Intensity Hanita. Subjects/methods This observational, prospective, longitudinal study included 64 eyes underwent bilateral cataract surgery with the Intensity IOL (Hanita Israel) implantation. Main outcome measures after 6 months were the following visual acuities (VAs) of uncorrected and corrected distance (UDVA and CDVA), uncorrected and distance corrected intermediate VAs (UIVA and DCIVA), uncorrected and distance corrected near (UNVA and DCNVA), refraction, slitlamp biomicroscopy, defocus curve (DFC), high ocular aberrations (HOA), contrast sensitivity (CS), optical quality, subjective quality of vision (QoV) and near activity visual questionnaires (NAVQ). Results Sixty-six percent of eyes having UDVA 0.10 logMAR or better. DFC showed maximum vision at distance (0.02 ± 0.07 LogMAR at 0.0 D), with flat decline through intermediate and near vision (0.11 ± 0.08 LogMAR at −1.5 D and 0.12 ± 0.12 at −2.5 D). No significant changes in CS were found (all spatial frequencies, p  ≥ 0.06). The RMS of HOA, coma, trefoil, and SA were 0.21 ± 0.10, 0.10 ± 0.06, 0.11 ± 0.07, and 0.00 ± 0.04 μm and the Strehl ratio was 0.12 ± .04 at 6 months. Subjective symptoms (halos and glare) were reported mild but well tolerated, not causing significant disturbance in daily activities. The NAVQ showed high levels of satisfaction performing daily near-vision tasks. Conclusions The Hanita Intensity diffractive IOL successfully restores all distances of vision. The flat profile of the monocular defocus curve confirms the five-foci distribution principle that provides vision at all ranges while increasing the depth of focus.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Ophthalmology', 'Laboratory Medicine', 'Surgery', 'Surgical Oncology', 'Pharmaceutical Sciences/Technology', 'Public Health', 'Ophthalmology', 'Biomedical Research', 'Surgery', 'Cancer Therapy', 'Pharmaceutics']"
Springer,10.1038/s41586-024-08365-1,Long-lived entanglement of molecules in magic-wavelength optical tweezers,"Realizing quantum control and entanglement of particles is crucial for advancing both quantum technologies and fundamental science. Substantial developments in this domain have been achieved in a variety of systems^ 1 – 5 . In this context, ultracold polar molecules offer new and unique opportunities because of their more complex internal structure associated with vibration and rotation, coupled with the existence of long-range interactions^ 6 , 7 . However, the same properties make molecules highly sensitive to their environment^ 8 – 10 , affecting their coherence and utility in some applications. Here we show that by engineering an exceptionally controlled environment using rotationally magic^ 11 , 12 optical tweezers, we can achieve long-lived entanglement between pairs of molecules using detectable hertz-scale interactions. We prepare two-molecule Bell states with fidelity $$0.92{4}_{-0.016}^{+0.013}$$ 0.92 4 − 0.016 + 0.013 , limited by detectable leakage errors. When correcting for these errors, the fidelity is $$0.97{6}_{-0.016}^{+0.014}$$ 0.97 6 − 0.016 + 0.014 . We show that the second-scale entanglement lifetimes are limited solely by these errors, providing opportunities for research in quantum-enhanced metrology^ 7 , 13 , ultracold chemistry^ 14 and the use of rotational states in quantum simulation, quantum computation and as quantum memories. The extension of precise quantum control to complex molecular systems will enable their additional degrees of freedom to be exploited across many domains of quantum science^ 15 – 17 .","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1038/s41586-024-08318-8,Engineered extrachromosomal oncogene amplifications promote tumorigenesis,"Focal gene amplifications are among the most common cancer-associated mutations^ 1 but have proven challenging to engineer in primary cells and model organisms. Here we describe a general strategy to engineer large (more than 1 Mbp) focal amplifications mediated by extrachromosomal DNAs (ecDNAs)^ 2 in a spatiotemporally controlled manner in cells and in mice. By coupling ecDNA formation with expression of selectable markers, we track the dynamics of ecDNA-containing cells under physiological conditions and in the presence of specific selective pressures. We also apply this approach to generate mice harbouring Cre-inducible Myc - and Mdm2 -containing ecDNAs analogous to those occurring in human cancers. We show that the engineered ecDNAs spontaneously accumulate in primary cells derived from these animals, promoting their proliferation, immortalization and transformation. Finally, we demonstrate the ability of Mdm2 -containing ecDNAs to promote tumour formation in an autochthonous mouse model of hepatocellular carcinoma. These findings offer insights into the role of ecDNA-mediated gene amplifications in tumorigenesis. We anticipate that this approach will be valuable for investigating further unresolved aspects of ecDNA biology and for developing new preclinical immunocompetent mouse models of human cancers harbouring specific focal gene amplifications. Large extrachromosomal DNAs are engineered using a CRISPR- and Cre– loxP -based approach and shown to drive cancer in mouse models, with potential applications in determining the role of oncogene amplifications in human cancers.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1057/s41599-025-04400-2,Social links vs. language barriers: decoding the global spread of streaming content,"The development of the internet has allowed for the global distribution of content, redefining media communication and property structures through various streaming platforms. Previous studies successfully clarified the factors contributing to trends in each streaming service, yet the similarities and differences between platforms are commonly unexplored; moreover, the influence of social connections and cultural similarity is usually overlooked. We hereby examine the social aspects of three significant streaming services–Netflix, Spotify, and YouTube–with an emphasis on the dissemination of content across countries. Using two-year-long trending chart datasets, we find that streaming content can be divided into two types: video-oriented (Netflix) and audio-oriented (Spotify). This characteristic is differentiated by accounting for the significance of social connectedness and linguistic similarity: audio-oriented content travels via social links, but video-oriented content tends to spread throughout linguistically akin countries. Interestingly, user-generated contents, YouTube, exhibits a dual characteristic by integrating both visual and auditory characteristics, indicating the platform is evolving into unique medium rather than simply residing a midpoint between video and audio media.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Humanities and Social Sciences, multidisciplinary', 'Humanities and Social Sciences', 'Behavioral Sciences and Psychology', 'Business and Management']"
Springer,10.1038/s41598-025-87692-3,The application of mind mapping in the standardized education of inpatient physicians in nephrology,"This study aims to investigate the effectiveness of mind mapping in the standardized training and education of internal medicine resident physicians in nephrology. Sixty trainees undergoing rotations in the Nephrology Department at Chengdu University Affiliated Hospital between January 2021 and December 2023 were randomly assigned to control and observation groups, each comprising 30 trainees. The observation group received mind mapping teaching, while the control group received traditional teaching methods. The clinical thinking training of the two groups was then compared. The observation group outperformed the control group in all aspects of the Mini-CEX assessment, including medical interviews, humanistic care, clinical judgment, communication skills, overall performance, and total score, with statistical significance ( P  < 0.05). However, there were no statistically significant differences between the two groups in the areas of physical examination and organizational effectiveness ( P  > 0.05). Furthermore, the observation group achieved higher scores than the control group in the final theory exam, skill exam results, and satisfaction, with statistically significant differences ( P  < 0.05). Mind mapping not only aids in the consolidation of fundamental memorized knowledge but also enhances clinical thinking skills, thereby playing a pivotal role in assisting resident physicians in establishing a comprehensive knowledge base in nephrology and improving the effectiveness of their training.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1038/s41612-024-00855-3,Vertical and spatial differences in ozone formation sensitivities under different ozone pollution levels in eastern Chinese cities,"Ozone is the primary air pollutant in eastern China during the warm season. Clarifying the differences in the spatio–temporal evolution of the ozone formation sensitivity between ozone polluted days and clean air days is key for the precise formulation of ozone prevention policies. By combining ground–and satellite–based remote sensing with ground station observations, we identified large spatio–temporal differences in the ozone formation sensitivity in eastern Chinese cities under different ozone pollution levels. Diurnally, the NO_2 concentration was higher in the morning and lower at noon on the ozone exceedance days. The HCHO concentration was higher throughout the day, and the transition limited regime or NO_x–limited regime contributed more to the ozone formation sensitivity on the ozone exceedance days. Vertically, the ratio of HCHO to NO_2 (FNR) was higher on ozone exceedance days, and the contributions of NO_x–limited regime at 0–2 km and the transition limited regime at 0–1 km on ozone exceedance days increased considerably. Spatially, HCHO in the North China Plain and middle–lower Yangtze River Plain was significantly increased on ozone exceedance days, while the NO_2 concentration in the southeast hills was increased on ozone exceedance days. The difference in FNR values between northern and southern cities in eastern China on O_3 exceedance days narrowed, and the ozone formation sensitivity in eastern China tended to be under a transition limited regime. The shifts in the ozone formation sensitivity under different ozone pollution levels implies that controlling only one of the precursors cannot achieve the best O_3 prevention effect, and the most appropriate ratio of O_3 precursor emission reductions should be designed according to ozone formation sensitivity in the different regions.","['Earth Sciences', 'Earth Sciences, general', 'Climate Change/Climate Change Impacts', 'Atmospheric Sciences', 'Climatology', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Earth Sciences', 'Climate Sciences', 'Atmospheric Science', 'Pollution']"
Springer,10.1038/s41534-024-00948-0,Faster-than-Clifford simulations of entanglement purification circuits and their full-stack optimization,"Generating quantum entanglement is plagued by decoherence. Distillation and error-correction are employed against such noise, but designing a good distillation circuit, especially on today’s imperfect hardware, is challenging. We develop a simulation algorithm for distillation circuits with per-gate complexity of $${\mathcal{O}}(1)$$ O ( 1 ) , drastically faster than $${\mathcal{O}}(N)$$ O ( N ) Clifford simulators or $${\mathcal{O}}({2}^{N})$$ O ( 2 N ) wavefunction simulators over N qubits. This simulator made it possible to optimize distillation circuits much larger than previously feasible. We design distillation circuits from n raw Bell pairs to k purified pairs and study the use of these circuits in the teleportation of logical qubits. The resulting purification circuits are the best-known for finite-size noisy hardware and can be fine-tuned for specific error-models. Furthermore, we design purification circuits that shape the correlations of errors in the purified pairs such that the performance of potential error-correcting codes is greatly improved.","['Physics', 'Physics, general', 'Quantum Physics', 'Quantum Information Technology, Spintronics', 'Quantum Computing', 'Quantum Field Theories, String Theory', 'Classical and Quantum Gravitation, Relativity Theory', 'Physics and Astronomy', 'Quantum Physics', 'Spintronics', 'Quantum Computing', 'Elementary Particles, Quantum Field Theory', 'Classical and Quantum Gravity']"
Springer,10.1038/s41586-024-08391-z,A foundation model of transcription across human cell types,"Transcriptional regulation, which involves a complex interplay between regulatory sequences and proteins, directs all biological processes. Computational models of transcription lack generalizability to accurately extrapolate to unseen cell types and conditions. Here we introduce GET (general expression transformer), an interpretable foundation model designed to uncover regulatory grammars across 213 human fetal and adult cell types^ 1 , 2 . Relying exclusively on chromatin accessibility data and sequence information, GET achieves experimental-level accuracy in predicting gene expression even in previously unseen cell types^ 3 . GET also shows remarkable adaptability across new sequencing platforms and assays, enabling regulatory inference across a broad range of cell types and conditions, and uncovers universal and cell-type-specific transcription factor interaction networks. We evaluated its performance in prediction of regulatory activity, inference of regulatory elements and regulators, and identification of physical interactions between transcription factors and found that it outperforms current models^ 4 in predicting lentivirus-based massively parallel reporter assay readout^ 5 , 6 . In fetal erythroblasts^ 7 , we identified distal (greater than 1 Mbp) regulatory regions that were missed by previous models, and, in B cells, we identified a lymphocyte-specific transcription factor–transcription factor interaction that explains the functional significance of a leukaemia risk predisposing germline mutation^ 8 – 10 . In sum, we provide a generalizable and accurate model for transcription together with catalogues of gene regulation and transcription factor interactions, all with cell type specificity. A foundation model learns transcriptional regulatory syntax from chromatin accessibility and sequence data across a range of cell types to predict gene expression and transcription factor interactions, with generalizability to unseen cell types.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1140/epjqt/s40507-025-00308-7,Defining quantum games,"In this research article, we survey existing quantum physics-related games and, based on this survey, propose a definition for the concept of quantum games. We define a quantum game as any type of rule-based game that either employs the principles of quantum physics or references quantum phenomena or the theory of quantum physics through any of three proposed dimensions: the perceivable dimension of quantum physics, the dimension of quantum technologies, and the dimension of scientific purposes, such as citizen science or education. We also discuss the concept of quantum computer games, which are games on quantum computers, as well as definitions for the concept of science games. Various games explore quantum physics and quantum computing through digital, analogue, and hybrid means, with various incentives driving their development. As interest in games as educational tools for supporting quantum literacy grows, understanding the diverse landscape of quantum games becomes increasingly important. We propose that the three dimensions of quantum games identified in this article be used for designing, analysing, and defining the phenomenon of quantum games.","['Physics', 'Quantum Physics', 'Quantum Information Technology, Spintronics', 'Nanotechnology and Microengineering', 'Quantum Physics', 'Spintronics', 'Microsystems and MEMS']"
Springer,10.1007/s10639-024-13297-4,Redefining computational thinking: A holistic framework and its implications for K-12 education,"In the realm of K-12 Education, the growing significance of Computational Thinking has sparked extensive inquiry into its nature and instructional methodologies. Despite a wealth of literature on the subject, ongoing debates persist regarding its fundamental components. Across global educational landscapes, Computational Thinking is being incorporated into curricula with varying degrees of emphasis, ranging from robotics to computer coding to broader algorithmic problem-solving approaches. This article aims to reassess established definitions of Computational Thinking and propose a nuanced understanding that highlights the necessity and importance of data in contemporary problem-solving contexts. Additionally, we present a comprehensive framework aimed at cultivating Computational Thinking skills in students, encompassing technical competencies alongside cognitive and metacognitive processes. Subsequently, we offer (1) a refined definition emphasizing the importance of data, (2) a comprehensive framework outlining essential components to foster Computational Thinking skills in students, and (3) a discussion of these concepts within the context of K-12 education.","['Computer Science', 'Computers and Education', 'Educational Technology', 'User Interfaces and Human Computer Interaction', 'Education, general', 'Information Systems Applications (incl.Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Computers and Education', 'Digital Education and Educational Technology', 'User Interfaces and Human Computer Interaction', 'Education', 'Computer and Information Systems Applications', 'Computer Application in Social and Behavioral Sciences']"
Springer,10.1007/s41965-024-00176-7,Chemical pure reaction automata in maximally parallel manner,"This work presents a new class of reaction automata, called Chemical Pure Reaction Automata (CPRA). CPRA combines characteristics of chemical reaction automata, as introduced by Okubo et al. in 2016, with those of the more recently defined pure reaction automata. Unlike standard chemical reaction automata, CPRA lack permanence, meaning their result states consist solely of the reaction products, with unconsumed reactants being discarded. We investigate the computational power of two CPRA variants, both working in a maximally parallel manner. We first prove that deterministic CPRA (DCPRA)—in which at every state, for each input symbol, the resulting state is the same for all multisets of enabled reactions—are not Turing complete. We then show that non-deterministic CPRA are Turing complete and thus strictly more powerful than DCPRA: namely, the set of languages accepted by CPRA in the maximally parallel manner contains the set of languages accepted by standard chemical reaction automata in the same manner.","['Computer Science', 'Theory of Computation', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Optimization', 'Computational Biology/Bioinformatics', 'Theory of Computation', 'Artificial Intelligence', 'Theory of Computation', 'Optimization', 'Computational and Systems Biology']"
Springer,10.3758/s13428-024-02587-x,The fundamentals of eye tracking part 3: How to choose an eye tracker,"There is an abundance of commercial and open-source eye trackers available for researchers interested in gaze and eye movements. Which aspects should be considered when choosing an eye tracker? The paper describes what distinguishes different types of eye trackers, their suitability for different types of research questions, and highlights questions researchers should ask themselves to make an informed choice.","['Psychology', 'Cognitive Psychology', 'Cognitive Psychology']"
Springer,10.1007/s42524-025-4136-9,Vision-language model-based human-robot collaboration for smart manufacturing: A state-of-the-art survey,"human-robot collaboration (HRC) is set to transform the manufacturing paradigm by leveraging the strengths of human flexibility and robot precision. The recent breakthrough of Large Language Models (LLMs) and Vision-Language Models (VLMs) has motivated the preliminary explorations and adoptions of these models in the smart manufacturing field. However, despite the considerable amount of effort, existing research mainly focused on individual components without a comprehensive perspective to address the full potential of VLMs, especially for HRC in smart manufacturing scenarios. To fill the gap, this work offers a systematic review of the latest advancements and applications of VLMs in HRC for smart manufacturing, which covers the fundamental architectures and pretraining methodologies of LLMs and VLMs, their applications in robotic task planning, navigation, and manipulation, and role in enhancing human–robot skill transfer through multimodal data integration. Lastly, the paper discusses current limitations and future research directions in VLM-based HRC, highlighting the trend in fully realizing the potential of these technologies for smart manufacturing.","['Engineering', 'Industrial and Production Engineering', 'Civil Engineering', 'Operations Management', 'Industrial and Production Engineering', 'Civil Engineering', 'Operations Management']"
Springer,10.1007/s10618-024-01074-3,Missing value replacement in strings and applications,"Missing values arise routinely in real-world sequential (string) datasets due to: (1) imprecise data measurements; (2) flexible sequence modeling, such as binding profiles of molecular sequences; or (3) the existence of confidential information in a dataset which has been deleted deliberately for privacy protection. In order to analyze such datasets, it is often important to replace each missing value, with one or more valid letters, in an efficient and effective way. Here we formalize this task as a combinatorial optimization problem: the set of constraints includes the context of the missing value (i.e., its vicinity) as well as a finite set of user-defined forbidden patterns, modeling, for instance, implausible or confidential patterns; and the objective function seeks to minimize the number of new letters we introduce. Algorithmically, our problem translates to finding shortest paths in special graphs that contain forbidden edges representing the forbidden patterns. Our work makes the following contributions: (1) we design a linear-time algorithm to solve this problem for strings over constant-sized alphabets; (2) we show how our algorithm can be effortlessly applied to fully sanitize a private string in the presence of a set of fixed-length forbidden patterns [Bernardini et al. 2021a]; (3) we propose a methodology for sanitizing and clustering a collection of private strings that utilizes our algorithm and an effective and efficiently computable distance measure; and (4) we present extensive experimental results showing that our methodology can efficiently sanitize a collection of private strings while preserving clustering quality, outperforming the state of the art and baselines. To arrive at our theoretical results, we employ techniques from formal languages and combinatorial pattern matching.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Information Storage and Retrieval', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Information Storage and Retrieval', 'Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences']"
Springer,10.1007/s00371-024-03780-x,Correction: Grid-induced bounding volume hierarchy for ray tracing dynamic scenes,,"['Computer Science', 'Computer Graphics', 'Computer Science, general', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Computer Graphics', 'Computer Science', 'Artificial Intelligence', 'Computer Vision']"
Springer,10.1007/s11119-024-10208-y,Joint plant-spraypoint detector with ConvNeXt modules and HistMatch normalization,"Context Serrated tussock ( Nassella trichotoma ) is a weed of national significance in Australia which offers little to no nutritional value to livestock, and has the potential to reduce carrying capacity and agricultural return of infested pastures. Aims The aim of this study was to adapt existing Convolutional Neural Networks (CNNs) for plant segmentation and spraypoint detection in the challenging environments of pastures. Methods CNNs that were designed for joint plant and stem segmentation in crop fields were repurposed for dual-task applications in pastures. Given the poor performance of these models in complex pasture environments, a new model drawing inspiration from the recently proposed ConvNeXt was developed, tested for its effectiveness on unseen field data, and enhanced with a novel normalization technique, called HistMatch. Key results Experimentation demonstrated that unlike pre-existing models, which were designed for the simpler environments encountered in early-stage crop fields, our model was able to generalize well to growing conditions not seen during training, achieving 0.807 mIoU and 0.796 F1-score for the plant and spraypoint tasks respectively. This is in comparison to pre-existing models, which achieved 0.270 - 0.454 mIoU and 0.073 - 0.496 F1-score for the same tasks. These results were further improved to 0.854 mIoU and 0.806 F1-score using HistMatch normalization. In spite of greater model complexity, our model had a inference time of 15.7 ms which was comparable to pre-existing models, and suitable for real-time applications. Conclusion Models with greater complexity are required for the relatively complex environments encountered in pastures, but this greater complexity need not come at the expense of real time capability. HistMatch normalization can improve model accuracy, and is particularly effective in cases where models are struggling to generalize well to testing conditions that vary significantly from those seen during training. Implications and impacts The successful adaptation and improvement of CNNs for weed management in pastures could significantly reduce the reliance on blanket herbicide application. HistMatch normalization could also be considered for other agricultural applications, including weed management and disease detection in crop fields and orchards.","['Life Sciences', 'Agriculture', 'Soil Science & Conservation', 'Remote Sensing/Photogrammetry', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Atmospheric Sciences', 'Agriculture', 'Soil Science', 'Geographical Information System', 'Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Atmospheric Science']"
Springer,10.1038/s41598-024-81359-1,Gaming time and impulsivity as independent yet complementary predictors of gaming disorder risk,"Prolonged gaming time, along with increased impulsivity—a key element of poor self-regulation—has been identified as linked to gaming disorder. Despite existing studies in this field, the relationship between impulsivity and gaming time remains poorly understood. The present study explored the connections between impulsivity, measured both by self-report and behavioral assessments, gaming time and gaming disorder within a cohort of 82 participants. While gaming time exhibited a significant correlation with gaming disorder, only self-reported measures of impulsivity and one behavioral metric showed a correlation with gaming disorder. Self-report measures of impulsivity exclusively predicted gaming disorder when included in a regression model with gaming time. The interaction between gaming time and impulsivity, aside from one behavioral metric was deemed insignificant. These findings suggest that impulsivity and gaming time, although associated with gaming disorder risk, are independent variables. Further research should aim to clarify these relationships and explore potential interventions targeting both DGI and impulsivity to mitigate gaming disorder risk.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
Springer,10.1038/s41586-024-08406-9,Scaling and networking a modular photonic quantum computer,"Photonics offers a promising platform for quantum computing^ 1 – 4 , owing to the availability of chip integration for mass-manufacturable modules, fibre optics for networking and room-temperature operation of most components. However, experimental demonstrations are needed of complete integrated systems comprising all basic functionalities for universal and fault-tolerant operation^ 5 . Here we construct a (sub-performant) scale model of a quantum computer using 35 photonic chips to demonstrate its functionality and feasibility. This combines all the primitive components as discrete, scalable rack-deployed modules networked over fibre-optic interconnects, including 84 squeezers^ 6 and 36 photon-number-resolving detectors furnishing 12 physical qubit modes at each clock cycle. We use this machine, which we name Aurora, to synthesize a cluster state^ 7 entangled across separate chips with 86.4 billion modes, and demonstrate its capability of implementing the foliated distance-2 repetition code with real-time decoding. The key building blocks needed for universality and fault tolerance are demonstrated: heralded synthesis of single-temporal-mode non-Gaussian resource states, real-time multiplexing actuated on photon-number-resolving detection, spatiotemporal cluster-state formation with fibre buffers, and adaptive measurements implemented using chip-integrated homodyne detectors with real-time single-clock-cycle feedforward. We also present a detailed analysis of our architecture’s tolerances for optical loss, which is the dominant and most challenging hurdle to crossing the fault-tolerant threshold. This work lays out the path to cross the fault-tolerant threshold and scale photonic quantum computers to the point of addressing useful applications.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Life Sciences', 'Physical Sciences', 'Technology and Engineering', 'Mathematics and Computing', 'Humanities and Social Sciences']"
Springer,10.1038/s41598-025-87412-x,A two-stage HDR reconstruction pipeline for extreme dark-light RGGB images,"RGGB sensor arrays are commonly used in digital cameras and mobile photography. However, images of extreme dark-light conditions often suffer from insufficient exposure because the sensor receives insufficient light. The existing methods mainly employ U-Net variants, multi-stage camera parameter simulation, or image parameter processing to address this issue. However, those methods usually apply color adjustments evenly across the entire image, which may cause extensive blue or green noise artifacts, especially in images with dark backgrounds. This study attacks the problem by proposing a novel multi-step process for image enhancement. The pipeline starts with a self-attention U-Net for initial color restoration and applies a Color Correction Matrix (CCM). Thereafter, High Dynamic Range (HDR) image reconstruction techniques are utilized to improve exposure using various Camera Response Functions (CRFs). After removing under- and over-exposed frames, pseudo-HDR images are created through multi-frame fusion. Also, a comparative analysis is conducted based on a standard dataset, and the results show that the proposed approach performs better in creating well-exposed images and improves the Peak-Signal-to-Noise Ratio (PSNR) by 0.16 dB compared to the benchmark methods.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary', 'Technology and Engineering', 'Physical Sciences', 'Life Sciences', 'Behavioral Sciences and Psychology', 'Computer Science']"
